<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Raft 博士论文翻译</title>
    <link href="/raft-translate/"/>
    <url>/raft-translate/</url>
    
    <content type="html"><![CDATA[<h2 id="4-集群成员变更"><a href="#4-集群成员变更" class="headerlink" title="4 集群成员变更"></a>4 集群成员变更</h2><p>到目前为止，我们一直假设集群配置(参与共识算法的服务器集合)是固定的。实际上，在生产环境中偶尔需要变更配置，例如在服务器出现故障替换服务器时或者变更副本数时。这时可以使用以下两种方法手动完成：</p><ul><li>可以通过使整个集群下线，更新配置文件，然后重启集群来变更配置。但是这将导致整个集群在配置变更期间不可用。</li><li>或者，新的服务器可以通过获取某个集群成员的网络地址来替换它。<br>然而，管理员必须保证被替换的服务器永远不会恢复，否则系统将失去其安全属性(例如，将有一个额外的投票)。</li></ul><p>这两种成员变更的方法都有显著的缺点，而且如果有任何手动步骤都会有操作错误的风险。</p><p>为了避免这些问题，我们决定将配置变更自动化，并将它们合并 Raft 共识算法中。Raft 允许集群在配置变更期间继续正常运行，并且只需对基本共识算法进行少量扩展即可实现成员变更。图 4.1 总结了用于集群成员变更的 RPC，本章其余部分将描述其元素。</p><blockquote><p><img src="/raft-translate/4_1.png" srcset="/img/loading.gif" alt><br>图 4.1：用于变更集群成员的 RPC。AddServer RPC 用于向当前配置添加新服务器，RemoveServer RPC 用于从当前配置中删除服务器。诸如 4.1 之类的节号指出了讨论特殊功能的地方。第 4.4 节讨论了在完整系统中使用这些 RPC 的方法。</p></blockquote><h3 id="4-1-安全"><a href="#4-1-安全" class="headerlink" title="4.1 安全"></a>4.1 安全</h3><p>保障安全是配置变更的首要挑战。为了确保该机制的安全，在过渡期间不能出现两位领导人同时当选的情况。如果单个配置变更添加或删除了许多服务器，则将集群从旧配置直接切换到新配置可能是不安全的；一次自动切换所有服务器是不可能的，因此集群可能在转换期间分裂成两个独立的主体（见图 4.2）。</p><blockquote><p><img src="/raft-translate/4_2.png" srcset="/img/loading.gif" alt><br>图 4.2：直接从一个配置切换到另一个配置是不安全的，因为不同的服务器将在不同的时间进行切换。在本例中，集群从 3 个服务器增长到 5 个服务器。不幸的是，在某个时间点，两个不同的领导人可以在同一个任期内被选举出来，一个拥有旧配置（C<sub>old</sub>）的多数，另一个拥有新配置（C<sub>new</sub>）的多数。</p></blockquote><p>大多数成员变更算法都引入了其他机制来处理这种问题。这是我们最初为 Raft 所做的，但后来我们发现了一个更简单的方法，即禁止会导致多数成员不相交的成员变更。因此，Raft 限制了允许的变更类型：一次只能从集群中添加或删除一个服务器。成员变更中更复杂的变更是通过一系列单服务器变更实现的。本章的大部分内容描述了单服务器方法，它比我们原来的方法更容易理解。为了完整起见，第 4.3 节描述了原始的方法，它增加了处理任意配置变更的复杂性。在发现更简单的单服务器变更方法之前，我们在 LogCabin 中实现了更复杂的方法；在撰写本文时，它仍然使用更复杂的方法。</p><p>当向集群中添加或删除单个服务器单个服务器时，旧集群的任何多数与新集群的任何多数重叠，参见图 4.3。这种重叠阻止了集群分裂成两个独立的多数派；在第 3.6.3 节的安全论证中，它保证了“投票人”的存在。因此，当只添加或删除一个服务器时，可以安全地直接切换到新配置。Raft 利用此属性，几乎不使用其他机制即可安全地变更集群成员。</p><blockquote><p><img src="/raft-translate/4_3.png" srcset="/img/loading.gif" alt><br>图 4.3：从偶数和奇数大小的集群中添加和删除单个服务器。在每个图中，蓝色矩形显示旧集群的大部分，红色矩形显示新集群的大部分。在每个单服务器成员变更中，旧集群的任何多数与新集群的任何多数之间都会有重叠，这用来保证安全性。例如在（b）中，旧集群的大部分必须包括其余 3 个服务器中的 2 个，而新集群的大部分必须包括新集群中的 3 个服务器，其中至少有 2 个来自旧集群。</p></blockquote><p>集群配置使用复制日志中的特殊条目进行存储和通信。这利用了 Raft 的现有机制来复制和持久化配置信息。通过对配置变更和客户机请求进行排序（允许两者在管道或批处理被同时复制），它还允许集群在进行配置变更时继续为客户机请求提供服务。</p><p>当领导者收到从当前配置(C<sub>old</sub>)中添加或删除服务器的请求时，它将新配置(C<sub>new</sub>)作为一个条目添加到其日志中，并使用常规的 Raft 机制复制该条目。新配置一旦添加到服务器的日志中，就会在这个服务器上生效：C<sub>new</sub> 条目被复制到 C<sub>new</sub> 指定的服务器上，而大部分服务器的新配置生效被用于确定 C<sub>new</sub> 条目的提交。这意味着服务器不会等待配置条目被提交，并且每个服务器总是使用在其日志中找到的最新配置。</p><p>一旦提交了 C<sub>new</sub> 条目，配置变更就完成了。此时，领导者知道大多数的 C<sub>new</sub> 指定的服务器已经采用了 C<sub>new</sub>。它还知道，没有收到 C<sub>new</sub> 条目的任意服务器都不能再构成集群的大多数，没有收到 C<sub>new</sub> 的服务器也不能再当选为领导者。C<sub>new</sub> 的提交让三件事得以继续:</p><ol><li>领导可以确认配置变更的成功完成。</li><li>如果配置变更删除了服务器，则可以关闭该服务器。</li><li>可以启动进一步的配置变更。在此之前，重叠的配置变更可能会降级为不安全的情况，如图 4.2 所示。</li></ol><p>如上所述，服务器总是在其日志中使用最新的配置，而不管是否提交了配置条目。这使得领导者可以很容易地避免重叠的配置变更（上面的第三项），方法是直到之前的变更的条目提交之后才开始新的变更。只有当旧集群的大多数成员都在 C<sub>new</sub> 的规则下运行时，才可以安全地开始另一次成员变更。如果服务器只在了解到 C<sub>new</sub> 已提交时才采用 C<sub>new</sub>，那么 Raft 的领导者将很难知道何时旧集群的大部分已经采用它。它们需要跟踪哪些服务器知道配置变更条目的提交，服务器也需要将它们的提交索引保存到磁盘；Raft 其实不需要这些机制。相反，Raft 的每台服务器只要发现该条目存在于其日志中，就采用 C<sub>new</sub>，并且知道一旦提交了 C<sub>new</sub> 条目，就可以安全地允许进一步的配置变更。不幸的是，这个决定意味着配置变更的日志条目可以被删除（如果领导者发生变化）；在这种情况下，服务器必须准备好返回到其日志中的先前配置。</p><p>在 Raft 中，用于达成一致意见的是调用者的配置，包括投票和日志复制:</p><ul><li>即使领导者并不在服务器的最新配置中，服务器依然接受它的 AppendEntries 请求。否则将永远不能将新服务器添加到集群中（它永远不会接受添加服务器的配置条目之前的任何日志条目）。</li><li>服务器还允许投票给不属于服务器当前最新配置的候选人(如果候选人有足够的最新日志和当前任期）。为了保持集群可用，可能偶尔需要进行投票表决。例如，考虑将第四个服务器添加到三个服务器的集群中。如果一个服务器出现故障，就需要新服务器的投票来形成多数票并选出一个领导人。</li></ul><p>因此，服务器可以直接处理传入的 RPC 请求，而无需查询其当前配置。</p><h3 id="4-2-可用性"><a href="#4-2-可用性" class="headerlink" title="4.2 可用性"></a>4.2 可用性</h3><p>集群成员变更在保持集群可用性方面引入了多个问题。第 4.2.1 节讨论了在将新服务器添加到集群之前使其追赶，以防止它拖延对新日志条目的提交。第 4.2.2 节介绍了如何从集群中删除现有的领导者。第 4.2.3 节介绍了如何防止已删除的服务器干扰新集群的领导者。最后，第 4.2.4 节以一个论点结束，说明为什么最终的成员变更算法足以在任何成员变更期间保留可用性。</p><h4 id="4-2-1-让新服务器追赶"><a href="#4-2-1-让新服务器追赶" class="headerlink" title="4.2.1 让新服务器追赶"></a>4.2.1 让新服务器追赶</h4><p>当服务器被添加到集群中时，它通常不会存储任何日志条目。如果以这种状态将它添加到集群中，那么它的日志可能需要相当长的时间才能赶上领导者的日志，在此期间，集群更容易出现不可用的情况。例如，一个包含三个服务器的集群通常可以容忍一个故障而不损失可用性。但是，如果向同一集群添加了第四个服务器，其中有一个日志为空，并且原来的三个服务器之一出现故障，则集群将暂时无法提交新条目（参见图 4.4(a)）。如果一个集群中连续不断地添加许多新服务器，就会出现另一个可用性问题，此时需要新服务器来组成集群的大部分（参见图 4.4(b)）。在这两种情况下，直到新服务器的日志赶上领导者的日志之前，集群都将不可用。</p><blockquote><p><img src="/raft-translate/4_4.png" srcset="/img/loading.gif" alt><br>图 4.4：添加空日志的服务器如何使可用性面临风险的示例。图中显示了服务器在两个不同集群中的日志。每个集群都有三个服务器，S1-S3。在（a）中，添加了 S4，然后 S3 失败。集群应该能够在一个节点失败后正常运行，但事实上它失去了可用性：它需要四个服务器中的三个来提交一个新条目，但是 S3 失败了，S4 的日志太晚了，无法追加新条目。在（b）中，快速连续地添加了 S4–S6。提交添加 S6（第三个新服务器）的配置条目需要 4 个服务器的日志来存储该条目，但是 S4-S6 的日志远远不够。在新服务器的日志赶上之前，这两个集群都是不可用的。</p></blockquote><p>为了避免可用性缺口，Raft 在配置变更之前引入了一个附加阶段，其中一个新服务器作为无投票权成员加入集群。领导者复制日志条目到它，但出于投票或提交目的，尚未计入多数。一旦新服务器赶上了集群的其余部分，就可以按照上面的描述进行重新配置。（支持无投票权服务器的机制在其他上下文中也很有用；例如，它可以用于将状态复制到大量服务器，这些服务器可以保证最终一致性提供只读请求。)</p><p>领导者需要确定什么时候一个新的服务器已经赶上进度从而可以继续进行配置变更。这需要注意保持可用性：如果服务器添加得太快，集群的可用性可能面临风险，如上所述。我们的目标是将任何暂时的不可用保持在一个选举超时以下，因为客户必须已经能够容忍这种程度的偶发性不可用（在领导者失败的情况下）。此外，如果可能的话，我们希望通过使新服务器的日志更接近领导者的日志来进一步减少不可用性。</p><p>如果新服务器不可用或速度太慢以至于永远无法赶上，则领导者还应中止变更。这项检查很重要：因为没有将这些检查包括在内，Lamport 的古老 Paxos 政府破裂了。他们不小心将成员变更为溺水的水手，无法再取得进步。尝试添加不可用或速度较慢的服务器通常是一个错误。实际上，我们的第一个配置变更请求就包含了网络端口号中的一个输入错误；系统正确地中止了变更并返回了一个错误。</p><p>我们建议使用以下算法来确定何时新服务器能够赶上并能被添加到集群中。将条目复制到新服务器的过程分为几轮，如图 4.5 所示。每一轮开始时都将当前领导者日志中的所有日志条目复制到新服务器的日志中。在为其当前轮复制条目时，新条目可能会到达领导者；它将在下一轮中复制它们。随着进程的进行，轮持续时间在时间上缩短。该算法等待固定的轮数（如 10 轮）。如果最后一轮的持续时间少于一个选举超时，则领导者将新服务器添加到集群中，并假设没有足够的未复制条目来创建显著的可用性缺口。否则，领导者将因错误中止配置变更。调用者可能总会再试一次（下一次成功的可能性更大，因为新服务器的日志已经追赶上一部分）。</p><blockquote><p><img src="/raft-translate/4_5.png" srcset="/img/loading.gif" alt><br>图 4.5：为了让新服务器追赶，领导者将条目复制到新服务器的过程分为几轮。每一轮结束后，新服务器就会拥有这一轮开始时领导者日志中记录的所有条目。然而，到那时，领导者可能已经收到了新的条目；这些将在下一轮中重复。</p></blockquote><p>作为让新服务器追赶的第一步，领导者必须发现新服务器的日志是空的。对于新服务器，在 AppendEntries 中的一致性检查将不断失败，直到领导者的 nextIndex 最终下降到 1。这种遍历可能是影响在集群中添加新服务器的性能的主要因素（在此阶段之后，可以通过使用批处理将日志条目以更少的 RPC 传输给跟随者）。各种方法可以使 nextIndex 更快地收敛到它的正确值，包括第 3 章中描述的方法。然而，解决添加新服务器这一特殊问题的最简单方法是，使跟随者在 AppendEntries 响应中返回其日志的长度；从而使领导者可以相应地捕捉跟随者的 nextIndex。</p><h4 id="4-2-2-删除当前领导人"><a href="#4-2-2-删除当前领导人" class="headerlink" title="4.2.2 删除当前领导人"></a>4.2.2 删除当前领导人</h4><p>如果要求现有领导者将自己从集群中删除，则它必须在某个时候下台。一种简单的方法是使用第 3 章中介绍的领导权禅让扩展：被要求删除自身的领导者会将其领导权转移到另一台服务器，该服务器随后将正常执行成员变更。</p><p>我们最初为 Raft 开发了一种不同的方法，在该方法中，现有领导者进行成员变更以删除自己，然后下台。这使 Raft 处于有些尴尬的操作模式，即领导者临时管理一个它不是其成员的配置。最初，我们需要这种方法来进行任意配置变更（请参阅第 4.3 节），其中旧的配置和新的配置可能没有任何可以转移领导权的服务器，同样的方法也适用于那些没有实现领导权禅让的系统。</p><p>在这种方法中，一旦提交 C<sub>new</sub> 条目，从配置中删除的领导者将会下台。如果领导者在此之前下台，它可能仍会超时并再次成为领导者，从而延迟了进度。在极端情况下，从两台服务器的集群中删除领导者时，服务器甚至可能必须再次成为领导者，集群才能取得进展；参见图4.6。因此，领导者要等到 C<sub>new</sub> 提交再下台。这是新配置肯定可以在没有被废黜的领导者参与的情况下运行的第一点：C<sub>new</sub> 的成员总是有可能从他们当中选出新的领导者。被删除的领导者卸任后，C<sub>new</sub> 中的服务器将超时并赢得选举。这种较小的可用性缺口一般是可以容忍的，因为在领导者失败时也会出现类似的可用性缺口。</p><blockquote><p><img src="/raft-translate/4_6.png" srcset="/img/loading.gif" alt><br>图 4.6：在提交 C<sub>new</sub> 条目之前，已删除的服务器可能需要引导集群取得进展。 该图显示了从两个服务器集群中删除 S1。 S1 目前是负责人，S1 还不应该下台；它仍然需要作为领导者。 在 S2 收到 S1 的 C<sub>new</sub> 条目之前，它无法成为领导者（因为 S2 仍然需要 S1 的投票才能构成 C<sub>old</sub> 的多数票，并且 S1 不会向 S2 投票，因为 S2 的日志不是最新的）。</p></blockquote><p>这种方法对决策产生了两种影响，这些影响不是特别有害，但可能令人惊讶。首先，领导者可以在一段时间内（提交 C<sub>new</sub> 时）管理一个不包含自身的集群。它复制日志条目，但不占多数。其次，不属于其自身最新配置的服务器仍应开始新的选举，因为在提交 C<sub>new</sub> 条目之前可能仍需要它（如图 4.6 所示）。除非它是其最新配置的一部分，否则它不会在选举中计入自己的选票。</p><h4 id="4-2-3-破坏性服务器"><a href="#4-2-3-破坏性服务器" class="headerlink" title="4.2.3 破坏性服务器"></a>4.2.3 破坏性服务器</h4><p>如果没有其他机制，不在 C<sub>new</sub> 中的服务器可能会破坏集群。一旦集群领导者创建了 C<sub>new</sub> 条目，不在 C<sub>new</sub> 中的服务器将不再接收心跳信号，因此它将超时并开始新的选举。此外，它不会收到 C<sub>new</sub> 条目，也不会了解该条目已经提交，因此它不会知道它已从集群中删除。服务器将使用新的任期编号发送 RequestVote RPC，这将导致当前的领导者恢复为跟随者状态。虽然终将选举出 C<sub>new</sub> 的一位新领导者，但破坏性服务器将再次超时，并且该过程将重复进行，从而导致可用性降低。如果从集群中删除了多个服务器，情况还可能进一步恶化。</p><p>我们消除干扰的第一个想法是，如果一个服务器要开始选举，它首先要检查它是否浪费了每个人的时间——它有机会赢得选举。这为选举引入了一个新阶段，称为预投票阶段。候选人首先会询问其他服务器其日志是否足够新以获得他们的投票。只有当候选人相信自己能从集群中的大多数人那里获得选票时，他才会增长任期编号并开始正常的选举。</p><p>不幸的是，预投票阶段并没有解决干扰服务器的问题：在某些情况下，干扰服务器的日志已经足够新，但是开始选举仍然会造成干扰。可能令人惊讶的是，这些可能在配置变更完成之前就发生了。例如，图 4.7 显示了从集群中删除的服务器。一旦领导者创建了 C<sub>new</sub> 日志条目，被删除的服务器可能会造成混乱。在这种情况下，预投票检查没有帮助，因为被删除的服务器的日志比大多数集群的日志更新。(虽然预投票阶段并没有解决破坏性服务器的问题，但总体上来说，它确实是提高领导人选举稳健性的一个有用的想法；参见第9章)。</p><blockquote><p><img src="/raft-translate/4_7.png" srcset="/img/loading.gif" alt><br>图 4.7：一个服务器如何在提交 C<sub>new</sub> 日志条目之前造成破坏，并且预投票阶段没有帮助的示例。图中显示了从一个包含四个服务器的集群中删除 S1 的过程。S4 是新集群的领导者，它在日志中创建了 C<sub>new</sub> 条目，但还没有复制该条目。旧集群中的服务器不再接收来自 S4 的心跳。甚至在提交 C<sub>new</sub> 之前，S1 就可以超时，增加它的任期，并将这个较大的任期号发送给新的集群，迫使 S4 下台。预投票算法没有帮助，因为 S1 的日志与大多数集群中的服务器一样是最新的。</p></blockquote><p>基于这种情况，我们现在认为，仅仅基于比较日志（如预投票检查）的解决方案不足以判断选举是否具有破坏性。我们不能要求服务器在开始选举之前检查 C<sub>new</sub> 中每个服务器的日志，因为 Raft 必须始终能够容忍错误。我们也不希望假设领导者能够足够快地可靠地复制条目，从而快速地通过图 4.7 所示的场景；这在实践中可能是可行的，但这取决于更强的假设，即我们更希望避免探索日志分歧的性能和复制日志条目的性能。</p><p>Raft 的解决方案是使用心跳来确定何时存在有效的领导者。在 Raft 中，如果领导者能够保持跟随者的心跳状态，则它被认为是活跃的（否则，另一个服务器将开始选举）。因此，服务器不应该能够干扰正在发送心跳的领导者。我们对 RequestVote RPC 进行了修改，以实现此目的：如果服务器在从当前领导人那里听到的最小选举超时内收到 RequestVote 请求，它不会更新其任期或授予其投票。它可以放弃请求，拒绝投票或延迟请求。结果基本相同。这不会影响正常的选举，在正常的选举中，每个服务器在开始选举之前至少要等待最小选举超时的时间。但是，它有助于避免 C<sub>new</sub> 以外的服务器造成的干扰：尽管领导者能够对其集群发出心跳，但不会因更大任期的节点而被废黜。</p><p>此变更与第 3 章中所述的领导权禅让机制相冲突，在第 3 章中，服务器合法地开始选举而无需等待选举超时。在这种情况下，即使其他服务器认为当前的集群领导者存在，也应该处理 RequestVote 消息。这些 RequestVote 请求可以包含一个特殊标志来指示此行为（“我有权破坏领导者，是它告诉我的！”）。</p><h4 id="4-2-4-可用性参数"><a href="#4-2-4-可用性参数" class="headerlink" title="4.2.4 可用性参数"></a>4.2.4 可用性参数</h4><p>本节认为，上述解决方案足以在成员变更期间保持可用性。由于 Raft 的成员变更是基于领导者的，因此我们证明了该算法将能够在成员变更期间维护和替换领导者，而且领导者将同时为客户请求提供服务并完成配置变更。我们假设大多数旧配置是可用的（至少在提交 C<sub>new</sub> 之前），并且大部分新配置可用。</p><ol><li><p>可以在配置变更的所有步骤中选举一位领导者。</p><ul><li>如果新集群中具有最新日志的可用服务器具有 C<sub>new</sub> 条目，它可以从大多数 C<sub>new</sub> 那里收集选票并成为领导者。</li><li>否则，C<sub>new</sub> 条目必然尚未提交。 在旧集群和新集群中，具有最新日志的可用服务器可以收集大多数  C<sub>old</sub> 和大多数 C<sub>new</sub> 的投票，因此，无论使用哪种配置，它都可以成为领导者。</li></ul></li><li><p>领导一经选举便得到维持，假设他的心跳达到了正常状态，<br>除非它因不在 C<sub>new</sub> 中但已提交 C<sub>new</sub> 而有意退出。</p><ul><li>如果领导者可以可靠地将心跳发送到其自己的跟随者，则它或其跟随者都不会接受更高的任期：他们不会超时开始任何新的选举，并且他们将忽略来自其他服务器的更高任期的任何 RequestVote 消息。因此，领导者不会被迫下台。</li><li>如果不在 C<sub>new</sub> 中的服务器提交 C<sub>new</sub> 条目并退出，则 Raft 将选出新的领导者。这个新领导者很可能将成为 C<sub>new</sub> 的一部分，从而完成配置变更。 但是，下台的服务器可能会再次成为领导者，这存在一些（较小）风险。 如果它再次当选，它将确认 C<sub>new</sub> 条目的提交并很快下台，并且 C<sub>new</sub> 中的服务器下次可能再次成功。</li></ul></li><li><p>在整个配置变更期间，领导者将为客户端请求提供服务。</p><ul><li>领导者可以在整个变更过程中继续将客户请求添加到他们的日志中。</li><li>由于在将新服务器添加到集群之前对其进行了跟踪，因此领导者可以提前提交其提交索引并及时回复客户端。</li></ul></li><li><p>领导者将通过提交 C<sub>new</sub> 来推进并完成配置变更，并在必要时退出以允许 C<sub>new</sub> 中的服务器成为领导者。</p></li></ol><p>因此，在上述假设下，本节中描述的机制足以在任何成员变更期间保持可用性。</p><h3 id="4-3-使用联合共识进行任意配置变更"><a href="#4-3-使用联合共识进行任意配置变更" class="headerlink" title="4.3 使用联合共识进行任意配置变更"></a>4.3 使用联合共识进行任意配置变更</h3><p>本节介绍了一种更复杂的集群成员变更方法，该方法可以处理对配置的任意变更。例如，可以一次将两个服务器添加到集群中，也可以一次替换五个服务器集群中所有的服务器。这是我们提出的第一种处理成员变更的方法，这里只是为了完整性而进行描述它。既然我们知道了更简单的单服务器方法，我们建议改为使用单服务器方法，因为处理任意变更需要额外的复杂性。任意变更通常是文献中假定的成员变更方式，但是我们认为在实际系统中并不需要这种灵活性，在实际系统中，一系列单服务器变更可以将集群成员变更为任何所需的配置。</p><p>为了确保跨任意配置变更的安全性，集群首先切换到过渡配置，我们称之为联合共识；一旦联合共识被提交，系统便过渡到新配置。联合共识将新旧配置结合到了一起：</p><ul><li>日志条目被复制到所有包含新配置或旧配置的服务器。</li><li>来自任一配置的任何服务器都可以作为领导者。</li><li>协议（用于选举和条目提交）要求新老配置各占多数。 例如，当从 3 个服务器的集群变更为 9 个服务器的不同集群时，协议要求既需要旧配置的 3 个服务器中的 2 个，也需要新配置的 9 个服务器中的 5 个。</li></ul><p>联合共识允许各个服务器在不同的时间在不同的配置之间转换，且不会影响安全性。 此外，联合共识允许集群在整个配置变更期间继续为客户请求提供服务。</p><p>这种方法通过联合配置的中间日志条目扩展了单服务器成员变更算法。图 4.8 说明了该过程。当领导者收到将配置从 C<sub>old</sub> 变更为 C<sub>new</sub> 的请求时，它保存用于联合共识的配置（图中的 C<sub>old</sub>,<sub>new</sub>）作为日志条目，并使用常规的 Raft 机制复制该条目。与单服务器配置变更算法一样，每台服务器将配置存储在日志中后即开始使用新配置。这意味着领导者将使用 C<sub>old</sub>,<sub>new</sub> 规则来确定何时提交 C<sub>old</sub>,<sub>new</sub> 的日志条目。如果领导者崩溃，则可以根据获胜的候选人是否收到了 C<sub>old</sub>,<sub>new</sub>，在 C<sub>old</sub> 或 C<sub>old</sub>,<sub>new</sub> 下选择新的领导者。无论如何，C<sub>new</sub> 不能在此期间做出单方面决定。</p><p>一旦提交了 C<sub>old</sub>,<sub>new</sub>，C<sub>old</sub>,<sub>new</sub> 都无法在未经对方批准的情况下做出决策，并且领导人完整性属性确保只有具有 C<sub>old</sub>,<sub>new</sub> 日志条目的服务器才能被选为领导者。现在，领导者可以安全地创建 C<sub>new</sub> 日志条目并将其复制到集群。一旦服务器看到该配置，它将立即生效。当在 C<sub>new</sub> 规则下提交了 C<sub>new</sub> 日志条目时，旧的配置就变为不相关，不在新配置中的服务器此时可以关闭。如图 4.8 所示，C<sub>old</sub> 和 C<sub>new</sub> 都没有时间可以单方面做出决定。这样可以保证安全。</p><blockquote><p><img src="/raft-translate/4_8.png" srcset="/img/loading.gif" alt><br>图4.8：使用联合共识进行配置变更的时间表。 虚线表示已创建但尚未提交的配置条目，实线表示最新提交的配置条目。领导者首先在其日志中创建 C<sub>old</sub>,<sub>new</sub> 配置条目，并将其提交给 C<sub>old</sub>,<sub>new</sub>（大多数 C<sub>old</sub> 和 C<sub>new</sub>）。 然后，它创建 C<sub>new</sub> 条目并将其提交给大多数 C<sub>new</sub>。<br>在任何时候，C<sub>old</sub> 和 C<sub>new</sub> 都不能同时独立地做出决策。</p></blockquote><p>联合共识方法可以推广到允许在先前的变更仍在进行时开始新的配置变更。然而，这样做没有太大的实际好处。相反，当配置变更已在进行时（当其最新配置未提交或不是简单多数时），领导者将拒绝其他配置变更。以这种方式拒绝的变更可以简单地等待并稍后再试。</p><p>这种联合共识方法比单服务器变更更为复杂，因为它需要在中间配置之间进行转换。联合配置还要求变更所有投票和提交决定的方式；领导者必须检查服务器是否既构成旧集群的大部分，也构成新集群的大多数，而不是简单地对服务器进行计数。要实现这一点，需要发现并变更我们的 Raft 实现中的六个比较。</p><h3 id="4-4-系统集成"><a href="#4-4-系统集成" class="headerlink" title="4.4 系统集成"></a>4.4 系统集成</h3><p>Raft 的实现可以通过不同的方式实现本章描述的集群成员变更机制。例如，图 4.1 中的 AddServer 和 RemoveServer RPC 可以由管理员直接调用，也可以由使用一系列单服务器步骤以任意方式变更配置的脚本调用。</p><p>在响应服务器故障等事件时自动调用成员变更可能是可取的。然而，这最好根据一个合理的政策来做。例如，对于集群来说，自动删除失败的服务器可能是危险的，因为它可能会留下太少的副本来满足预期的持久性和容错需求。一种可行的方法是让系统管理员配置所需的集群大小，在此约束下，可用的服务器可以自动替换失败的服务器。</p><p>当进行需要多个单服务器步骤的集群成员变更时，最好能够在删除服务器之前添加服务器。例如，要替换三个服务器集群中的一个服务器，添加一个服务器，然后删除另一个服务器，则系统在整个过程中始终允许处理一个服务器故障。但是，如果在添加另一个服务器之前先删除一个服务器，那么系统将暂时无法容忍任何故障（因为两个服务器集群需要两个服务器都可用）。</p><p>成员变更激发了引导集群的另一种方法。如果没有动态成员变更，每个服务器只有一个列出配置的静态文件。有了动态成员变更，服务器不再需要静态配置文件，因为系统在 Raft 日志中管理配置；当然它也可能容易出错（例如，应使用哪种配置来初始化新服务器？）。实际上，我们建议在第一次创建集群时，使用配置条目作为其日志中的第一个条目来初始化一个服务器。此配置仅列出该服务器。它本身构成其配置的大部分，因此它可以认为此配置已提交。从那时起，其他服务器应使用空日志进行初始化；它们被添加到集群中，并通过成员变更机制了解当前配置。</p><p>成员变更也需要一种动态的方法让客户找到集群。这将在第 6 章中讨论。</p><h3 id="4-5-结论"><a href="#4-5-结论" class="headerlink" title="4.5 结论"></a>4.5 结论</h3><p>本章介绍了 Raft 用于自动处理集群成员变更的扩展。这是基于共识的完整系统的重要组成部分，因为容错要求可能会随时间的推移而变化，并且最终需要更换故障服务器。</p><p>由于新配置会影响“多数”的含义，因此共识算法必须从根本上保证在配置变更期间的安全性。本章介绍了一种简单的方法，可以一次添加或删除单个服务器。这些操作简单地保障了安全性，因为在变更期间至少有一台服务器与大多数服务器重叠。可以组合多个单服务器变更来更彻底地修改集群。Raft 允许集群在成员变更期间继续正常运行。</p><p>要在配置变更期间保持可用性，需要处理几个重要的问题。特别是，不属于新配置的服务器干扰有效集群领导人的问题非常微妙；在选择基于心跳的有效解决方案之前，我们在几个基于日志比较的低效解决方案中挣扎。</p><h2 id="5-日志压缩"><a href="#5-日志压缩" class="headerlink" title="5 日志压缩"></a>5 日志压缩</h2><p>随着越来越多的客户请求，Raft 的日志在正常运行期间会不断增长。随着它变的越来越大，它会占用更多的空间，同时也需要更多的时间来回放。如果没有压缩日志的方法，最终将导致可用性问题：即服务器存储空间不足，或者启动时间太长。因此，任何实际系统都需要某种形式的日志压缩。</p><p>日志压缩的一般思想是，日志中的许多信息会随着时间的流逝而过时并可以丢弃。例如，如果稍后的操作将 x 设置为 3，则将 x 设置为 2 的操作已过时。一旦提交了日志条目并将其应用于状态机，就不再需要用于到达当前状态的中间状态和操作，并且可以压缩它们。</p><p>与 Raft 核心算法和成员变更不同，不同的系统在日志压缩方面有不同的需求。由于多种原因，没有一种适合所有人的解决方案来进行日志压缩。首先，不同的系统可能会选择在不同程度上权衡简单性和性能。其次，状态机必须紧密地参与日志压缩，并且状态机的大小以及状态机是基于磁盘还是易失性存储器的差异都很大。</p><p>本章的目的是讨论各种日志压缩方法。在每种方法中，日志压缩的大部分责任都落在状态机上，状态机负责将状态写入磁盘并压缩状态。状态机可以通过不同的方式来实现这一目标，本章将对此进行介绍，并在图 5.1 中进行总结：</p><blockquote><p><img src="/raft-translate/5_1.png" srcset="/img/loading.gif" alt><br>图 5.1：该图显示了如何在Raft中使用各种日志压缩方法。图中日志结构合并树的详细信息基于 LevelDB，日志清理的详细信息基于 RAMCloud。 省略了删除管理规则。</p></blockquote><ul><li>从概念上讲，为基于内存的状态机创建快照最为简单。在创建快照时，整个当前系统状态被写入稳定存储上的快照，然后丢弃该点之前的所有日志。这种方法在 Chubby 和 ZooKeeper 中使用，并且我们在 LogCabin 中也实现了它。它是本章第 5.1 节中最深入介绍的方法。</li><li>对于基于磁盘的状态机，作为正常操作的一部分，系统状态的最新副本将保留在磁盘上。因此，一旦状态机反映了对磁盘的写入，就可以丢弃 Raft 日志，并且仅在将一致的磁盘映像发送到其他服务器时才使用快照（第 5.2 节）。</li><li>第 5.3 节介绍了增量式日志压缩方法，例如日志清理和日志结构合并树。这些方法有效地将数据写入磁盘，并且随着时间的推移它们平均地利用资源。</li><li>最后，第 5.4 节讨论了通过将快照直接存储在日志中来最小化所需机制的快照方法。尽管该方法更容易实现，但仅适用于非常小的状态机。</li></ul><p>LogCabin 当前仅实现基于内存的快照方法（它嵌入了基于内存的状态机）。</p><p>各种压缩方法都共享了几个核心概念。首先，不要将压缩决策集中在领导者上，而是每个服务器独立地压缩其日志的提交前缀。这避免了领导者向已经在日志中记录了数据的跟随者发送数据。它还有助于模块化：日志压缩的大部分复杂性都包含在状态机中，并且与 Raft 本身没有太多交互。这有助于将整个系统的复杂性降至最低：Raft 的复杂性递增而不是倍增了日志压缩的复杂性。在第 5.4 节中进一步讨论了将压缩责任集中在领导者上的其他方法（对于非常小的状态机，基于领导者的方法可能更好）。</p><p>其次，状态机和 Raft 之间的基本交互涉及到将一个日志前缀的责任从 Raft 转移到状态机。在应用条目之后，状态机将这些条目以一种可以恢复当前系统状态的方式反映到磁盘上。完成后，它告诉 Raft 放弃相应的日志前缀前的所有日志。在 Raft 放弃对日志前缀前所有日志的责任之前，它必须保存一些描述日志前缀的自身状态。具体来说，Raft 保留了它丢弃的最后一个条目的索引和任期；这会将其余的日志锚定在状态机的状态之后，并允许 AppendEntries 一致性检查继续进行（它需要日志中第一个条目之前条目的索引和任期）。为了支持集群成员的更改，Raft 还保留了丢弃日志前缀前的最新配置。</p><p>第三，一旦 Raft 丢弃了日志前缀前的日志，状态机将承担两项新的责任。如果服务器重新启动，则状态机需要先从磁盘加载与被丢弃的日志条目应用相对应的状态，然后才能应用 Raft 日志中的任何条目。此外，状态机可能需要拍摄一个一致的状态映像，以便可以将其发送给缓慢的跟随者（跟随者的日志远远落后于领导者的日志）。将压缩延迟到日志条目“完全复制”到集群中的每个成员之前是不可行的，因为必须保证少数缓慢的跟随者不阻止集群完全可用，并且随时可以将新服务器添加到集群中。因此，缓慢的跟随者或新服务器偶尔需要通过网络接收其初始状态。当 AppendEntries 中所需的下一个条目已在领导者的当前日志中删除时，Raft 会检测到。在这种情况下，状态机必须提供一个一致的状态映像，然后由领导者发送给跟随者。</p><h3 id="5-1-基于内存的状态机快照"><a href="#5-1-基于内存的状态机快照" class="headerlink" title="5.1 基于内存的状态机快照"></a>5.1 基于内存的状态机快照</h3><p>第一种快照方法适用于状态机的数据结构保存在内存中的情况。对于数据集为 GB 或数十 GB 的状态机，这是一个合理的选择。它使操作能够快速完成，因为它们不必从磁盘中获取数据。它也很容易编程实现，因为可以使用丰富的数据结构，并且每个操作都可以运行完成（不阻塞 I/O）。</p><p>图 5.2 显示了当状态机保持在内存中时，在 Raft 中进行快照的基本思想。每个服务器都独立拍摄快照，仅覆盖其日志中的已提交条目。快照中的大部分工作涉及序列化状态机的当前状态，这特定于特定的状态机实现。例如，LogCabin 的状态机使用树作为其主要数据结构；它使用顺序的深度优先遍历序列化此树（以便在应用快照时，在其子节点之前创建父节点）。状态机还必须序列化其保留的信息，来为客户端提供线性化能力（请参阅第 6 章）。</p><blockquote><p><img src="/raft-translate/5_2.png" srcset="/img/loading.gif" alt><br>图 5.2：服务器用一个新的快照替换日志中提交的条目（索引 1 到 5），该快照只存储当前状态（本例中是变量 x 和 y）。在丢弃条目 1 到 5 之前，Raft 保存快照最后包含的索引（5）和任期（3），以便将快照放置到条目 6 之前的日志中。</p></blockquote><p>一旦状态机完成了快照的写入，就可以将日志截断。Raft 首先存储重新启动所需的状态：快照中包含的最后一个条目的索引和任期以及该索引之前的最新配置。然后，它将丢弃日志索引在该索引前的日志。由于以前的快照不再有用，因此也可以将其丢弃。</p><p>如上所述，领导者有时可能需要将其状态发送给缓慢的跟随者和正在加入集群的新服务器。在快照中，此状态只是最新的快照，领导者使用名为 InstallSnapshot 的新 RPC 进行传输，如图 5.3 所示。当跟随者使用此 RPC 接收快照时，它必须决定如何处理其现有的日志条目。通常，快照将包含跟随者日志中尚未包含的新信息。在这种情况下，跟随者将丢弃其整个日志；它全部被快照取代，并且可能具有与快照冲突的未提交条目。相反，如果跟随者接收到描述其某个日志前缀的快照（由于重新传输或错误操作），则快照所覆盖的日志条目将被删除，但快照之后的条目仍然有效，必须保留。</p><blockquote><p><img src="/raft-translate/5_3.png" srcset="/img/loading.gif" alt><br>图 5.3：领导者调用 InstallSnapshot RPC 将快照发送给缓慢的跟随者。当 AppendEntries 中所需的下一个条目已在领导者的当前日志中删除时，他才会发送快照。他将快照分成多个块进行传输。除其他好处外，这还给跟随者一种领导人还活跃的迹象，因此可以重置其选举计时器。每个块均按顺序发送，从而简化了将文件写入磁盘的过程。RPC 包括 Raft 在重新启动时加载快照所需的状态：快照所覆盖的最后一个条目的索引和任期，以及此时的最新配置。</p></blockquote><p>本节的其余部分讨论了基于内存的状态机快照的次要问题：</p><ul><li>第 5.1.1 节讨论了如何在正常操作的同时拍摄快照，以最大程度地减少拍摄快照对客户端的影响。</li><li>第 5.1.2 节讨论何时应该拍摄快照，从而平衡空间的使用和拍摄快照的开销。</li><li>第 5.1.3 节讨论实现快照时出现的问题。</li></ul><h4 id="5-1-1-并发拍摄快照"><a href="#5-1-1-并发拍摄快照" class="headerlink" title="5.1.1 并发拍摄快照"></a>5.1.1 并发拍摄快照</h4><p>创建快照可能需要很长时间，无论是序列化状态还是将其写入磁盘。例如，在当今的服务器上复制 10GB 内存大约需要 1 秒钟的时间，而对其进行序列化通常将花费更长的时间：即使是固态磁盘也只能在 1 秒钟内写入约 500MB，因此，序列化和写入快照必须与正常操作同时进行，以避免出现可用性缺口。</p><p>幸运的是，写时复制技术允许应用热更新，而不会影响正在写入的快照。有两种解决方法：</p><ul><li>状态机可以用不可变的（功能性）数据结构来支持这一点。因为状态机命令不会修改适当的状态，所以快照任务可以保留对某个先前状态的引用，并将其一起写入快照中。</li><li>或者，可以使用操作系统的写时复制支持（在编程环境允许的情况下）。例如，在 Linux 上，内存状态机可以使用 fork 来复制服务器整个地址空间。然后，子进程可以写出状态机的状态并退出，而父进程则继续为请求提供服务。LogCabin 的实现当前使用了此方法。</li></ul><p>服务器需要额外的内存来并创建快照，应该提前计划和管理这些内存。对于状态机来说，有一个快照文件的流接口是非常重要的，这样快照在拍摄时就不必完全暂存在内存中。尽管如此，写时复制需要的额外内存与快照过程中一直在改变的状态机成比例。此外，由于错误共享，依靠操作系统进行写时复制通常会使用更多的内存（例如，如果两个不相关的数据项恰好位于同一页内存中，即使只有第一项发生了更改，第二项也会重复）。两个不相关的数据项恰好位于内存的同一页上，即使只有第一项发生了更改，第二项也会重复)。在拍摄快照期间内存容量耗尽的不幸事件中，服务器应停止接受新的日志条目，直到完成快照为止。这将暂时牺牲服务器的可用性（集群可能仍保持可用状态），但至少允许服务器恢复。最好不要中止快照并稍后重试，因为下一次尝试也可能会遇到相同的问题。（LogCabin 实现了流接口，但目前还不能优雅地处理内存耗尽问题。）</p><h4 id="5-1-2-何时拍摄快照"><a href="#5-1-2-何时拍摄快照" class="headerlink" title="5.1.2 何时拍摄快照"></a>5.1.2 何时拍摄快照</h4><p>服务器必须决定何时进行快照。如果服务器快照太频繁，则会浪费磁盘带宽和其他资源；如果快照太不频繁，则可能会耗尽其存储容量，并增加了重新启动期间重放日志所需的时间。</p><p>一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果将此大小设置为比快照的预期大小大得多，那么用于快照的磁盘带宽开销将很小。但是，对于小型状态机，这可能导致不必要的没有被压缩的大量日志。</p><p>更好的方法是将快照的大小与日志的大小进行比较。如果快照比日志小很多倍，则可能值得拍摄快照。但是，在拍摄快照之前计算快照的大小可能既困难又麻烦，给状态机带来了很大的负担，也许需要与实际拍摄快照几乎相同的工作来动态地计算快照的大小。压缩快照文件可以节省空间和带宽，但是很难预测压缩输出的大小。</p><p>幸运的是，使用上一个快照的大小而不是下一个快照的大小可以得到合理的行为。一旦日志的大小超过前面快照的大小乘以一个可配置的扩展因子，服务器就会拍摄快照。扩展因子在磁盘带宽与空间利用率之间进行权衡。例如，扩展因子 4 导致约 20% 的磁盘的带宽用于快照（对于快照的每 1 字节，将写入 4 字节的日志条目)，并且存储一个快照（旧的快照，日志 4 倍，和新写入的快照）需要大约其 6 倍的磁盘容量。</p><p>快照仍然会导致 CPU 和磁盘带宽的大量使用，这可能会影响客户机的性能。这可以通过增加硬件来缓解；例如，可以使用第二个磁盘驱动器来提供额外的磁盘带宽。</p><p>其实也可能以客户端请求永远不会在正在快照的服务器上等待的方式来调度拍摄快照。在这种方法中，服务器将进行协调，以便在任何时候（如果可能）仅对集群中的少数服务器拍摄快照。由于 Raft 只需要大多数服务器来提交日志条目，因此少数拍摄快照的服务器通常不会对客户端产生不利影响。当领导者希望拍摄快照时，它将首先退出，从而允许另一台服务器来管理集群。如果此方法足够可靠，则还可以消除并发拍摄快照的需求；服务器拍摄快照时可能只是不可用（尽管它们将计入集群掩盖故障的能力）。这对于将来的工作来说是一个令人兴奋的机会，因为它有可能提高整体系统性能并减少机制。</p><h4 id="5-1-3-实现问题"><a href="#5-1-3-实现问题" class="headerlink" title="5.1.3 实现问题"></a>5.1.3 实现问题</h4><p>本节回顾了拍摄快照实现所需的主要组件，并讨论了实现它们的困难：</p><ul><li><p>保存和加载快照：保存快照涉及序列化状态机的状态并将该数据写到文件中，而加载则是相反的过程。尽管从各种类型的数据对象的原型表示中序列化它们有些繁琐，我们发现这还是相当简单。从状态机到磁盘上文件的流接口有助于避免将整个状态机状态缓冲到内存；压缩流并对其应用校验和也可能是有益的。LogCabin 首先将每个快照写入一个临时文件，然后在写入完成并刷新到磁盘后重命名该文件。这样可以确保没有服务器在启动时加载部分写入的快照。</p></li><li><p>传输快照：传输快照涉及实现 InstallSnapshot RPC 的领导者和跟随者。这非常简单，并且可以与从磁盘中保存和加载快照共享一些代码。传输的性能通常不是很重要（需要此快照的跟随者尚未参与条目的提交，所以可能不需要很快；另一方面，如果集群遭受其他故障，则可能需要跟随者尽快追赶上以恢复可​​用性）。</p></li><li><p>消除不安全的日志访问并丢弃日志条目：我们最初设计 LogCabin 时没有考虑日志压缩，因此代码假定如果日志中存在条目 i，则也将存在条目 1 至 i - 1。考虑日志压缩之后，这不再正确。例如，在确定 AppendEntries RPC 中上一个条目的任期时，该条目可能已被丢弃。在整个代码中去除这些假设需要仔细的推理和测试。如果编译器能够强制每个对日志的访问都处理索引越界的情况，那么在更强大的类型系统的帮助下，这将更容易实现。一旦我们确保所有日志访问的安全，丢弃日志前缀就很简单了。在此之前，我们只能单独测试保存、加载和传输快照，但是当日志条目可以安全地丢弃时，这些都可以在系统范围的测试中执行。</p></li><li><p>写时复制并发拍摄快照：并发拍摄快照可能需要重新运行状态机或利用操作系统的 fork 操作。 LogCabin 目前使用了 fork，它与线程和 C++ 析构函数的交互性很差。要使其正常工作会带来一些困难。但是，它只需要少量代码，并且完全不需要修改状态机的数据结构，因此我们认为这是正确的方法。</p></li><li><p>决定何时拍摄快照:我们建议在开发期间应用每个日志条目后进行快照，因为这有助于快速捕获 bug 。一旦实现完成后，就应该添加一个更有用的拍摄快照策略（例如，使用有关 Raft 日志大小和最后一个快照大小的统计信息）。</p></li></ul><p>我们发现快照的分段开发和测试是一个挑战。在可以丢弃日志条目之前，这些组件中的大多数必须已经就位，但只有在此之后，才会在系统范围的测试中执行许多新的代码路径。因此，实现者应该仔细考虑实现和测试这些组件的顺序。</p><h3 id="5-2-基于磁盘的状态机快照"><a href="#5-2-基于磁盘的状态机快照" class="headerlink" title="5.2 基于磁盘的状态机快照"></a>5.2 基于磁盘的状态机快照</h3><p>本节讨论了使用磁盘作为其主要记录位置的大型状态机（数十或数百 GB）的快照方法。这些状态机的行为有所不同，它们总是在磁盘上准备好状态的副本，以防崩溃。应用 Raft 日志中的每个条目都会更改磁盘上的状态，并有效地获得新的快照。因此，一旦应用了条目，就可以从 Raft 日志中将其丢弃。（状态机还可以缓冲内存中的写操作，以期提高磁盘效率；一旦将它们写入磁盘，相应的条目就可以从 Raft 日志中丢弃。）</p><p>基于磁盘的状态机的主要问题是改变磁盘上的状态会导致性能下降。如果没有写缓冲，那么每个命令应用时都需要一个或多个随机磁盘写，这会限制系统的总体写吞吐量（而写缓冲可能也没有多大帮助）。第 5.3 节讨论了日志压缩的增量方法，该方法可通过大量顺序写更有效地将数据写入磁盘。</p><p>基于磁盘的状态机必须能够提供一直的磁盘快照，以便将其传输给缓慢的跟随者。尽管他们总是在磁盘上有快照，但他们也在不断对其进行修改。因此，他们仍然需要写时复制技术，以便在足够长的时间内保持一致的快照，以便传输它。幸运的是，磁盘格式几乎总是划分为逻辑块，因此在状态机中实现写时复制应该很简单。基于磁盘的状态机也可以依赖于操作系统支持来获取快照。例如，Linux 上的 LVM（逻辑卷管理）可用于创建整个磁盘分区的快照，另外一些最新的文件系统允许快照单个目录。</p><p>复制磁盘映像的快照可能会花费很长时间，并且随着对磁盘修改的积累，保留快照所需的额外磁盘使用量也会增加。尽管我们尚未实现基于磁盘的快照，但我们推测基于磁盘的状态机可以通过以下算法传输其磁盘内容来避免大部分此类开销：</p><ol><li>对于每个磁盘块，跟踪其上次修改的时间。</li><li>在继续正常操作的同时，将整个磁盘内容逐块传输到跟随者。在此过程中，领导者上没有使用额外的磁盘空间。由于块是并发修改的，因此很可能导致跟随者磁盘上的磁盘映像不一致。当每个块从领导者转移时，注意它最后的修改时间。</li><li>拍摄磁盘内容的写时复制快照。一旦采取了这种措施，领导者将拥有其磁盘内容的一致性副本，但是由于客户端的持续操作，对磁盘的修改会使用额外的磁盘空间。</li><li>仅重新传输在步骤 2 中首次传输它们与在步骤 3 中拍摄快照之间所修改的磁盘块。</li></ol><p>我们希望一致性快照的大多数块在步骤 3 拍摄快照前已经被传输。如果是这种情况，步骤 4 中的传输将很快进行：在步骤 4 中用于保留领导者快照的额外磁盘容量将很少，而且在步骤 4 中用于再次传输修改磁盘块的额外网络带宽也很低。</p><h3 id="5-3-增量清理方法"><a href="#5-3-增量清理方法" class="headerlink" title="5.3 增量清理方法"></a>5.3 增量清理方法</h3><p>也可以采用增量方法进行压缩，例如日志清理和日志结构合并树（LSM 树）。 尽管它们比快照更复杂，但增量方法具有一些理想的特性：</p><ul><li>它们一次只处理一部分数据，因此它们会随时间平均分配压缩负载。</li><li>无论是非正常操作还是压缩期间，它们都可以高效地写入磁盘。在两种情况下，它们都使用了大量顺序写。增量方法还选择性地压缩了磁盘中可回收空间最大的部分，因此与基于内存的状态机快照（在每个快照上重写所有磁盘）相比，它们向磁盘写入的数据更少。</li><li>他们可以很轻松地传输一致的状态快照，因为它们不会修改磁盘区域。</li></ul><p>第 5.3.1 节和第 5.3.2 节首先介绍了日志清理和 LSM 树的基本知识。然后，第 5.3.3 节讨论了如何将它们应用于 Raft。</p><h4 id="5-3-1-日志清理基础"><a href="#5-3-1-日志清理基础" class="headerlink" title="5.3.1 日志清理基础"></a>5.3.1 日志清理基础</h4><p>日志清理是在日志结构文件系统的上下文中引入的，最近已提出用于内存存储系统（例如 RAMCloud）的方法。原则上，日志清理可用于任何类型的数据结构，尽管某些类型的数据结构比其他类型的数据结构更难有效地实现。</p><p>日志清理功能将日志保留为系统状态的记录位置。该布局针对顺序写进行了优化，并高效的实现了随机读。因此，需要索引结构来定位要读取的数据项。</p><p>在日志清理时，会将日志分为称为段的连续区域。日志清理器的每次都使用三步算法来压缩日志：</p><ol><li>首先选择要清除的段，这些段已累积了大量的过期条目。</li><li>然后，将活动条目（那些有助于当前系统状态的活动条目）从这些段复制到日志的头部。</li><li>最后，它释放了这些段的存储空间，从而使该空间可用于新段。</li></ol><p>为了最小化对正常运行的影响，这个过程并发执行。</p><p>将活动条目复制到日志开头的结果是，条目无法按顺序进行重放。这些条目可以包含其他信息（例如版本号），以便在应用日志时重新创建正确的顺序。</p><p>选择清理哪一部分的策略对性能有很大的影响；先前的研究提出了一种成本效益策略，该策略不仅考虑了活动条目所使用的空间数量，而且还考虑了这些条目可能保持活动的时间。</p><p>确定条目是否是活动的是状态机的职责。例如，在键值存储中，如果键存在并且当前已设置为给定值，则用于将键设置为特定值的日志条目是活动的。确定删除键的日志条目是否是活动的则更加微妙：只要在日志中存在设置该键的任何先前条目，该日志条目就是活动的。RAMCloud 根据需要保留删除命令（称为逻辑删除命令），但是另一种方法是定期写出当前状态下存在的键的摘要，然后所有未列出的键的日志条目都不是活动的。键值存储是一个相当简单的例子；其他状态机是可能确定活动性的，但不幸的是，其方法各不相同。</p><h4 id="5-3-2-日志结构合并树基础"><a href="#5-3-2-日志结构合并树基础" class="headerlink" title="5.3.2 日志结构合并树基础"></a>5.3.2 日志结构合并树基础</h4><p>日志结构合并树（LSM 树）最初由 O’Neil 描述，后来通过 BigTable 在分布式系统中流行。 它们用在诸如 Apache Cassandra 和 HyperDex 之类的系统中，并且可以作为诸如 LevelDB 及其分支（例如 RocksDB 和 HyperLevelDB ）之类的系统使用。</p><p>LSM 树是存储有序键值对的树状数据结构。在较高的层次上看，他们使用磁盘的方式类似于日志清理方法：它们会进行大量顺序写，并且不会就地修改磁盘上的数据。 但是，LSM 树不维护日志中的所有状态，而是重新组织状态以实现更好的随机访问。</p><p>典型的 LSM 树将最近写入的键保留在磁盘上的小日志中。当日志达到固定大小时，将按键对日志进行排序，并按排序顺序将其写入称为 Run 的文件。Run 永远不会被修改，但压缩过程会定期将多个R un 合并在一起，从而产生新的 Run 并丢弃旧的 Run。合并让人想起归并排序；当一个键在多个输入 Run 中时，仅保留最新版本，因此生成的 Run 更加紧凑。图 5.1 总结了 LevelDB 中使用的压缩策略。它按年龄区分运行以提高效率（类似于日志清理）。</p><p>在正常运行期间，状态机可以直接对此数据进行操作。要读取键，它首先检查该键是否最近在其日志中被修改，然后检查每个 Run。为了避免在每次查找 Run 是否有键时实行遍历，某些系统会为每个 Run 创建一个布隆过滤器（紧凑的数据结构，在某些情况下可以肯定地说某个键在 Run 中不存在，尽管有时即使键不存在也可能需要搜索 Run）。</p><h4 id="5-3-3-Raft-中的日志清理和日志结构化合并树"><a href="#5-3-3-Raft-中的日志清理和日志结构化合并树" class="headerlink" title="5.3.3 Raft 中的日志清理和日志结构化合并树"></a>5.3.3 Raft 中的日志清理和日志结构化合并树</h4><p>我们还没有尝试在 Raft 中实现日志清理或 LSM 树，但是我们推测两者都可以工作的很好。将 LSM 树应用于 Raft 似乎相当简单。因为 Raft 日志已经将最近的条目持久化到磁盘上，所以 LSM 树可以将最近的数据以更方便的树格式保存在内存中。这对于服务查找将是快速的，并且当 Raft 日志达到固定大小时，树已经排好序，可以作为新的 Run 写入磁盘。将状态从领导者转移到缓慢的跟随者需要将所有 Run 发送到跟随者（但不包括内存树）；幸运的是，Run 是不可变的，因此不必担心在传输期间会修改 Run。</p><p>对 Raft 进行日志清理则不是很明确。我们首先考虑了一种方法，其中将 Raft 分为几段并进行清理（见图 5.4（a））。不幸的是，清理会在释放段的日志中放置很多空洞，这需要修改日志复制的方法。我们认为这种方法是可行的，但是它给 Raft 及其与状态机的交互增加了极大的复杂性。此外，由于只有领导者可以追加到 Raft 日志中，因此清理工作必须基于领导者，这将浪费领导者的网络带宽（这将在 5.4 节中进一步讨论）。</p><blockquote><p><img src="/raft-translate/5_4.png" srcset="/img/loading.gif" alt><br>图 5.4：在 Raft 中进行日志清理的两种可能方法。</p></blockquote><p>更好的方法是类似于 LSM 树来处理日志清理：Raft 将最近的更改保留为连续的日志，而状态机将其自身的状态保留为日志，但是这些日志在逻辑上是不同的（请参见图 5.4（b））。当 Raft 日志增长到固定大小时，它的新条目将被写入状态机日志中的一个新段，并且 Raft 日志的相应前缀前的日志将被丢弃。状态机中的段将在每台服务器上独立清理，并且 Raft 日志将完全不受此影响。与直接清除 Raft 日志相比，我们更喜欢这种方法，因为日志清除的复杂性完全封装在状态机中（状态机和 Raft 之间的接口仍然很简单），并且服务器可以独立进行清理。</p><p>如上所述，这种方法将要求状态机将所有 Raft 的日志条目写入其自己的日志中（尽管它可以大批量地这样做）。可以通过直接从 Raft 日志中移动包含日志条目的文件，并将该文件合并到状态机的数据结构中，从而优化掉这个额外的副本。这对于性能至关重要的系统可能是有用的优化，但是不幸的是，它将更紧密地耦合状态机模块和 Raft 模块，因为状态机将需要了解 Raft 日志的磁盘表示。</p><h3 id="5-4-备选方案：基于领导者的方法"><a href="#5-4-备选方案：基于领导者的方法" class="headerlink" title="5.4 备选方案：基于领导者的方法"></a>5.4 备选方案：基于领导者的方法</h3><p>本章介绍的日志压缩方法有别于 Raft 强有力的领导者原则，因为服务器可以在领导者不知情的情况下压缩日志。但是，我们认为这种背离是合理的。尽管拥有领导者可以避免在达成共识时出现冲突的决策，但是在拍摄快照时已经达成共识，因此不会有决策冲突。数据仍然仅从领导者流向跟随者，但是跟随者现在可以独立地重组其数据。</p><p>我们还考虑了基于领导者的日志压缩方法，但是任何好处通常都被性能考虑所抵消。领导者压缩它的日志，然后将结果发送给跟随者，这是很浪费的，因为他们也可以独立地压缩自己的日志。将冗余状态发送给每个跟随者将浪费网络带宽并减慢压缩过程。每个跟随者已经掌握了压缩自身状态所需的信息，而领导者的向外网络带宽通常是 Raft 最宝贵的资源（瓶颈）。对于基于内存的快照，从服务器本地状态拍摄快照通常比通过网络发送和接收快照要节约资源得多。对于增量压缩方法，这更多地取决于硬件配置，但是我们也认为独立压缩会更节约资源。</p><h4 id="5-4-1-将快照存储在日志中"><a href="#5-4-1-将快照存储在日志中" class="headerlink" title="5.4.1 将快照存储在日志中"></a>5.4.1 将快照存储在日志中</h4><p>基于领导者的方法的一个可能好处是，如果所有系统状态都可以存储在日志中，那么就不需要新的机制来复制和保持状态。因此，我们考虑了一种基于领导者的快照方法，在这种方法中，领导者将创建快照并将快照存储为 Raft 日志中的条目，如图 5.5 所示。领导者随后将使用 AppendEntries RPC 将快照发送给其每个跟随者。为了减少对正常操作的任何干扰，每个快照将被分成许多条目，并与日志中的普通客户端命令交织在一起。</p><blockquote><p><img src="/raft-translate/5_5.png" srcset="/img/loading.gif" alt><br>图 5.5：基于领导者的方法将快照存储在日志中的块中，与客户端请求交错。快照过程在开始条目处开始，并在结束条目处完成。快照存储在开始和结束之间的多个日志条目中。因此，客户端请求可以与快照并行进行，每个条目的大小受到限制，并且条目附加到日志的速率受到限制：只有当领导者获悉前一个快照块已经提交时，下一个快照块才会继续附加日志。每个服务器得知结束条目已提交后，就可以丢弃其日志中的条目直到对应的开始条目为止。重放日志需要一个两步算法：首先应用最后一个完整的快照，然后再应用快照开始条目之后的客户端请求。</p></blockquote><p>与将快照存储在日志之外相比，这将实现更好的机制经济性，因为服务器不需要单独的机制来传输或持久化快照（它们将像其他日志条目一样被复制并持久化）。但是这些跟随者可能很容易生成自己的快照，所以这样会浪费跟随者的网络带宽，除此以外这还有一个严重的问题，如果领导者在创建快照的过程中失败，则它将部分快照保留在服务器的日志中。原则上，这种情况可能会反复发生，并且会耗尽服务器的存储容量，并会因无数次失败的快照尝试而积累起来。因此，我们认为这种机制在实践中不可行。</p><h4 id="5-4-2-对于非常小的状态机使用基于领导者的方法"><a href="#5-4-2-对于非常小的状态机使用基于领导者的方法" class="headerlink" title="5.4.2 对于非常小的状态机使用基于领导者的方法"></a>5.4.2 对于非常小的状态机使用基于领导者的方法</h4><p>对于非常小的状态机，将快照存储在日志中不仅可行，而且还可以大大简化。如果快照足够小（最多约 1M 字节），则可以轻松地将其放入单个日志条目中，而不会中断正常操作时间太长。为了以这种方式压缩服务器的日志，领导者需要：</p><ol><li>停止接受新的客户请求。</li><li>等待提交其日志中的所有条目，并等待其状态机应用其日志中的所有条目。</li><li>（同步）拍摄快照。</li><li>将快照追加到其日志末尾的单个日志条目中。</li><li>恢复接受新的客户请求。</li></ol><p>一旦每个服务器都知道快照条目已提交，它就可以丢弃其日志中快照之前的每个条目。这种方法会在客户端请求被停止和快照条目被传输时造成一个小的可用性缺口，但是它对非常小的状态机的影响是有限的。</p><p>这种更简单的方法避免了在日志之外持久化快照，使用新的 RPC 传输快照以及并发快照的实现工作。然而，成功的系统往往比它们最初设计者所期望的使用得更多，这种方法对于大型的状态机并不适用。</p><h3 id="5-5-结论"><a href="#5-5-结论" class="headerlink" title="5.5 结论"></a>5.5 结论</h3><p>本章讨论了 Raft 中日志压缩的几种方法，图 5.1 对此进行了概述。不同的方法适用于不同的系统，具体取决于状态机的大小，所需的性能水平以及预算的复杂程度。Raft 支持具有多种共享同一概念框架的方法：</p><ul><li>每个服务器都独立压缩其日志的提交前缀。</li><li>状态机和 Raft 之间的基本交互涉及将日志前缀的责任从 Raft 转移到状态机。一旦状态机将命令应用到磁盘上，它就会指示 Raft 放弃日志的相应前缀前的日志。Raft 会保留它最后丢弃条目的索引和任期，以及该索引的最新配置。</li><li>一旦 Raft 丢弃了日志前缀前的日志，状态机将承担两项新的职责：在重新启动时加载状态，提供一致的映像传输给缓慢的跟随者。</li></ul><p>基于内存的状态机快照已在包括 Chubby 和 ZooKeeper 在内的多个生产系统中成功使用，并且我们已在 LogCabin 中实现了这种方法。尽管对大多数操作而言，在内存数据结构上进行操作很快速，但快照过程中的性能可能会受到很大的影响。并发拍摄快照有助于隐藏资源使用情况，将来，在集群中调度服务器在不同时间进行快照可能会使快照过程完全不影响客户端。</p><p>在适当位置改变其状态的基于磁盘的状态机在概念上很简单。他们仍然需要写时复制技术来将一致的磁盘映像传输到其他服务器，但这对于磁盘来说可能是一个很小的负担，因为磁盘自然会分成多个块。但是，正常操作期间的磁盘随机写速度通常会很慢，因此这种方法将限制系统的写入吞吐量。</p><p>最终，增量方法可能是最有效的压缩形式。通过一次对状态的小片段进行操作，它们可以限制资源使用的突然增加（并且还可以并发压缩）。它们还可以避免将相同的数据重复写入磁盘。稳定的数据应该进入不经常压缩的磁盘区域。尽管实现增量压缩可能很复杂，但是可以将这种复杂性转移到诸如 LevelDB 之类的库中。此外，通过将数据结构保留在内存中并在内存中缓存更多磁盘，具有增量压缩功能的客户端操作的性能可以接近基于内存的状态机。</p>]]></content>
    
    
    <categories>
      
      <category>分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 协议介绍</title>
    <link href="/raft/"/>
    <url>/raft/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="共识算法"><a href="#共识算法" class="headerlink" title="共识算法"></a>共识算法</h3><p>共识算法允许一组节点像一个整体一样一起工作，即使其中一些节点出现故障也能够继续工作下去。更详细点，可以认为每一个节点上都运行着一个状态机和一组日志。在客户端提交命令后，状态机负责执行命令并返回结果，同时还可能改变自己的状态；日志则记录了所有可能会影响到状态机的命令。任何一个处于初始状态的状态机都可以通过重新按顺序执行这组日志，来让自己恢复到最终状态。</p><p>共识算法常被用来确保每一个节点上的状态机一定都会按相同的顺序执行相同的命令， 并且最终会处于相同的状态。换句话说，可以理解为共识算法就是用来确保每个节点上的日志顺序都是一致的。（不过需要注意的是，只确保“提交给状态机的日志”顺序是一致的，而有些日志项可能只是暂时添加，尚未决定要提交给状态机）。正因为如此，共识算法在构建可容错的大规模分布式系统中扮演着重要的角色。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_rsm.png" srcset="/img/loading.gif" alt></p><p>上图就是每个节点上的状态机，日志和同步模块与客户端交互的过程。</p><p>当然，实际使用系统中的共识算法一般满足以下特性：</p><ul><li>在非拜占庭条件（无恶意欺骗）下保证共识的一致性；</li><li>在多数节点存活时，保持可用性；</li><li>不依赖于时间，错误的时钟和高延迟只会导致可用性问题，而不会导致一致性问题；</li><li>在多数节点一致后就返回结果，而不会受到个别慢节点的影响。<br>（注：“多数”永远指的是配置文件中所有节点的多数，而不是存活节点的多数）<br>（注：非拜占庭条件，指的就是每一个节点都是诚实可信的，每一次信息的传递都是真实的且符合协议要求的，当节点无法满足协议所要求的条件时，就停止服务，节点仅会因为网络延迟或崩溃出现不一致，而不会有节点传递错误的数据或故意捏造假数据。）</li></ul><h3 id="Raft-的由来与宗旨"><a href="#Raft-的由来与宗旨" class="headerlink" title="Raft 的由来与宗旨"></a>Raft 的由来与宗旨</h3><p>众所周知，Paxos 是一个非常划时代的共识算法。在 Raft 出现之前的 10 年里，Paxos 几乎统治着共识算法这一领域：因为绝大多数共识算法的实现都是基于 Paxos 或者受其影响，同时 Paxos 也成为了教学领域里讲解共识问题时的示例。</p><p>但是不幸的是，尽管有很多工作都在尝试降低 Paxos 的复杂性，但是它依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。比如 <code>Google Chubby</code> 的论文就提到，因为 Paxos 的描述和现实差距太大，所以最终人们总会实现一套未经证实的类 Paxos 协议。</p><p>基于以上背景，<code>Diego Ongaro</code> 在就读博士期间，深入研究 Paxos 协议后提出了 Raft 协议，旨在提供更为易于理解的共识算法。Raft 的宗旨在于可实践性和可理解性，并且相比 Paxos 几乎没有牺牲多少性能。</p><blockquote><p>趣闻：<a href="https://groups.google.com/forum/#!topic/raft-dev/95rZqptGpmU" target="_blank" rel="noopener">Raft 名字的来源</a>。简而言之，其名字即来自于 <code>R{eliable|plicated|dundant} And Fault-Tolerant</code>， 也来自于这是一艘可以帮助你逃离 Paxos 小岛的救生筏（Raft）。</p></blockquote><h3 id="工业界的实现"><a href="#工业界的实现" class="headerlink" title="工业界的实现"></a>工业界的实现</h3><ul><li><code>Tidb</code></li><li><code>Consul</code></li><li><code>etcd</code></li><li>…</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>这一部分会简单介绍 Raft 的一些基本概念。若暂时没看懂并没有关系，后面会一一介绍清楚，带着问题耐心读完此博客即可。</p><h3 id="Raft-的子问题"><a href="#Raft-的子问题" class="headerlink" title="Raft 的子问题"></a>Raft 的子问题</h3><p>Raft 将共识算法这个难解决的问题分解成了多个易解决，相对独立的子问题，这些问题都会在接下来的章节中进行介绍。</p><ul><li><code>选主</code>：选出集群的 leader 来统筹全局。</li><li><code>日志同步</code>：leader 负责从客户端接收请求，并且在集群中扩散同步。</li><li><code>安全</code>：各节点间状态机的一致性保证。</li></ul><p>在完整的论文和 etcd 实现中，其实还有一些问题：</p><ul><li><code>配置变更</code>：集群动态增删节点。</li><li><code>禅让</code>：能够将 leader 迅速转给另一个 follower。</li><li><code>Pre-Vote</code>：在竞选开始时先进行一轮申请，若被允许再转变为 candidate，这样有助于防止某些异常节点扰乱整个集群的正常工作。</li><li>…</li></ul><h3 id="Raft-的节点类型"><a href="#Raft-的节点类型" class="headerlink" title="Raft 的节点类型"></a>Raft 的节点类型</h3><p>Raft 将所有节点分为三个身份：</p><ul><li><code>leader</code>：集群内最多只会有一个 leader，负责发起心跳，响应客户端，创建日志，同步日志。</li><li><code>candidate</code>：leader 选举过程中的临时角色，由 follower 转化而来，发起投票参与竞选。</li><li><code>follower</code>：接受 leader 的心跳和日志同步数据，投票给 candidate。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_state.png" srcset="/img/loading.gif" alt></p><p>上图可以看出 Raft 中节点状态之间变迁的条件。</p><p>在完整的论文和 etcd 实现中，其实又增加了几种中间状态：</p><ul><li><code>Learner</code>：新加入的节点，不具有选举权，需要从 leader 同步完数据后， 才能转变为 follower。严格来说，learner 并不算集群成员。</li><li><code>Pre-Candidate</code>：刚刚发起竞选，还在等待 <code>Pre-Vote</code> 结果的临时状态， 取决于 <code>Pre-Vote</code> 的结果，可能进化为 candidate，可能退化为 follower。<br>（注：此两种节点状态的流程和作用后面章节会介绍，可先无视此两个状态）</li></ul><h3 id="Raft-的节点状态"><a href="#Raft-的节点状态" class="headerlink" title="Raft 的节点状态"></a>Raft 的节点状态</h3><p>每一个节点都应该有的持久化状态：</p><ul><li><code>currentterm</code>：当前任期。</li><li><code>votedFor</code>：在当前 term，给哪个节点投了票，值为 NULL 或 <code>candidate id</code>。</li><li><code>log[]</code>：已经 committed 的日志。</li></ul><p>每一个节点都应该有的可以非持久化的状态：</p><ul><li><code>commitindex</code>：已提交的最大 index。</li><li><code>lastApplied</code>：已被状态机应用的最大 index。<br>（注：这两个不需要持久化是因为状态机本身是非持久化的，而状态机的状态可以通过 log[] 来恢复）</li></ul><p>leader 的非持久化状态：</p><ul><li><code>nextindex[]</code>：为每一个 follower 保存的，应该发送的下一份 <code>entry index</code>；<br>初始化为 Last index + 1。</li><li><code>matchindex[]</code>：已确认的，已经同步到每一个 follower 的 <code>entry index</code><br>初始化为 0，单调递增。<br>（注：每次选举后，都应该立刻重新初始化）</li></ul><h3 id="Raft-的任期概念"><a href="#Raft-的任期概念" class="headerlink" title="Raft 的任期概念"></a>Raft 的任期概念</h3><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_term.png" srcset="/img/loading.gif" alt></p><p>Raft 将时间划分成为任意不同长度的 term。term 用连续的数字进行表示。每一个 term 的开始都是一次选举，一个或多个 candidate 会试图成为 leader。如果一个  candidate 赢得了选举，它就会在该 term 的剩余时间担任 leader。在某些情况下，选票会被瓜分，有可能没有选出 leader，那么，将会开始另一个 term，并且立刻开始下一次选举。Raft 保证在给定的一个 term 最多只有一个 leader。</p><p>不同的服务器节点可能多次观察到 term 之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个 term 全程。term 在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如过期的 leader。</p><p>每个节点都会存储当前 term 号，这一编号在整个时间内单调增长。当服务器之间通信的时候会交换当前 term 号；如果一个服务器的当前 term 号比其他人小，那么他会更新自己的 term 到较大的 term 值。如果一个 candidate 或者 leader 发现自己的 term 过期了，那么他会立即退回 follower。如果一个节点接收到一个包含过期 term 号的请求，那么它会直接拒绝这个请求。</p><h3 id="Raft-的日志组成"><a href="#Raft-的日志组成" class="headerlink" title="Raft 的日志组成"></a>Raft 的日志组成</h3><ul><li><p><code>entry</code>：Raft 中，将每一个事件都称为一个 entry，每一个 entry 都有一个表明它在 log 中位置的 index（之所以从 1 开始是为了方便 <code>prevLogIndex</code> 从 0 开始）。只有 leader 可以创建 entry。entry 的内容为 <code>&lt;term, index, cmd&gt;</code>，其中 cmd 是可以应用到状态机的操作。被提交给状态机后，entry 被称为是 committed 的。</p></li><li><p><code>log</code>：由 entry 构成的数组，只有 leader 可以改变其他节点的 log。 entry 总是先被添加进 log（写操作都应该立刻持久化），然后才发起共识请求，通过后才会被 leader 提交给状态机。follower 只能从 leader 那获取到当前已经 commit 日志的最大索引号，然后应用到自己的状态机。</p></li></ul><h3 id="Raft-的保证"><a href="#Raft-的保证" class="headerlink" title="Raft 的保证"></a>Raft 的保证</h3><ul><li><code>Election Safety</code>：最多只会有一个 leader。</li><li><code>Leader Append-Only</code>：leader 的日志是只增的。</li><li><code>Log Matching</code>：如果两个节点的日志中有两个 entry 有相同的 index 和 term，那么它们就是相同的 entry。</li><li><code>Leader Completeness</code>：一旦一个操作被提交了，那么在之后的 term 中，该操作都会存在于日志中。</li><li><code>State Machine Safety</code>：一致性，一旦一个节点应用了某个 index 的操作到状态机，那么其他所有节点应用的该 index 的操作都是一致的。</li></ul><h2 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h2><p>Raft 使用心跳来维持 leader 身份。任何节点都以 follower 的身份启动。 leader 会定期的发送心跳给所有的 followers 以确保自己的身份。 每当 follower 收到心跳后，就刷新自己的 electionElapsed，重新计时。</p><p>（后文中，会将预设的选举超时称为 electionTimeout，而将当前经过的选举耗时称为 electionElapsed。）</p><p>一旦一个 follower 在指定的时间内没有收到任何 RPC（称为 electionTimeout），则会发起一次选举。 当 follower 试图发起选举后，其身份转变为 candidate，在增加自己的 term 后， 会向所有节点发起 RequestVoteRPC 请求，candidate 的状态会一直持续直到：</p><ul><li>赢得选举</li><li>其他节点赢得选举</li><li>一轮选举结束，无人胜出</li></ul><p>选举的方式非常简单，谁能获取到多数选票 <code>(N/2 + 1)</code>，谁就成为 leader。 在一个 candidate 节点等待投票响应的时候，它有可能会收到其他节点声明自己是 leader 的心跳， 此时有两种情况：</p><ul><li>该请求的 term 和自己一样或更大：说明对方已经成为 leader，自己立刻退为 follower；</li><li>该请求的 term 小于自己：拒绝请求。</li></ul><p>在 etcd 的实现中，如果 candidate 收到 term 大于自己的 RequestVote，也会退为 follower。 （准确的说是，收到一切 term 更大的，除了 RequestPreVote、PreVoteResp 外的所有消息，都退为 follower）。</p><p>follower 收到 candidate 的 RequestVote 后，会检查自己是否已经投过票。 不过如果来源于同一个 candidate，那么 follower 可以在同一 term 内多次投给同一个 candidate。</p><pre><code class="GO">// 检查自己是否已经投过票了，如果投票请求来自同一节点，可以重复投票。// We can vote if this is a repeat of a vote we&#39;ve already cast...canVote := r.Vote == m.From ||    // ...we haven&#39;t voted and we don&#39;t think there&#39;s a leader yet in this term...    (r.Vote == None &amp;&amp; r.lead == None) ||    // ...or this is a PreVote for a future term...    (m.Type == pb.MsgPreVote &amp;&amp; m.Term &gt; r.Term)</code></pre><p>为了防止在同一时间有太多的 follower 转变为 candidate 导致无法选出绝对多数， Raft 采用了随机选举超时（<code>randomized election timeouts</code>）的机制， 每一个 candidate 在发起选举后，都会记录一个选举超时（在 <code>150-300ms</code> 间）， 一旦超时后仍然没有完成选举，则增加自己的 term，然后发起新一轮选举。 在这种情况下，应该能在较短的时间内确认出 leader。 （因为 term 较大的有更大的概率压倒其他节点）</p><p>etcd 中将随机选举超时设置为 <code>[electiontimeout, 2 * electiontimeout - 1]</code>。</p><p>如果一个 leader 在 electionTimeout 内无法完成一次多数节点的 heartbeat， 说明该 leader 很可能已经与集群失去联系了，那么该 leader 应该向所有的客户端请求返回 fail， 并且退回到 follower。</p><h2 id="日志同步"><a href="#日志同步" class="headerlink" title="日志同步"></a>日志同步</h2><p>leader 被选举后，则负责所有的客户端请求。每一个客户端请求都包含一个命令，该命令可以被作用到 RSM。</p><p>leader 收到客户端请求后，会生成一个 entry，包含 &lt;index, term number, cmd&gt;。 在将这个 entry 添加到自己的日志末尾后，向所有的节点广播该 entry。</p><p>follower 如果同意接受该 entry，则在将 entry 添加到自己的日志后，返回同意。</p><p>如果 leader 收到了多数的成功答复，则将该 entry 应用到自己的 RSM， 之后，可以称该 entry 是 committed 的。该 committed 信息会随着 AppendEntriesRPC 被传达到其他节点。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_log.png" srcset="/img/loading.gif" alt></p><p>Raft 保证下列两个性质：</p><p>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd；<br>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们前面的 entry 也一定相同。<br>通过“仅有 leader 可以生成 entry”来确保第一个性质， 第二个性质则通过一致性检查（consistency check）来保证，该检查包含几个步骤：</p><p>leader 在通过 AppendEntriesRPC 和 follower 通讯时，会带上上一块 entry 的信息， 而 follower 在收到后会对比自己的日志 ， 如果发现这个 entry 的信息（index、term）和自己日志内的不符合，则会拒绝该请求。<br>一旦 leader 发现有 follower 拒绝了请求，则会与该 follower 再进行一轮一致性检查， 找到双方最大的共识点，然后用 leader 的 entries 记录覆盖 follower 所有在最大共识点之后的数据。</p><p>寻找共识点时，leader 还是通过 AppendEntriesRPC 和 follower 进行一致性检查， 方法是发送再上一块的 entry， 如果 follower 依然拒绝，则 leader 再尝试发送更前面的一块，直到找到双方的共识点。 因为分歧发生的概率较低，而且一般很快能够得到纠正，所以这里的逐块确认一般不会造成性能问题。<br>每个 leader 都会为每一个 follower 保存一个 nextIndex 的变量， 标志了下一个需要发送给该 follower 的 entry 的 index。 在 leader 刚当选时，该值初始化为该 leader 的 log 的 index+1。 一旦 follower 拒绝了 entry，则 leader 会执行 nextIndex–，然后再次发送。</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><h3 id="选举限制"><a href="#选举限制" class="headerlink" title="选举限制"></a>选举限制</h3><p>因为 leader 的强势地位，所以 Raft 在投票阶段就确保选举出的 leader 一定包含了整个集群中目前已 committed 的所有日志。</p><p>当 candidate 发送 RequestVoteRPC 时，会带上最后一个 entry 的信息。 所有的节点收到该请求后，都会比对自己的日志，如果发现自己的日志更新一些，则会拒绝投票给该 candidate。 （Pre-Vote 同理，如果 follower 认为 Pre-Candidate 没有资格的话，会拒绝 PreVote）</p><p>判断日志新旧的方式：获取请求的 entry 后，比对自己日志中的最后一个 entry。 首先比对 term，如果自己的 term 更大，则拒绝请求。 如果 term 一样，则比对 index，如果自己的 index 更大（说明自己的日志更长），则拒绝请求。</p><pre><code class="GO">func (l *raftLog) isUpToDate(lasti, term uint64) bool {    return term &gt; l.lastTerm() || (term == l.lastTerm() &amp;&amp; lasti &gt;= l.lastIndex())}</code></pre><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_leader_restriction.png" srcset="/img/loading.gif" alt></p><p>在上图中，raft 为了避免出现一致性问题，要求 leader 绝不会提交过去的 term 的 entry （即使该 entry 已经被复制到了多数节点上）。leader 永远只提交当前 term 的 entry， 过去的 entry 只会随着当前的 entry 被一并提交。（上图中的 c，term2 只会跟随 term4 被提交。）</p><p>如果一个 candidate 能取得多数同意，说明它的日志已经是多数节点中最完备的， 那么也就可以认为该 candidate 已经包含了整个集群的所有 committed entries。</p><p>因此 leader 当选后，应该立刻发起 AppendEntriesRPC 提交一个 no-op entry。</p><h3 id="节点崩溃"><a href="#节点崩溃" class="headerlink" title="节点崩溃"></a>节点崩溃</h3><p>如果 leader 崩溃，集群中的所有节点在 electionTimeout 时间内没有收到 leader的心跳信息就会触发新一轮的选主。总而言之，最终集群总会选出唯一的 leader 。按论文中的说法，即使一次RPC高达 <code>30～40ms</code> 时，<code>99.9%</code> 的选举依然可以在 <code>3s</code> 内完成，但一般一个机房内一次 RPC 只需 1ms。当然，选主期间整个集群对外是不可用的。 </p><p>如果 follower 和 candidate 奔溃相对而言就简单很多， 因为 Raft 所有的 RPC 都是幂等的，所以 Raft 中所有的请求，只要超时，就会无限的重试。follower 和 candidate 崩溃恢复后，可以收到新的请求，然后按照上面谈论过的追加或拒绝 entry 的方式处理请求。</p><h3 id="时间与可用性"><a href="#时间与可用性" class="headerlink" title="时间与可用性"></a>时间与可用性</h3><p>Raft 原则上可以在绝大部分延迟情况下保证一致性， 不过为了保证选择和 leader 的正常工作，最好能满足下列时间条件：</p><pre><code>broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF</code></pre><ul><li><code>broadcastTime</code>：向其他节点并发发送消息的平均响应时间；</li><li><code>electionTimeout</code>：follower 判定 leader 已经故障的时间（heartbeat 的最长容忍间隔）；</li><li><code>MTBF(mean time between failures)</code>：单台机器的平均健康时间；</li></ul><p>一般来说，broadcastTime 一般为 <code>0.5～20ms</code>（需要磁盘持久化），MTBF 一般为一两个月， electionTimeout 可以设置为 <code>10～500ms</code>。</p><h2 id="配置更改"><a href="#配置更改" class="headerlink" title="配置更改"></a>配置更改</h2><h3 id="一次变更一台"><a href="#一次变更一台" class="headerlink" title="一次变更一台"></a>一次变更一台</h3><h4 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h4><p>因为在 Raft 算法中，集群中每一个节点都存有整个集群的信息，而集群的成员有可能会发生变更（节点增删、替换节点等）。 Raft 限制一次性只能增／删一个节点，在一次变更结束后，才能继续进行下一次变更。</p><p>如果一次性只变更一个节点，那么只需要简单的要求“在新／旧集群中，都必须取得多数（N/2+1）”， 那么这两个多数中必然会出现交集，这样就可以保证不会因为配置不一致而导致脑裂。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_singlechange.png" srcset="/img/loading.gif" alt></p><p>当 leader 收到集群变更的请求后，就会生成一个特殊的 entry 项用来保存配置， 在将配置项添加到 log 后，该配置立刻生效（也就是说任何节点在收到新配置后，就立刻启用新配置）。 然后 leader 将该 entry 扩散至多数节点，成功后则提交该 entry。 一旦一个新配置项被 committed，则视为该次变更已结束，可以继续处理下一次变更了。</p><p>为了保证可用性，需要新增一项规则，节点在响应 RPC 时，不考虑来源节点是否在自己的配置文件之中。 也就是说，即使收到了一个并不在自己配置文件之中的节点发来的 RPC， 也需要正常处理和响应，包括 AppendEntriesRPC 和 RequestVoteRPC。</p><h3 id="一次变更多台"><a href="#一次变更多台" class="headerlink" title="一次变更多台"></a>一次变更多台</h3><h4 id="方式-1"><a href="#方式-1" class="headerlink" title="方式"></a>方式</h4><p>这种变更方式可以一次性变更多个节点（arbitrary configuration）。</p><p>当集群成员在变更时，为了保证服务的可用性（不发生中断），以及避免因为节点变更导致的一致性问题， Raft 提出了两阶段变更，当接收到新的配置文件后，集群会首先进入 joint consensus 状态， 待新的配置文件提交成功后，再回到普通状态。</p><p>更具体的，joint consensus 指的是包含新／旧配置文件全部节点的中间状态：</p><ul><li>entries 会被复制到新／旧配置文件中的所有节点；</li><li>新／旧配置文件中的任何一个节点都有可能被选为 leader；</li><li>共识（选举或提交）需要同时在新／旧配置文件中分别获取到多数同意（<code>separate majorities</code>）</li></ul><p>（注：<code>separate majorities</code>的意思是需要新／旧集群中的多数都同意。比如如果是从 3 节点切换为全新的 9 节点， 那么要求旧节点中的 2 节点，和新节点中的 4 节点都同意，才被认为达成了一次共识。）</p><p>所以，在一次配置变更中，一共有三个状态：</p><ul><li><code>C_new</code>：使用旧的配置文件；</li><li><code>C_old,new</code>：同时使用新旧配置文件，也就是新／旧节点的并集；</li><li><code>C_new</code>：使用新的配置文件。</li></ul><p>配置文件使用特殊的 entries 进行存储，一个节点一旦获取到新的配置文件， 即使该配置 entry 并没有 committed，也会立刻使用该配置。 所以一次完整的配置变更可以表示为下图：</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_jointchange.png" srcset="/img/loading.gif" alt></p><ol><li>C_old,new 被创建，集群进入 joint consensus，leader 开始传播该 entry；</li><li>C_old,new 被 committed，也就是说此时多数节点都拥有了 C_old,new，此后 C_old 已经不再可能被选为 leader；</li><li>leader 创建并传播 C_new；</li><li>C_new 被提交，此后不在 C_new 内的节点不允许被选为 leader，如有 leader 不在 C_new 则自行退位。</li></ol><h4 id="删除当前节点的有趣现象"><a href="#删除当前节点的有趣现象" class="headerlink" title="删除当前节点的有趣现象"></a>删除当前节点的有趣现象</h4><p>leader 可能不在新配置文件的节点之中</p><p>在原始论文中，leader 会持续工作直到 C_new 被提交。</p><p>当 C_new 被 committed 后，任何不在 C_new 中的 leader，立刻退化为 follower。 在试图提交 C_new 时，不在 C_new 的 leader 不参与计票。</p><p>在其博士论文中，当一个 leader 发现自己不在新配置文件中，在 C_new 提交后， 可以采取 3.10 节（p46）提到的 leadership transfer 机制，交出自己的管理权。</p><p>上述两种做法都导致两个很奇特的现象：</p><ul><li>这个 leader 可能会管理一个不包括自己的集群；</li><li>一个服务器可能在自己的配置文件都不包含自己的情况下参与选举，甚至成为 leader。</li></ul><h2 id="日志打包"><a href="#日志打包" class="headerlink" title="日志打包"></a>日志打包</h2><p>当日志 entries 数量过多时，节点间同步会耗费太多时间，最简单的优化办法就是定期做 snapshot。</p><p>snapshot 会包括：</p><ul><li>状态机当前的状态；</li><li>最后一块 entry 的 index 和 term（为了兼容其他 RPC 请求的参数）；<br>当前集群配置信息。</li><li>各个节点自行择机完成自己的 snapshot。</li></ul><p>如果 leader 发现需要发给某一个 follower 的 nextIndex 已经被做成了 snapshot， 则需要将 snapshot 发送给该 follower。</p><p>当 follower 接收到 snapshot 后，需要做出判断：</p><ul><li>如果 snapshot 领先于自己的 log，则使用 snapshot 完全替换自己的所有的 log；</li><li>如果 snapshot 落后于自己的 log，则使用 snapshot 替换掉该部分的 log，而保留后续的 log。</li></ul><p>snapshot 可能会带来两个问题：</p><ol><li><p>何时 snapshot？<br>一个简单的策略是设置一个固定的最大磁盘容量，当 log 超过这个容量时，就触发 snapshot。</p></li><li><p>对状态机写 snapshot 时，会影响新的更新操作。<br>建议采用 <code>copy-on-write</code> 操作，来尽可能少的影响新的更新操作。</p></li></ol><h2 id="禅让"><a href="#禅让" class="headerlink" title="禅让"></a>禅让</h2><p>有时候，会希望取消当前 leader 的管理权，比如：</p><ul><li>leader 节点因为运维原因需要重启；</li><li>有其他更适合当 leader 的节点；</li></ul><p>直接将 leader 节点停机的话，其他节点会等待 electionTimeout 后进入选举状态， 这期间会集群会停止响应。为了避免这一段不可用的时间，可以采用禅让机制（<code>leadership transfer</code>）。</p><p>禅让的步骤为：</p><ol><li>leader 停止响应客户端请求；</li><li>leader 向 target 节点发起一次日志同步；</li><li>leader 向 target 发起一次 TimeoutNowRPC，target 收到该请求后立刻发起一轮投票。</li></ol><p>etcd 中实现了更多的细节（也有一些改动）：</p><ol><li>leader 先检查禅让对象（leadTransferee）的身份，如果是 follower，直接忽略；</li><li>leader 检查是否有正在进行的禅让，如果有，则中止之前的禅让状态，开始处理最新的请求；</li><li>检查禅让对象是否是自己，如果是，忽略；</li><li>将禅让状态信息计入 leader 的状态，并且重置 electionElapsed（因为禅让应该在 electionTimeout 内完成）；</li><li>检查禅让对象的日志是否是最新的</li><li>如果禅让对象已经是最新，则直接发送 TimeoutNowRPC</li><li>如果不是，则发送 AppendEntriesRPC，待节点响应成功后，再发送 TimeoutNowRPC</li></ol><p>可以看出，在 etcd 中，leader 除了重置 electionElapsed 外，不会改动自己的状态。 既不会停止对客户端的响应，同时还会继续发送心跳。</p><p>因为 target 机器会更新自己的 term，而且率先发起投票，其有很大的概率赢得选举。 需要注意的是，target 发起的 RequestVoteRPC 中的 <code>isLeaderTransfer=true</code>， 以防止被其他节点忽略。</p><p>如果 target 机器没能在一次 electionTimeout 内完成选举，那么 leader 认为本次禅让失败， 立刻恢复响应客户端的请求。（这时可以再次重新发起一次禅让请求）</p><p>在 etcd/raft 中，RequestVoteRPC.context 会被设置为 campaignTransfer, 表明本次投票请求来源于 leader transfer，可以强行打断 follower 的租约发起选举。</p><h2 id="预投票"><a href="#预投票" class="headerlink" title="预投票"></a>预投票</h2><p>一个暂时脱离集群网络的节点，在重新加入集群后会干扰到集群的运行。</p><p>因为当一个节点和集群失去联系后，在等待 electionTimeout 后，它就会增加自己的 term 并发起选举， 因为联系不上其他节点，所以在 electionTimeout 后，它会继续增加自己的 term 并继续发起选举。</p><p>一段时间以后，它的 term 就会显著的高于原集群的 term。如果此后该节点重新和集群恢复了联络， 它的高 term 会导致 leader 立刻退位，并重新举行选举。</p><p>为了避免这一情形，引入了 Pre-Vote 的机制。在该机制下，一个 candidate 必须在获得了多数赞同的情形下， 才会增加自己的 term。一个节点在满足下述条件时，才会赞同一个 candidate：</p><ul><li>该 candidate 的日志足够新；</li><li>当前节点已经和 leader 失联（electionTimeout）。</li></ul><p>也就是说，candidate 会先发起一轮 Pre-Vote，获得多数同意后，更新自己的 term， 再发起一轮 RequestVoteRPC。</p><p>这种情形下，脱离集群的节点，只会不断的发起 Pre-Vote，而不会更新自己的 term。</p><p>在 etcd 的实现中，如果某个节点赞同了某个 candidate， 是不需要更新自己的状态的，它依然可以赞同其他 candidate。 而且，即使收到的 PreVote 的 term 大于自己，也不会更新自己的 term。 也就是说，PreVote 不会改变其他节点的任何状态。</p><p>etcd 中还有一个设计是，当发起 PreVote 的时候，针对的是下一轮的 term， 所以会向所有的节点发送一个 term+1 的 PreVoteReq。</p><pre><code class="GO">func (r *raft) campaign(t CampaignType) {    var term uint64    var voteMsg pb.MessageType    if t == campaignPreElection {        r.becomePreCandidate()        voteMsg = pb.MsgPreVote        // 这里需要注意的是，PreVote 会针对“下一轮 term”发起投票，        // 而 Vote 则是针对当前 term        // PreVote RPCs are sent for the next term before we&#39;ve incremented r.Term.        term = r.Term + 1    } else {        r.becomeCandidate()        voteMsg = pb.MsgVote        term = r.Term    }    // ...    // 发送投票请求    r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})    // ...}</code></pre><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>所有节点间仅通过三种类型的 RPC 进行通信：</p><ul><li><code>AppendEntriesRPC</code>：最常用的，leader 向 follower 发送心跳或同步日志。</li><li><code>RequestVoteRPC</code>：选举时，candidate 发起的竞选请求。</li><li><code>InstallsnapshotRPC</code>：用于 leader 下发 snapshot。</li></ul><p>在 Diego 后续的博士论文中，又增加了一些 RPCs：</p><ul><li><code>AddServerRPC</code>：添加单台节点。</li><li><code>RemoveServerRPC</code>：移除一个节点。</li><li><code>TimeoutNowRPC</code>：立刻发起竞选。<br>（实际上 etcd 的实现中定义了几十种消息类型，甚至把内部事件也封装为消息一并处理。）</li></ul><h3 id="AppendEntriesRPC"><a href="#AppendEntriesRPC" class="headerlink" title="AppendEntriesRPC"></a>AppendEntriesRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 当前的 term；</li><li><code>leaderId</code>：leader 的 节点id，让 follower 可以重定向客户端的连接；</li><li><code>prevLogIndex</code>：前一块 entry 的 index；</li><li><code>prevlogterm</code>：前一块 entry 的 term；</li><li><code>entries[]</code>：给 follower 发送的 entry，可以一次发送多个，heartbeat 时该项可缺省；</li><li><code>leaderCommit</code>：leader 当前的 <code>committed index</code>，follower 收到后可用于自己的状态机。</li></ul><p>返回：</p><ul><li><code>term</code>：响应者自己的 term；</li><li><code>success</code>：bool，是否接受请求。<br>该请求通过 leaderCommit 通知 follower 提交相应的 entries 到。通过 entries[] 复制 leader 的日志到所有的 follower。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，立刻返回 false</li><li>如果 prevLogIndex 不匹配，返回 false</li><li>如果自己有块 entry 和新的 entry 不匹配（在相同的 index 上有不同的 term）， 删除自己的那一块以及之后的所有 entry；</li><li>把新的 entries 添加到自己的 log；<br>5 。如果 <code>leaderCommit &gt; commitindex</code>，将 commitIndex 设置为 <code>min(leaderCommit, last index)</code>， 并且提交相应的 entries。</li></ol><h3 id="RequestVoteRPC"><a href="#RequestVoteRPC" class="headerlink" title="RequestVoteRPC"></a>RequestVoteRPC</h3><p>参数：</p><ul><li><code>term</code>：candidate 当前的 term；</li><li><code>candidateId</code>：candidate 的节点 id</li><li><code>lastlogindex</code>：candidate 最后一个 entry 的 index；</li><li><code>lastlogterm</code>：candidate 最后一个 entry 的 term。</li><li><code>isleaderTransfer</code>：用于表明该请求来自于禅让，无需等待 electionTimeout，必须立刻响应。</li><li><code>isPreVote</code>：用来表明当前是 PreVote 还是真实投票</li></ul><p>返回：</p><ul><li><code>term</code>：响应者当前的 term；</li><li><code>voteGranted</code>：bool，是否同意投票。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，返回 false；</li><li>如果 votedFor 为空或者为该 <code>candidated id</code>，且日志项不落后于自己，则同意投票。</li></ol><h3 id="InstallsnapshotRPC"><a href="#InstallsnapshotRPC" class="headerlink" title="InstallsnapshotRPC"></a>InstallsnapshotRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 的 term</li><li><code>leaderId</code>：leader 的 节点 id</li><li><code>lastIncludedindex</code>：snapshot 中最后一块 entry 的 index；</li><li><code>lastIncludedterm</code>：snapshot 中最后一块 entry 的 term；</li><li><code>offset</code>：该份 chunk 的 offset；</li><li><code>data[]</code>：二进制数据；</li><li><code>done</code>：是否是最后一块 chunk</li></ul><p>返回：</p><ul><li><code>term</code>：follower 当前的 term</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code> 就立即回复</li><li>如果是第一个分块（offset 为 0）就创建一个新的快照</li><li>在指定偏移量写入数据</li><li>如果 done 是 false，则继续等待更多的数据</li><li>保存快照文件，丢弃索引值小于快照的日志</li><li>如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保持</li><li>丢弃整个日志</li><li>使用快照重置状态机</li></ol><h3 id="AddServerRPC"><a href="#AddServerRPC" class="headerlink" title="AddServerRPC"></a>AddServerRPC</h3><p>参数：</p><ul><li><code>newServer</code>：新节点地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否添加成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>如果没有在 electionTimeout 内处理，则返回 TIMEOUT；</li><li>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="RemoveServerRPC"><a href="#RemoveServerRPC" class="headerlink" title="RemoveServerRPC"></a>RemoveServerRPC</h3><p>参数：</p><ul><li><code>oldServer</code>：要删除的节点的地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否删除成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="TimeoutNowRPC"><a href="#TimeoutNowRPC" class="headerlink" title="TimeoutNowRPC"></a>TimeoutNowRPC</h3><p>由 leader 发起，告知 target 节点立刻发起竞选，无视 electionTimeout。主要用于禅让。</p><h2 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h2><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2>]]></content>
    
    
    <categories>
      
      <category>分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CAP 定理介绍</title>
    <link href="/cap-theory/"/>
    <url>/cap-theory/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在互联网行业飞速发展的 21 世纪，分布式系统正变得越来越重要，大型互联网公司如 Google, Amazon, MicroSoft, Alibaba, Tencent 等之所以被认为技术很厉害，很大程度上是因为其后台十分强悍，而这些后台一定是由若干个大的分布式系统组成的，因此理解分布式系统的运行原理对于程序员有非常重要的意义。</p><p>CAP 定理是分布式系统方向一个比较宽泛但很重要的基本定理，也可以作为理解分布式系统的起点。这篇博客将详细介绍 CAP 定理并简单证明，最后谈一谈 CAP 定理在工业界的应用。</p><h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>2000年，柏克莱加州大学（University of California, Berkeley）的计算机科学家 Eric Brewer 在分布式计算原则研讨会（Symposium on Principles of Distributed Computing）提出，分布式系统有三个指标。</p><ul><li>Consistency</li><li>Availability</li><li>Partition tolerance</li></ul><p>它们的第一个字母分别是 C、A、P。</p><p>Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。</p><p>需要注意的是，尽管我们常说某个系统能够满足 CAP 属性中的 2 个，但并不是必须满足 2 个，许多系统只具有 0 或 1 个 CAP 属性。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h3><p>我们知道 ACID 中事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行前后，数据库都必须处于一致性状态。也就是说，事务的执行结果必须是使数据库从一个一致性状态转变到另一个一致性状态。</p><p>和 ACID 中的一致性不同，分布式环境中的一致性是指数据在多个副本之间是否能够保持一致的特性。</p><p>分布式系统中，数据一般会存在不同节点的副本中，如果对第一个节点的数据成功进行了更新操作，而第二个节点上的数据却没有得到相应更新，这时候读取第二个节点的数据依然是更新前的数据，即脏数据，这就是分布式系统数据不一致的情况。</p><p>在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都能读取到最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。</p><h3 id="Availability"><a href="#Availability" class="headerlink" title="Availability"></a>Availability</h3><p>可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果，如果超过了这个时间范围，那么系统就被认为是不可用的。</p><p>“有限的时间内”是在系统的运行指标，不同系统会有差别。例如搜索引擎通常在 0.5 秒内需要给出用户检索结果。</p><p>”返回结果”是可用性的另一个重要指标，它要求系统完成对用户请求的处理后，返回一个正常的响应结果，要明确的反映出对请求处理的成功或失败。如果返回的结果是系统错误，比如”OutOfMemory”等报错信息，则认为此时系统是不可用的。</p><h3 id="Partition-Tolerance"><a href="#Partition-Tolerance" class="headerlink" title="Partition Tolerance"></a>Partition Tolerance</h3><p>一个分布式系统中，节点组成的网络本来应该是连通的。然而可能因为某些故障，使得有些节点之间不连通了，整个网络就分成了几块区域，而数据就散布在了这些不连通的区域中，这就叫分区。</p><p>当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。</p><p>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项仍然能在其他区中读取，容忍性就提高了。然而，把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。</p><p>总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。</p><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><h3 id="简单理解"><a href="#简单理解" class="headerlink" title="简单理解"></a>简单理解</h3><p>根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。理解 CAP 理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了 C 性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了 A 性质。除非两个节点可以互相通信，才能既保证 C 又保证 A，这又会导致丧失 P 性质。</p><h3 id="详细证明"><a href="#详细证明" class="headerlink" title="详细证明"></a>详细证明</h3><p><img src="http://q5ijnj5w7.bkt.clouddn.com/cap_base.png" srcset="/img/loading.gif" alt></p><p>我们现在有两个网络 N1 和 N2，每个网络中都存在一个服务用于从 db 获取数据，初始状态下，db 中存储的数据都是 V0。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/cap_p.png" srcset="/img/loading.gif" alt></p><p>正常情况下，在网络 N1 通过服务 A 更新 V0 到 V1，更新成功后发送消息 M 使 N2 的 db 中的 V0 变为 V1，此时我们通过服务 B 获取数据时，获取到 V1。</p><pre><code>此时满足 CA，没有分区故不满足 P。</code></pre><p><img src="http://q5ijnj5w7.bkt.clouddn.com/cap_withoutp.png" srcset="/img/loading.gif" alt></p><p>但是一旦发生了网络分区，此时我们通过服务 A 更新数据到 V1 后，由于网络错误，V1 值同步不到 N2 网络中去，此时我们调用服务 B 去请求数据的时候，我们必须从 C 和 A 选一个，如果选择 C，我们需要等到数据同步到 N2，但是从服务 B 获取数据肯定是失败了，失去了 A。如果选择 A，那么从 B 我们获取到的数据不是最新的，失去了 C。</p><pre><code>此时有分区故满足 P，CA 只能满足一个。</code></pre><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="取舍策略"><a href="#取舍策略" class="headerlink" title="取舍策略"></a>取舍策略</h3><h4 id="CP-without-A"><a href="#CP-without-A" class="headerlink" title="CP without A"></a>CP without A</h4><p>如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在 CAP 三者中保障 CP 而舍弃 A。</p><p>一个保证了 CP 而一个舍弃了 A 的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。</p><p>设计成 CP 的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成 CP 的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如 Redis、HBase 等，还有分布式系统中常用的 Zookeeper 也是在 CAP 三者之中选择优先保证 CP 的。</p><p>无论是像 Redis、HBase 这种分布式存储系统，还是像 Zookeeper 这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？</p><p>ZooKeeper 是个 CP（一致性+分区容错性）的，即任何时刻对 ZooKeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper 可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成 CP 而不是 AP 特性的了。</p><h4 id="AP-wihtout-C"><a href="#AP-wihtout-C" class="headerlink" title="AP wihtout C"></a>AP wihtout C</h4><p>要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。</p><p>这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到 N 个 9，所以，对于很多业务系统来说，比如淘宝的购物，12306 的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p><p>你在 12306 买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。</p><p>但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。</p><p>对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃 C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p><h4 id="CA-without-P"><a href="#CA-without-P" class="headerlink" title="CA without P"></a>CA without P</h4><p>这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃 P，意味着要舍弃分布式系统。那也就没有必要再讨论 CAP 理论了。这也是为什么在前面的 CAP 证明中，我们以系统满足 P 为前提论述了无法同时满足 C 和 A。</p><p>比如我们熟知的关系型数据库，如 Mysql 和 Oracle 就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把 P 也考虑进来。</p><p>其实，在 CAP 理论中。C，A，P 三者并不是平等的，CAP 之父在《Spanner，真时，CAP 理论》一文中写到：</p><blockquote><p>如果说 Spanner 真有什么特别之处，那就是谷歌的广域网。Google 通过建立私有网络以及强大的网络工程能力来保证 P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。</p></blockquote><p>从 Google 的经验中可以得到的结论是，一直以来我们可能被 CAP 理论蒙蔽了双眼，CAP 三者之间并不对称，C 和 A 不是 P 的原因（P 不能和 CA trade-off，CP 和 AP 中不存在 trade-off，trade-off 在 CA 之间）。提高一个系统的抗毁能力或者说提高 P（分区容忍能力）是通过提高基础设施的稳定性来获得的，而不是通过降低 C 和 A 来获得的。也就说牺牲 C 和 A 也不能提高 P。</p><p>所以，对于一个分布式系统来说。P 是一个基本要求，CAP 三者中，只能在 CA 两者之间做权衡，并且要想尽办法提升 P。P 提升的越好，CA 同时满足就越有可能。</p><h3 id="业界应用分析"><a href="#业界应用分析" class="headerlink" title="业界应用分析"></a>业界应用分析</h3><table><thead><tr><th>应用</th><th>类型</th><th>解释</th></tr></thead><tbody><tr><td>MySQL</td><td>CA</td><td>主从模式为 AP</td></tr><tr><td>Spanner</td><td>CA/CP</td><td>技术实现是 CP 但号称是 CA，宣称 CA 系统并不意味着 100％ 的可用性</td></tr><tr><td>分布式协议-Raft/ZAB/Paxos</td><td>CP</td><td>在分区后，对于 A，只有分区内节点大于 Quorum 才对外服务</td></tr><tr><td>分布式事务-2PC</td><td>CP</td><td>锁住资源,该资源其他请求阻塞</td></tr><tr><td>分布式事务-TCC</td><td>AP</td><td>最终一致性</td></tr><tr><td>分布式事务-最大努力尝试</td><td>AP</td><td>最终一致性</td></tr><tr><td>DNS服务</td><td>AP</td><td>最终一致性</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，因此分区容错性也就成为了一个分布式系统必然要面对的问题，那么就只能在 C 和 A 之间进行取舍。</p><p>对于某些安全性要求极高的项目，比如银行的转账系统，涉及到金钱的对于数据一致性不能做出一丝的让步，C 必须保证，出现网络故障的话，宁可停止服务，也不能冒着出错误的风险继续提供服务。</p><p>对于网站，DNS 服务等，其内容的实时性不是特别严格，则可以牺牲一定的一致性，保证最高的可用性是最好的选择。</p><p>个人认为，CAP 定理的核心在于，在网络分区的情况下，我们需要对 C 和 A 做出相应的妥协，我们不可能完全满足 CA，但是我们可以合理控制 C 和 A 之间的比例让我们的应用/中间件正常提供服务，同时也尽量提升基础设施的稳定性来保障 P。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>《Spanner，真时，CAP 理论》是 Google VP，CAP 理论之父在情人节当天撰写的，主要介绍了 Google 的 Spanner 数据库的真时（TrueTime）服务和 CA 特性，以及结合 CAP 理论的一些思考，建议阅读，阅读 Spanner 论文后阅读更佳。</p><ul><li><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45855.pdf" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》</a></p></li><li><p><a href="https://toutiao.io/posts/zdqrx0/preview" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》中文</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iTerm2 快捷键介绍</title>
    <link href="/iTerm2-hotkeys/"/>
    <url>/iTerm2-hotkeys/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>iTerm2 是 MacOS 独有的终端工具，其有许多快捷键可以使用。为了便于开发并节约之后再次在搜索引擎上查询的时间成本，特写此博客以供自己日后查看。</p><h2 id="快捷键介绍"><a href="#快捷键介绍" class="headerlink" title="快捷键介绍"></a>快捷键介绍</h2><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><ul><li>新建标签：Command + T</li><li>关闭标签：Command + W</li><li>切换标签：Command + 数字 或 Command + 左右方向键</li></ul><h3 id="分屏"><a href="#分屏" class="headerlink" title="分屏"></a>分屏</h3><ul><li>垂直分屏：Command + D</li><li>水平分屏：Command + Shift + D</li><li>切换屏幕：Command + Option + 方向键 或 Command + [ / ]</li></ul><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><ul><li>局部搜索(包含单个终端)：Command + F</li><li>全局搜索(包含所有Tab)：Command + Option + E</li><li>搜索历史指令：Ctrl + R</li></ul><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><ul><li>查看历史命令：Command + ;</li><li>查看剪贴板历史：Command + Shift + H</li><li>上一条命令：Ctrl + P 或 上方向键</li></ul><h3 id="单行"><a href="#单行" class="headerlink" title="单行"></a>单行</h3><ul><li>光标到行首：Ctrl + A</li><li>光标到行尾：Ctrl + E</li><li>删除当前行：Ctrl + U</li><li>删除当前光标的字符：Ctrl + D</li><li>删除光标之前的字符：Ctrl + H</li><li>删除光标之前的单词：Ctrl + W</li><li>删除到文本末尾：Ctrl + K</li></ul><h3 id="内容大小"><a href="#内容大小" class="headerlink" title="内容大小"></a>内容大小</h3><ul><li>放大终端：Command + +</li><li>缩小终端：Command + - </li></ul><h3 id="常用快捷功能"><a href="#常用快捷功能" class="headerlink" title="常用快捷功能"></a>常用快捷功能</h3><ul><li>清屏：Command + R 或 Crtl + L</li><li>切换全屏：Command + Enter</li><li>选中即复制：在 iTerm2 界面，选择了一行就已经复制了</li></ul>]]></content>
    
    
    <categories>
      
      <category>开发工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的高效 Macbook 工作环境配置</title>
    <link href="/mac-configuration/"/>
    <url>/mac-configuration/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>工欲善其事，必先利其器，工具永远都是用来解决问题的，没必要为了工具而工具，一切工具都是为了能快速准确的完成工作和学习任务而服务。</p><p>我呢，在使用了 Windows，Ubuntu 和 MacOS 三种操作系统之后。结合种种体验和踩坑，最终还是觉得 MacOS 更舒适一点。每个人都有每个人的看法，每个人都有每个人的舒适点，MacOS 恰好捏住了我的舒适点。因此，我之后都将从 MacOS 上工作学习。</p><p>前一段时间我从公司实习离职，上交了公司发给我的 MacBook（<del>停止薅羊毛</del>），然而我又不想回到 Windows，于是打算自己买一台 MacBook。但是 MacBook 从 2016 年开始更换的蝶式键盘很让我恶心，姑且不说故障率高，触感实在太差劲了。尽管 2020 年新出的 16 寸 Pro 已经重回剪刀脚键盘了，但是我的需求是轻薄的 13 寸而不是 16 寸（<del>只是没钱而已</del>）。尽管听到业界的呼声说 2020 年的 MacBook 应该都会回到剪刀脚键盘，但由于 2020 年会换新模具，我也不想踩第一代模具的坑，因而暂且将目标定为 2021 年的 MacBook，目前一年多买个二手过渡下就可以了。</p><p>1 月份我在某宝平台上买了一台 2014 款 8+256 的二手 MacBook Pro，即使前期做了许多选店和辨伪的功课，拿到手之后却依然中招，总是无理由黑屏然后再无法开机一天，十分坑爹。所幸可以十五天无理由退换货，就赶快退了。之前早就听说二手 Mac 的水很深，被坑一次之后更加确信。接着我做了更多的功课，学到了许多辨伪技巧，浏览了许多店铺，也算有点心得，之后要是有时间可以写出来分享给大家。</p><p>前几天经过慎重选择我又在某东平台上入手了一台 2015 款 8+128 的二手 MacBook Pro。这次总算没什么问题，但比较有趣的一点是我买的 8+128 的，老板发给我的是 8+256 的，平白无故赚了 128G 的固态，只能说真的舒服了。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/home.jpeg" srcset="/img/loading.gif" alt></p><p>这是一个新的 MacBook 刚打开后的主页，接下来我要通过一系列的配置使其成为一个十分符合我开发习惯的机器，可供大家参考。</p><h2 id="系统篇"><a href="#系统篇" class="headerlink" title="系统篇"></a>系统篇</h2><h3 id="触屏板"><a href="#触屏板" class="headerlink" title="触屏板"></a>触屏板</h3><ul><li>2016 年及之后的 MacBook 触屏板都有 Force touch 的功能，即可以按压两次来实现更多的功能，但是我一直用不来这个功能，因此我的第一件事就是调整触摸屏板，首先先关掉 Force touch 的功能，然后开启轻点来点按的点击方式，个人觉得这样才符合 MacBook 轻巧的特性嘛，每次都按下去多麻烦啊，现在手指轻轻一碰触摸板，就达到鼠标单击的顺滑效果。</li><li>除此以外，可以根据自己的习惯开启或关闭一些手势。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/touch_1.png" srcset="/img/loading.gif" alt><br><img src="http://q5ijnj5w7.bkt.clouddn.com/touch_2.png" srcset="/img/loading.gif" alt><br><img src="http://q5ijnj5w7.bkt.clouddn.com/touch_3.png" srcset="/img/loading.gif" alt></p><h3 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h3><ul><li>由于 MacBook 默认的重复前延迟和按键重复配置太慢，限制了程序员们优秀的打字速度，所以建议都调整到最快的速度。</li><li>可以在闲置 5 分钟后关闭键盘背光灯来省点电。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/keyboard.png" srcset="/img/loading.gif" alt></p><h3 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h3><ul><li>由于 MacBook 默认的切换大小写的方式是长按 Caps 键，时间较慢需要等待，较为影响开发效率，建议关闭长按改为短按，配合极低的按键延迟会十分舒爽。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/input.png" srcset="/img/loading.gif" alt></p><ul><li>建议安装搜狗输入法 Mac 版替代系统自带输入法。</li></ul><h3 id="快速锁定屏幕"><a href="#快速锁定屏幕" class="headerlink" title="快速锁定屏幕"></a>快速锁定屏幕</h3><ul><li><p>如果你长时间离开电脑，最好锁定你的屏幕，以防止数据泄露。 那如何快速的锁定你的 MacBook 呢？ 答案是只需要一摸触摸板就可以了。</p><ul><li><p>打开系统偏好设置，点击桌面与屏幕保护程序图标，选择屏幕保护程序这个 Tab，再点击触发角，在弹出的如下界面里面，右下角选择将显示器置入睡眠状态，再确定即可。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/screen_saver.png" srcset="/img/loading.gif" alt></p></li><li><p>再打开系统偏好设置，点击安全性与隐私图标，在通用 Tab 内，勾选为进入睡眠或开始屏幕保护程序<strong>立即</strong>要求输入密码。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/screen_security.png" srcset="/img/loading.gif" alt></p></li></ul></li></ul><h2 id="开发环境篇"><a href="#开发环境篇" class="headerlink" title="开发环境篇"></a>开发环境篇</h2><h3 id="Xcode"><a href="#Xcode" class="headerlink" title="Xcode"></a>Xcode</h3><ul><li><p>首先安装 Xcode，然后使用下面的命令安装 Xcode command line tools，这将为我们安装很多终端下面常用的命令，将来很可能会使用到。</p><pre><code class="Shell">  xcode-select --install</code></pre></li></ul><h3 id="Homebrew"><a href="#Homebrew" class="headerlink" title="Homebrew"></a>Homebrew</h3><ul><li><p>Homebrew 是一款终端下的命令程序包管理器，安装非常简单，复制如下命令在终端下运行，按回车并输入密码后等待安装成功：</p><pre><code class="Shell">  ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</code></pre></li></ul><h3 id="iTerm2-Zsh-Z"><a href="#iTerm2-Zsh-Z" class="headerlink" title="iTerm2 + Zsh + Z"></a>iTerm2 + Zsh + Z</h3><ul><li>常用终端 iTerm2 + 优秀 Shell Zsh + 扁平目录跳转命令 Z，安装好之后开发十分舒适。具体安装可参考这个<a href="https://www.jianshu.com/p/a5f478a143dc" target="_blank" rel="noopener">博客</a>。</li></ul><h3 id="快捷键迅速打开-iTerm2"><a href="#快捷键迅速打开-iTerm2" class="headerlink" title="快捷键迅速打开 iTerm2"></a>快捷键迅速打开 iTerm2</h3><ul><li>可以设置快捷键再 Home 页面输入 Command + , 直接打开 iTerm2，这样就不用再去点击 iTerm2 了。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/iTerm2_hotkey.png" srcset="/img/loading.gif" alt></p><ul><li>可以设置 iTerm2 默认占满全屏，这样子快捷键打开之后就直接是一个全屏的 iTerm2 可以使用了</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/iTerm2_screen.png" srcset="/img/loading.gif" alt> </p><h3 id="VScode命令行迅速打开"><a href="#VScode命令行迅速打开" class="headerlink" title="VScode命令行迅速打开"></a>VScode命令行迅速打开</h3><ul><li>打开VScode后输入 Command + Shift + P 打开命令面板，再输入 code，再确定</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/vscode_code.png" srcset="/img/loading.gif" alt></p><h3 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h3><ul><li>创建新的公钥私钥并与自己的 Github 账户连起来，这样就可以开始在 Github 遨游啦。</li></ul><h2 id="常用软件"><a href="#常用软件" class="headerlink" title="常用软件"></a>常用软件</h2><ul><li>网易云音乐</li><li>微信</li><li>QQ</li><li>SSR</li><li>Chrome</li><li>VScode</li><li>IDEA</li><li>Docker</li><li>…</li></ul>]]></content>
    
    
    <categories>
      
      <category>开发工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
