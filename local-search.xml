<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Time, Clocks, and the Ordering of Events in a Distributed System 论文阅读</title>
    <link href="/time-clock-order-in-distributed-system-thesis/"/>
    <url>/time-clock-order-in-distributed-system-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这篇文章是 Leslie Lamport 于 1978 年发表的，并在 2007 年被选入 SOSP 的名人堂，被誉为第一篇真正的”分布式系统”论文，该论文曾一度成为计算机科学史上被引用最多的文章（截止 2021 年 3 月 22 日已达到 12497 次）。文章的作者 Lamport 享有分布式计算原理之父的美誉，并且因其对分布式系统研究作出的卓越贡献，于 2013 年被授予了图灵奖。</p><p>在分布式系统中的时间同步是一个非常难的问题，因为分布式系统是使用消息进行通信的，若使用物理时钟来进行同步，一方面是不同进程的时钟有差异，另一方面是时间的计算也有一定的误差，这样若有两个时间相同的事件，则无法区分它们谁前谁后了。因此目前通用的解决方案是通过逻辑时钟来同步时间从而确定偏序关系。</p><p>本论文中首先介绍了分布式系统中的偏序关系，接着又提出了一个算法，可以基于 Happened Before 这种偏序关系（因果一致性）来扩展出强一致性的全序关系。</p><h2 id="内容概要"><a href="#内容概要" class="headerlink" title="内容概要"></a>内容概要</h2><p>可参考此<a href="https://zhuanlan.zhihu.com/p/34057588" target="_blank" rel="noopener">博客</a>。</p><h2 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h2><h3 id="偏序"><a href="#偏序" class="headerlink" title="偏序"></a>偏序</h3><p><img src="/time-clock-order-in-distributed-system-thesis/order.png" srcset="/img/loading.gif" alt></p><p>对于上图中的的进程 P 和进程 Q，进程 P 在某时刻发生了事件 p1，经过一段时间后，发生事件 p2，可以说：p1 发生在 p2 前面。进程 P 向进程 Q 发送消息，进程 P 发送消息时记为事件 p1，进程 Q 接收到进程 P 发送的消息记为事件 q2，可以说：p1 发生在 q2 前面。以上两个例子表明了一种偏序的关系。之所以说明这两个例子所代表的关系是偏序的，是因为：当需要判断下图中的 p3 和 q3 这两个事件谁先谁后时，在偏序关系下是无法判断的。即偏序只能为系统中的部分事件定义先后顺序。这里的部分其实是有因果关系的事件。</p><p>偏序的定义：（偏序以符号 -&gt; 表示）</p><ul><li>在同一个进程中有两个事件 a、b，如果 a 先发生，b 后发生，则表示为 a-&gt;b。</li><li>在不同的两个进程中，若 a 为一个进程发消息的事件，b 为另一个进程接受消息的事件，则 a 先发生，b 后发生，表示为 a-&gt;b。</li><li>如果 a-&gt;b，b-&gt;c，那么 a-&gt;c。</li><li>若无法判断 a 和 b 谁先发生，则表示为并发的。</li></ul><p>Lamport 在论文中提出了一种利用逻辑时钟设计的偏序系统方法：</p><ul><li>每个进程存在独立的事件序列发生器，每次产生新的事件，该序列发生器自增 1，并将结果赋予该事件。</li><li>如果进程的事件 E 需要向其他进程发送消息 M，那么在 M 中携带 E 的序列号。</li><li>如果进程收到外部消息 M，获取 M 中携带的序列号，与自身的事件序列发生器取最大值，然后自增 1，赋给由于 M 而触发的新的外部事件。</li></ul><p>根据上面的定义，我们可以得到如下结论：</p><ul><li>进程内部的事件均可以比较先后顺序。</li><li>进程之间的因果事件可以确定先后顺序，而进程之间的独立事件则无法比较先后顺序。</li></ul><h3 id="全序"><a href="#全序" class="headerlink" title="全序"></a>全序</h3><p>全序是指所有的事件都可以区分先后顺序。无论是真实或是虚拟世界，这都是受欢迎的。因为，所有的事件都有了一个统一的评判标准，我们一直欢迎统一而拒绝分裂。</p><p>在偏序的基础上，Lamport 定义了一种全序方案，在偏序的基础上有所增强：每个事件表示为一个三元组：<E, c, p><br>其中：</E,></p><ul><li>E：代表事件。</li><li>C：代表事件所发生的进程赋予该事件的序列号(逻辑时间)。</li><li>P：代表事件所发生的进程的编号。</li></ul><p>有了该三元组后，定义事件先后的顺序就变成了：</p><blockquote><p>如果 C(i, a) &lt; C(j, b) 或者如果 C(i, a) = C(j, b) 并且 P(i) &lt; P(j)。</p></blockquote><p>作者通过对进程进行排序（arbitrary total ordering），当进程之间有并发的事件时，以进程的顺序来决定事件的顺序，这样就可以对所有的事件定义一个全序（全序以符号 =&gt; 表示）了，并解决了对分布式系统对事件进行排序的问题。</p><p>论文中提到的全序的一个例子解析可以参考此<a href="https://zhuanlan.zhihu.com/p/27503041" target="_blank" rel="noopener">博客</a>。</p><h3 id="物理时钟"><a href="#物理时钟" class="headerlink" title="物理时钟"></a>物理时钟</h3><p>过于晦涩，没看懂。</p><h3 id="并发事件与-Vector-clock"><a href="#并发事件与-Vector-clock" class="headerlink" title="并发事件与 Vector clock"></a>并发事件与 Vector clock</h3><p>参考此<a href="https://blog.csdn.net/paxhujing/article/details/51612105" target="_blank" rel="noopener">博客</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>记住一点：在分布式系统和现实世界中，事件发生的顺序都是偏序来定义的。</p><p>个人感受：Lamport 大佬的论文还是照常晦涩难懂，这篇分布式系统的开山论文看下来也只能说似懂非懂吧，希望以后能有真的懂了的一天。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://www.cnblogs.com/hzmark/p/Time_Clocks_Ordering.html" target="_blank" rel="noopener">论文翻译</a><br><a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VM-FT 论文阅读</title>
    <link href="/vm-ft-thesis/"/>
    <url>/vm-ft-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>本论文主要介绍了一个用于提供容错虚拟机 (fault-tolerant virtual machine) 的企业级商业系统，该系统包含了两台位于不同物理机的虚拟机，其中一台为 primary，另一台为 backup，backup 备份了 primary 的所有执行。当 primary 出现故障时，backup 可以上线接管 primary 的工作，以此来提供容错。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>论文内容可以参考此<a href="https://www.cnblogs.com/brianleelxt/p/13245754.htmls" target="_blank" rel="noopener">博客</a>。</p><h2 id="分布式容错方案"><a href="#分布式容错方案" class="headerlink" title="分布式容错方案"></a>分布式容错方案</h2><p>分布式系统中需要对一份数据进行冗余存储才能提供容错性，因此问题是：</p><blockquote><p>如果有一份会随时变动的数据，如何确保它正确地存储于网络中的几台不同机器之上？</p></blockquote><p>对于 primary/backup 型的同步方式，可行的解决方案有两种：状态转移（State Transfer）和操作转移（Operation Transfer）。</p><h3 id="状态转移"><a href="#状态转移" class="headerlink" title="状态转移"></a>状态转移</h3><p>对于状态转移，其方案是 primary 持续地将所有状态（包括 CPU、内存和 I/O 设备等或者整个状态机实例）变化发送给 backup，当然也可以有一些优化，比如只传送相比上次变化的数据等等，但总之这种方法所需带宽非常大，延迟较高，因此在工业界中应用不多。</p><h3 id="操作转移"><a href="#操作转移" class="headerlink" title="操作转移"></a>操作转移</h3><p>操作转移能够运行的前提是状态机，其特性是：</p><blockquote><p>任何初始状态一样的状态机，如果执行的命令序列一样，则最终达到的状态也一样。如果将此特性应用在多参与者进行协商共识上，可以理解为系统中存在多个具有完全相同的状态机（参与者），这些状态机能最终保持一致的关键就是起始状态完全一致和执行命令序列完全一致。</p></blockquote><p>操作转移一般有两种复制级别：</p><ul><li>机器级别：按序复制 CPU 指令，中断，客户端请求等等来保证不同节点间状态一致，这也就是本文的解决方案。这种方案需要解决若干问题，比如多节点间如何处理不明确性命令（获取当前时间），如果避免脑裂等等。看了论文之后，个人感觉论文中的做法不是很优雅，而且很多关键点并没有纰漏实现细节，此外该解决方案与虚拟机过于耦合，较难扩展，可能逐渐会被云原生时代淘汰。</li><li>应用级别：按序复制明确性的操作来保证不同节点间的状态一致。比如 MySQL Cluster 的主从全同步复制需要保证所有的从节点接受成功才能返回成功，这一定程度上会导致扩展性受限，即每增加一个 Slave 节点，都导致造成整个系统可用性风险增加一分。因此也出现了基于 Paxos，Raft 的 quorum 同步方案，即一旦系统中过半数的节点中完成了状态的转换，就认为数据的变化已经被正确地存储在系统当中，这样就可以容忍少数（通常是不超过半数）的节点失联，使得增加机器数量对系统整体的可用性变成是有益的，这也是目前大多数应用的容错方案。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>作为一篇 10 年前的论文，在那时 Raft 还没有出现，Paxos 还没有得到广泛的承认，该论文对于分布式容错方案提出了一种基于机器级别的操作转移方案，拓展了理论的边界并证明了工业界可用，可以说是比较划时代的贡献了。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://icyfenix.cn/distribution/consensus/" target="_blank" rel="noopener">分布式共识算法讲解</a><br><a href="https://pdos.csail.mit.edu/6.824/notes/l-vm-ft.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="https://www.bilibili.com/video/BV1R7411t71W?p=4" target="_blank" rel="noopener">6.824 视频</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/1899928.1899932" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GFS 论文阅读</title>
    <link href="/gfs-thesis/"/>
    <url>/gfs-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>单机文件系统我们见得已经很多，像 xfs，ext4 等等都已经是很出名的单机文件系统。然而，单机文件系统的容量始终是有限的，随着数据量的不断增大，就算对单机不断的 scale up 也会逐渐达到上限。因此，支持 scale out 的分布式文件系统是技术的必然。作为世界上数据量可能最多的公司，Google 在 21 世纪初就已经遇到了这个挑战，随之其开发了 GFS 这个创时代的分布式文件系统并将该成果发表在了 2003 年的 SOSP 会议上，之后随着 MapReduce 和 BigTable 论文的发表，Google 的三架马车整整齐齐，掀开了大数据时代的帷幕，引领工业界开始了辉煌的 NoSQL 运动。</p><p>之后，MapReduce 和 BigTable 或多或少都被新时代的大数据技术栈替代和压榨，只有 GFS 在大规模分布式文件系统领域鲜有敌手，只有某些针对不同场景的优化竞品而已，例如针对海量小文件做了优化的 haystack。总之，即使在 2021 年的今天，学习 GFS 的设计思想也是十分有意义的。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>实现一个高可用，可扩展，高性能，低成本，可容错的大规模分布式文件系统，同时为用户提供近可能简单的文件读写接口并屏蔽底层的复杂实现细节。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>本来打算好好的写一篇博客总结一下自己时隔两年后再读 GFS 的理解，但是偶然看到了一篇写的很详细且有作者想法的<a href="https://spongecaptain.cool/post/paper/googlefilesystem/" target="_blank" rel="noopener">博客</a>，因此就没必要浪费时间自己再写一遍了，感兴趣的可以去详读这篇博客。</p><p>对于博客中所述，有一处有关 chunk version 的地方我个人不太认同，原文如下：</p><blockquote><p>Master 节点在接受到修改请求时，会找此 file 文件最后一个 chunk 的 up-to-date 版本（最新版本），最新版本号应当等于 Master 节点的版本号；</p><p>什么叫最新版本。chunk 使用一个 chunk version 进行版本管理（分布式环境下普遍使用版本号进行管理，比如 Lamport 逻辑时钟）。一个修改涉及 3 个 chunk 的修改，如果某一个 chunk 因为网络原因没能够修改成功，那么其 chunk version 就会落后于其他两个 chunk，此 chunk 会被认为是过时的。</p><p>Master 在选择好 primary 节点后递增当前 chunk 的 chunk version，并通过 Master 的持久化机制持久化；</p><p>通过 Primary 与其他 chunkserver，发送修改此 chunk 的版本号的通知，而节点接收到次通知后会修改版本号，然后持久化；</p><p>Primary 然后开始选择 file 最后一个文件的 chunk 的末尾 offset 开始写入数据，写入后将此消息转发给其他 chunkserver，它们也对相同的 chunk 在 offset 处写入数据；</p></blockquote><p>博客作者认为 Master 维护 chunk version 的步骤是先递增当前 chunk 的 chunk version 再本地持久化，然后通知 Primary。个人认为这样不妥，因为一旦 Master 在持久化完 chunk version 和通知到 Primary 之间挂了，在 Master 重启之后，其会与所有 chunkserver 进行心跳来获取这些 chunkserver 拥有的 chunk 和其 chunk version，接着 Matser 会发现该 chunk 的所有 chunk version 都小于其从磁盘恢复的该 chunk 的 chunk version，从而认为真正持有最新 chunk version 的 chunkserver 还未恢复，进而不为该 chunk 提供服务。</p><p>因此我认为，Master 维护 chunk version 的步骤应该是先递增当前 chunk 的 chunk version 并通知 Primary，得到确切回复后再本地持久化，这样一旦 Master 在期间挂了，其恢复之后会发现 chunkserver 心跳上来的消息中，该 chunk 的 chunk version 大于本地磁盘恢复的 chunk version，从而认为 chunkserver 的 chunk version 最新并更新且持久化本地的 chunk version，进而能够提供服务，这样就与论文中的描述自恰了。</p><p>除以上一点外，其他的细节个人觉得博客都讲的很正确很 make sense 了~</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>优秀的设计往往很简单</li><li>很难设计出一个满足所有业务场景的系统，针对不同的场景会有不同的解决方案</li><li>设计系统时就需要考虑到系统的瓶颈在何处（磁盘，网络，CPU）</li><li>单节点的性能即使不高也没太大问题，扩展性更重要</li></ul><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://pdos.csail.mit.edu/6.824/notes/l-gfs.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="https://www.bilibili.com/video/BV1R7411t71W?p=3" target="_blank" rel="noopener">6.824 双语视频</a><br><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库系统调优时有关操作系统的知识与测试监控</title>
    <link href="/operating-system-performance-testing/"/>
    <url>/operating-system-performance-testing/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在测试数据库的性能时，往往需要通过从网卡，CPU，内存，磁盘，代码等方面出发进行性能调优，这期间可能会纵向扩展机器以获得更好的性能。然而，在不同的硬件机器上测试时，有时的结果可能与预期相差较多，单从软件方面进行猜想可能并不靠谱。因此，需要对机器硬件进行一定的了解和对应的测试，这样就能够在调优分析时利用底层的硬件信息来支撑瓶颈分析，从而更可能做出正确的判断。本篇博客将从网卡，CPU，内存，磁盘出发来介绍一些基本的知识和性能评测方式，最后也会介绍 Linux 下常用的性能测试工具 sysbench 的使用和性能监控工具 glances 的使用。</p><h2 id="网卡"><a href="#网卡" class="headerlink" title="网卡"></a>网卡</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>网卡，即网络接口卡（network interface card），也叫 NIC 卡，是一种允许网络连接的计算机硬件设备。网卡应用广泛，市场上有许多不同种类，如 PCIe网卡，服务器网卡。</p><p>基于不同的速度，网卡有 10Mbps，100Mbps， 10/100Mbps 自适应卡，1000Mbps、10Gbps、25Gbps、100Gbps 甚至更高速度的网卡。10Mbps、100Mbps 和 10/100Mbps 自适应网卡适用于小型局域网、家庭或办公室。1000Mbps 网卡可为快速以太网提供更高的带宽。10Gbps,25Gbps,100Gbps 网卡以及更高速度的网卡则受到大企业与数据中心的欢迎。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>Linux 下要想得知本机的网卡速率。可以查也可以测。<br>对于查，可以使用 <code>ifconfig</code> 找到网卡对应的设备名，然后使用 <code>ethtool &lt;device&gt;</code> 来查看对应网卡设备的额定速率。如下图所示：<br><img src="/operating-system-performance-testing/ethtool.png" srcset="/img/loading.gif" alt><br>对于测，可以使用 iperf 或者 iperf3 来测量 tcp/udp 协议的吞吐量，进而测试网卡的性能。当然，由于连接参数（比如客户端连接数，包大小）的不同，测试时可能也跑不满理论带宽，因此对实际线上系统调优操作系统网络栈就是一个可以优化性能的方向。<br>iperf 和 iperf3 的具体使用方式可参考此<a href="https://www.cnblogs.com/cloudwas/p/13084815.html" target="_blank" rel="noopener">博客</a>。</p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>市面上常说的千兆网卡速率是 1000Mbps，即 125MB/s；万兆网卡速率是 10000Mbps，即 1.22GB/s；十万兆网卡速率是 100000Mbps，即 12.21 GB/s。</p><h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>中央处理器（CPU），是电子计算机的主要设备之一，电脑中的核心配件。其功能主要是解释计算机指令以及处理计算机软件中的数据。CPU 是计算机中负责读取指令，对指令译码并执行指令的核心部件。中央处理器主要包括两个部分，即控制器、运算器，其中还包括高速缓冲存储器及实现它们之间联系的数据、控制的总线。</p><p>CPU 的性能与以下衡量指标都有关系，具体可以参考此<a href="https://mp.weixin.qq.com/s/Hrn9J2l_Di3gZy0BvPcx6A" target="_blank" rel="noopener">博客</a>。</p><ul><li>主频（时钟频率）</li><li>外频（基准频率）</li><li>总线(FSB)频率</li><li>CPU 的位和字长</li><li>倍频系数</li><li>缓存</li><li>CPU 内核和 I/O 工作电压</li><li>制造工艺</li><li>指令集</li><li>CPU 扩展指令集</li><li>架构（如 UMA 或者 NUMA 架构），具体可以参考此<a href="https://www.cnblogs.com/linhaostudy/p/9980383.html" target="_blank" rel="noopener">博客</a>。</li></ul><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><p>Linux 下想要得知本机有关 CPU 的信息，可以使用 <code>lscpu</code> 指令，其会显示 CPU 的型号，架构，主频大小，缓存大小，物理和逻辑核心数等等，如下图所示。<br><img src="/operating-system-performance-testing/lscpu.png" srcset="/img/loading.gif" alt></p><p>我们在知道 CPU 型号之后也可以去相关网站寻找更多有关此 cpu 的数据信息，比如<a href="https://www.cpubenchmark.net/" target="_blank" rel="noopener">PassMark</a>。</p><p>在系统运行过程中，可以评测 CPU 使用率的一个重要参考指标就是平均负载（load average）。如下图所示<br><img src="/operating-system-performance-testing/load.png" srcset="/img/loading.gif" alt></p><p>平均负载是指单位时间内平均活跃进程数，包括可运行状态的进程数，以及不可中断状态的进程（如等待 IO,等待硬件设备响应），其可以一定程度上反映一段时间内 CPU 的繁忙程度。但是平均负载高 CPU 的使用率不一定高，其主要表现如下，具体可参考此<a href="https://www.cnblogs.com/maxwellsky/p/10629564.html" target="_blank" rel="noopener">博客</a>。</p><ul><li>CPU 密集型进程，导致平均负载和 CPU 使用率比较高</li><li>IO 密集型进程，等待 IO 会导致平均负载升高，但是 CPU 使用率不一定高</li><li>等待 CPU 的进程调度也会导致平均负载升高，此时 CPU 使用率也高</li></ul><h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h2><h3 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h3><p>内存是计算机中重要的部件之一，它是与 CPU 进行沟通的桥梁。计算机中所有程序的运行都是在内存中进行的，因此内存的性能对计算机的影响非常大。内存也被称为内存储器，其作用是用于暂时存放 CPU 中的运算数据，以及与硬盘等外部存储器交换的数据。只要计算机在运行中，CPU 就会把需要运算的数据调到内存中进行运算，当运算完成后 CPU 再将结果传送出来，内存的运行也决定了计算机的稳定运行。内存是由内存芯片、电路板、金手指等部分组成的。</p><p>有关内存的发展，分类等详细信息可以参考<a href="https://baike.baidu.com/item/%E5%86%85%E5%AD%98" target="_blank" rel="noopener">百度百科</a>。</p><h3 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h3><p>Linux 可以通过 <code>free -h</code> 命令来查看本机的内存大小等等，如下图所示<br><img src="/operating-system-performance-testing/memory.png" srcset="/img/loading.gif" alt><br>具体参数含义为：</p><ul><li>total：服务器内存总大小：31G</li><li>used：已经使用了多少内存：26G</li><li>free：未被任何应用使用的真实空闲内存</li><li>shared：被共享的物理内存</li><li>buff/cache：缓冲、缓存区内存数，缓存在应用之中</li><li>available：真正剩余的可被程序应用的内存数</li><li>Swap：swap 分区的大小</li></ul><h4 id="free-和-available-的区别"><a href="#free-和-available-的区别" class="headerlink" title="free 和 available 的区别"></a>free 和 available 的区别</h4><p>free 是真正尚未被使用的物理内存数量。<br>available 是应用程序认为可用内存数量，available = free + buffer + cache (注：只是大概的计算方法)</p><p>Linux 为了提升读写性能，会消耗一部分内存资源缓存磁盘数据，对于内核来说，buffer 和 cache 其实都属于已经被使用的内存。但当应用程序申请内存时，如果 free 内存不够，内核就会回收 buffer 和 cache 的内存来满足应用程序的请求。</p><h4 id="buff-和-cache-的区别"><a href="#buff-和-cache-的区别" class="headerlink" title="buff 和 cache 的区别"></a>buff 和 cache 的区别</h4><p>buffer 名为缓冲，cache 名为缓存。</p><p><img src="/operating-system-performance-testing/buffer_cache.png" srcset="/img/loading.gif" alt></p><ul><li><p>cache：文件系统层级的缓存，从磁盘里读取的内容是存储到这里，这样程序读取磁盘内容就会非常快，比如使用 grep 和 find 等命令查找内容和文件时，第一次会慢很多，再次执行就快好多倍，几乎是瞬间。但如上所说，如果对文件的更新不关心，就没必要清 cache，否则如果要实施同步，必须要把内存空间中的 cache 清楚下。</p></li><li><p>buffer：磁盘等块设备的缓冲，内存的这一部分是要写入到磁盘里的。这种情况需要注意，位于内存 buffer 中的数据不是即时写入磁盘，而是系统空闲或者 buffer 达到一定大小统一写到磁盘中，所以断电易失，为了防止数据丢失所以我们最好正常关机或者多执行几次 sync 命令，让位于 buffer 上的数据立刻写到磁盘里。</p></li></ul><h4 id="swap-是什么"><a href="#swap-是什么" class="headerlink" title="swap 是什么"></a>swap 是什么</h4><p>在 Linux 下，swap 的作用类似 Windows 系统下的“虚拟内存”。当物理内存不足时，拿出部分硬盘空间当 swap 分区（虚拟成内存）使用，从而解决内存容量不足的情况。</p><p>swap 意思是交换，顾名思义，当某进程向 OS 请求内存发现不足时，OS 会把内存中暂时不用的数据交换出去，放在 swap 分区中，这个过程称为 swap out。当某进程又需要这些数据且 OS 发现还有空闲物理内存时，又会把 swap 分区中的数据交换回物理内存中，这个过程称为 swap in。</p><p>当然，swap 大小是有上限的，一旦 swap 使用完，操作系统会触发 OOM-Killer 机制，把消耗内存最多的进程 kill 掉以释放内存。</p><p>有关数据库与 swap 的关系以及 swap 的细节机制可以参考此<a href="http://hbasefly.com/2017/05/24/hbase-linux/?lkfgjq=xbbdl2&amp;ekpqfu=awbp92" target="_blank" rel="noopener">博客</a>。</p><h3 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h3><p>CPU 的处理速度一般约为 10GB /s，常用的 DDR4 内存到 CPU 的吞吐一般约在 30GB/s ，所以内存的吞吐量往往不会成为瓶颈。当然，不同的语言对内存管理的方式不同，某些自动管理内存的语言（比如 Java）会出现 STW (Stop the world）的状况来调整内存空间，其会暂停所有用户线程，对性能影响很大。因此，基于业务负载和语言特性制定更好的 GC 策略能够更好地利用 CPU，从而进一步发掘内存的吞吐量。</p><h2 id="磁盘"><a href="#磁盘" class="headerlink" title="磁盘"></a>磁盘</h2><h3 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h3><p>磁盘是指利用磁记录技术存储数据的存储器。 磁盘是计算机主要的存储介质，可以存储大量的二进制数据，并且断电后也能保持数据不丢失。 早期计算机使用的磁盘是软磁盘（Floppy Disk，简称软盘），如今常用的磁盘是硬磁盘（Hard disk，简称硬盘）。</p><p>目前常见的硬盘大可分为三类：机械硬盘（HDD）采用磁性碟片来存储，固态硬盘（SSD）采用闪存颗粒来存储，混合硬盘（HHD）是把磁性碟片和闪存集成到一起的一种硬盘。</p><p>HDD 和 SSD 的区别主要如下：<br><img src="/operating-system-performance-testing/comparison.jpg" srcset="/img/loading.gif" alt></p><p>总体上来说，HDD 主要为 SATA 和 SAS 接口，目前家用类别的 HDD 多为 SATA 接口，SAS 接口则为企业级应用。SAS 可满足高性能、高可靠性的应用，SATA 则满足大容量、非关键业务的应用。</p><p>此外业界也常用多块 HDD 来组装磁盘阵列（比如 RAID 5 等等），从而提供更好的吞吐量和磁盘级别的容错，具体细节可参考此<a href="https://mp.weixin.qq.com/s/WRYQFRnf28RBK3CUkN7REg" target="_blank" rel="noopener">博客</a>。</p><p>至于固态硬盘，其常见接口和速率如下图所示：<br><img src="/operating-system-performance-testing/ssd.jpg" srcset="/img/loading.gif" alt></p><h3 id="测试-3"><a href="#测试-3" class="headerlink" title="测试"></a>测试</h3><p>Linux 可以通过 <code>df -hT</code> 命令来查看本机磁盘设备挂载的磁盘目录，文件系统以及容量等等，如下图所示<br><img src="/operating-system-performance-testing/df.png" srcset="/img/loading.gif" alt></p><p>此外还可以通过 <code>fdisk -l &lt;device&gt;</code> 命令来查看磁盘设备的源信息，比如可以通过此命令来查看磁盘是 SSD 还是 HDD，如果有”heads”（磁头），”track”（磁道）和”cylinders”（柱面）等等则就是 HDD，否则则是 SSD。如下图所示就是 SSD。<br><img src="/operating-system-performance-testing/fdisk.png" srcset="/img/loading.gif" alt></p><pre><code class="hljs yaml"><span class="hljs-comment"># SSD 示例</span><span class="hljs-attr">Disk /dev/nvme0n1:</span> <span class="hljs-number">238.5</span> <span class="hljs-string">GiB,</span> <span class="hljs-number">256060514304</span> <span class="hljs-string">bytes,</span> <span class="hljs-number">500118192</span> <span class="hljs-string">sectors</span><span class="hljs-attr">Units:</span> <span class="hljs-string">sectors</span> <span class="hljs-string">of</span> <span class="hljs-number">1</span> <span class="hljs-string">*</span> <span class="hljs-number">512</span> <span class="hljs-string">=</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-string">Sector</span> <span class="hljs-string">size</span> <span class="hljs-string">(logical/physical):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-string">I/O</span> <span class="hljs-string">size</span> <span class="hljs-string">(minimum/optimal):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-attr">Disklabel type:</span> <span class="hljs-string">dos</span><span class="hljs-attr">Disk identifier:</span> <span class="hljs-number">0xad91c214</span><span class="hljs-comment"># HDD 示例</span><span class="hljs-attr">Disk /dev/sda:</span> <span class="hljs-number">120.0</span> <span class="hljs-string">GB,</span> <span class="hljs-number">120034123776</span> <span class="hljs-string">bytes</span><span class="hljs-number">255</span> <span class="hljs-string">heads,</span> <span class="hljs-number">63</span> <span class="hljs-string">sectors/track,</span> <span class="hljs-number">14593</span> <span class="hljs-string">cylinders</span><span class="hljs-string">Units</span> <span class="hljs-string">=</span> <span class="hljs-string">cylinders</span> <span class="hljs-string">of</span> <span class="hljs-number">16065</span> <span class="hljs-string">*</span> <span class="hljs-number">512</span> <span class="hljs-string">=</span> <span class="hljs-number">8225280</span> <span class="hljs-string">bytes</span><span class="hljs-string">Sector</span> <span class="hljs-string">size</span> <span class="hljs-string">(logical/physical):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-string">I/O</span> <span class="hljs-string">size</span> <span class="hljs-string">(minimum/optimal):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-attr">Disk identifier:</span> <span class="hljs-number">0x00074f7d</span></code></pre><p>如果想要测试磁盘的 IOPS 或者吞吐量，则可以使用一些磁盘测试工具进行测试，如 smartctl，sysbench 等等，后文会介绍如何使用 sysbench 测量磁盘性能。</p><p>有关文件系统的区别（xfs or ext4），可以参考此<a href="https://segmentfault.com/a/1190000008481493" target="_blank" rel="noopener">博客</a>。至于具体性能，由于上层应用不同，其在不同文件系统上的表现也不同，建议实际测试一下才能知道真实场景下哪个更优秀，一般情况下选择 xfs 就够用了。</p><p>有关磁盘 IO 的理论细节可以参考美团技术团队的<a href="https://tech.meituan.com/2017/05/19/about-desk-io.html" target="_blank" rel="noopener">博客</a>，十分详细。</p><h3 id="补充-2"><a href="#补充-2" class="headerlink" title="补充"></a>补充</h3><ul><li>吞吐量<ul><li>对于顺序读写，普通的 HDD 的吞吐量一般在 100 MB/s 左右，某些企业级 HDD 能够达到 200 MB/s 左右。对于随机读写，HDD 的性能表现很差，一般是几十兆每秒。</li><li>对于顺序读写，普通的 SSD 的吞吐量一般在 300~500MB/s 左右，某些企业级 SSD（例如适配 PCIE 3.0 接口）甚至能够达到 2GB/s。对于随机读写，SSD 的性能相比顺序读写下降较少，一般也是几百兆每秒。 </li></ul></li><li>时延<ul><li>图胜千言<br><img src="/operating-system-performance-testing/latency.jpg" srcset="/img/loading.gif" alt></li></ul></li></ul><h2 id="sysbench-测试工具"><a href="#sysbench-测试工具" class="headerlink" title="sysbench 测试工具"></a>sysbench 测试工具</h2><p>sysbench 是一款开源的多线程性能测试工具，可以执行 CPU/内存/线程/IO/数据库等方面的性能测试。<br>sysbench 支持以下几种测试模式 ：</p><ol><li>CPU 运算性能</li><li>内存分配及传输速度</li><li>磁盘 IO 性能</li><li>POSIX 线程性能</li><li>互斥锁性能测试</li><li>数据库性能(OLTP 基准测试)。目前 sysbench 主要支持 MySQL，PostgreSQL 等几种数据库。</li></ol><p>sysbench 的安装建议直接参考<a href="https://github.com/akopytov/sysbench" target="_blank" rel="noopener">官方 repo</a>，最详细最不踩坑。具体负载参数可以用 <code>sysbench --help</code> 查看也可以参考官方 repo，当然也可以查看此<a href="https://blog.51cto.com/3842834/2364563" target="_blank" rel="noopener">博客</a>和此<a href="https://blog.51cto.com/cyent/2350925" target="_blank" rel="noopener">博客</a>。</p><p>以下列出几种测试实例：</p><h3 id="CPU-运算性能"><a href="#CPU-运算性能" class="headerlink" title="CPU 运算性能"></a>CPU 运算性能</h3><h4 id="单核性能"><a href="#单核性能" class="headerlink" title="单核性能"></a>单核性能</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=cpu</span> <span class="hljs-string">--cpu-max-prime=5000</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Prime numbers limit:</span> <span class="hljs-number">5000</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">CPU speed:</span>    <span class="hljs-attr">events per second:</span>  <span class="hljs-number">3325.67</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0004s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">33274</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.30</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.30</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">1.21</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.30</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">9995.62</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">33274.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.9956</span><span class="hljs-string">/0.00</span></code></pre><h4 id="多核性能"><a href="#多核性能" class="headerlink" title="多核性能"></a>多核性能</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=cpu</span> <span class="hljs-string">--cpu-max-prime=5000</span> <span class="hljs-string">--threads=192</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">192</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Prime numbers limit:</span> <span class="hljs-number">5000</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">CPU speed:</span>    <span class="hljs-attr">events per second:</span> <span class="hljs-number">411077.30</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0017s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">4112237</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.30</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.46</span>         <span class="hljs-attr">max:</span>                                  <span class="hljs-number">147.50</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.46</span>         <span class="hljs-attr">sum:</span>                              <span class="hljs-number">1885541.21</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">21417.9010</span><span class="hljs-string">/378.98</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.8205</span><span class="hljs-string">/0.14</span></code></pre><h3 id="内存吞吐量"><a href="#内存吞吐量" class="headerlink" title="内存吞吐量"></a>内存吞吐量</h3><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=memory</span> <span class="hljs-string">--memory-block-size=4k</span> <span class="hljs-string">--memory-total-size=740G</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Running memory speed test with the following options:</span>  <span class="hljs-attr">block size:</span> <span class="hljs-string">4KiB</span>  <span class="hljs-attr">total size:</span> <span class="hljs-string">757760MiB</span>  <span class="hljs-attr">operation:</span> <span class="hljs-string">write</span>  <span class="hljs-attr">scope:</span> <span class="hljs-string">global</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">Total operations:</span> <span class="hljs-number">31972684</span> <span class="hljs-string">(3195628.16</span> <span class="hljs-string">per</span> <span class="hljs-string">second)</span><span class="hljs-number">124893.30</span> <span class="hljs-string">MiB</span> <span class="hljs-string">transferred</span> <span class="hljs-string">(12482.92</span> <span class="hljs-string">MiB/sec)</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0001s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">31972684</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">0.10</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.00</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">7085.34</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">31972684.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">7.0853</span><span class="hljs-string">/0.00</span></code></pre><h3 id="磁盘性能"><a href="#磁盘性能" class="headerlink" title="磁盘性能"></a>磁盘性能</h3><h4 id="顺序读写"><a href="#顺序读写" class="headerlink" title="顺序读写"></a>顺序读写</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=fileio</span> <span class="hljs-string">--num-threads=1</span> <span class="hljs-string">--file-num=20</span> <span class="hljs-string">--file-total-size=4G</span> <span class="hljs-string">--file-test-mode=seqrewr</span> <span class="hljs-string">prepare(run)</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Extra file open flags:</span> <span class="hljs-string">(none)</span><span class="hljs-number">20</span> <span class="hljs-string">files,</span> <span class="hljs-number">204.</span><span class="hljs-string">8MiB</span> <span class="hljs-string">each</span><span class="hljs-string">4GiB</span> <span class="hljs-string">total</span> <span class="hljs-string">file</span> <span class="hljs-string">size</span><span class="hljs-string">Block</span> <span class="hljs-string">size</span> <span class="hljs-string">16KiB</span><span class="hljs-string">Periodic</span> <span class="hljs-string">FSYNC</span> <span class="hljs-string">enabled,</span> <span class="hljs-string">calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">each</span> <span class="hljs-number">100</span> <span class="hljs-string">requests.</span><span class="hljs-string">Calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">at</span> <span class="hljs-string">the</span> <span class="hljs-string">end</span> <span class="hljs-string">of</span> <span class="hljs-string">test,</span> <span class="hljs-string">Enabled.</span><span class="hljs-string">Using</span> <span class="hljs-string">synchronous</span> <span class="hljs-string">I/O</span> <span class="hljs-string">mode</span><span class="hljs-string">Doing</span> <span class="hljs-string">sequential</span> <span class="hljs-string">rewrite</span> <span class="hljs-string">test</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">File operations:</span>    <span class="hljs-attr">reads/s:</span>                      <span class="hljs-number">0.00</span>    <span class="hljs-attr">writes/s:</span>                     <span class="hljs-number">50759.51</span>    <span class="hljs-attr">fsyncs/s:</span>                     <span class="hljs-number">10153.80</span><span class="hljs-attr">Throughput:</span>    <span class="hljs-string">read,</span> <span class="hljs-attr">MiB/s:</span>                  <span class="hljs-number">0.00</span>    <span class="hljs-string">written,</span> <span class="hljs-attr">MiB/s:</span>               <span class="hljs-number">793.07</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0011s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">609479</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.02</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">1.68</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.01</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">9834.28</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">609479.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.8343</span><span class="hljs-string">/0.00</span></code></pre><h4 id="乱序读写"><a href="#乱序读写" class="headerlink" title="乱序读写"></a>乱序读写</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=fileio</span> <span class="hljs-string">--num-threads=1</span> <span class="hljs-string">--file-num=20</span> <span class="hljs-string">--file-total-size=4G</span> <span class="hljs-string">--file-test-mode=rndrw</span> <span class="hljs-string">prepare(run)</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Extra file open flags:</span> <span class="hljs-string">(none)</span><span class="hljs-number">20</span> <span class="hljs-string">files,</span> <span class="hljs-number">204.</span><span class="hljs-string">8MiB</span> <span class="hljs-string">each</span><span class="hljs-string">4GiB</span> <span class="hljs-string">total</span> <span class="hljs-string">file</span> <span class="hljs-string">size</span><span class="hljs-string">Block</span> <span class="hljs-string">size</span> <span class="hljs-string">16KiB</span><span class="hljs-attr">Number of IO requests:</span> <span class="hljs-number">0</span><span class="hljs-attr">Read/Write ratio for combined random IO test:</span> <span class="hljs-number">1.50</span><span class="hljs-string">Periodic</span> <span class="hljs-string">FSYNC</span> <span class="hljs-string">enabled,</span> <span class="hljs-string">calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">each</span> <span class="hljs-number">100</span> <span class="hljs-string">requests.</span><span class="hljs-string">Calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">at</span> <span class="hljs-string">the</span> <span class="hljs-string">end</span> <span class="hljs-string">of</span> <span class="hljs-string">test,</span> <span class="hljs-string">Enabled.</span><span class="hljs-string">Using</span> <span class="hljs-string">synchronous</span> <span class="hljs-string">I/O</span> <span class="hljs-string">mode</span><span class="hljs-string">Doing</span> <span class="hljs-string">random</span> <span class="hljs-string">r/w</span> <span class="hljs-string">test</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">File operations:</span>    <span class="hljs-attr">reads/s:</span>                      <span class="hljs-number">43044.01</span>    <span class="hljs-attr">writes/s:</span>                     <span class="hljs-number">28696.01</span>    <span class="hljs-attr">fsyncs/s:</span>                     <span class="hljs-number">14348.50</span><span class="hljs-attr">Throughput:</span>    <span class="hljs-string">read,</span> <span class="hljs-attr">MiB/s:</span>                  <span class="hljs-number">672.54</span>    <span class="hljs-string">written,</span> <span class="hljs-attr">MiB/s:</span>               <span class="hljs-number">448.36</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0007s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">861345</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.01</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">0.41</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.04</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">9816.61</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">861345.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.8166</span><span class="hljs-string">/0.00</span></code></pre><h3 id="POSIX-线程性能"><a href="#POSIX-线程性能" class="headerlink" title="POSIX 线程性能"></a>POSIX 线程性能</h3><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span>  <span class="hljs-string">--test=threads</span> <span class="hljs-string">--num-threads=200</span> <span class="hljs-string">--thread-yields=100</span> <span class="hljs-string">--thread-locks=1</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">200</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0140s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">54268</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                   <span class="hljs-number">11.03</span>         <span class="hljs-attr">avg:</span>                                   <span class="hljs-number">36.88</span>         <span class="hljs-attr">max:</span>                                   <span class="hljs-number">86.79</span>         <span class="hljs-attr">95th percentile:</span>                       <span class="hljs-number">47.47</span>         <span class="hljs-attr">sum:</span>                              <span class="hljs-number">2001322.28</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">271.3400</span><span class="hljs-string">/3.10</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">10.0066</span><span class="hljs-string">/0.00</span></code></pre><h3 id="互斥锁性能"><a href="#互斥锁性能" class="headerlink" title="互斥锁性能"></a>互斥锁性能</h3><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=mutex</span> <span class="hljs-string">--mutex-num=2048</span> <span class="hljs-string">--mutex-locks=20000</span> <span class="hljs-string">--mutex-loops=5000</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">0.</span><span class="hljs-string">0387s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">1</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                   <span class="hljs-number">38.49</span>         <span class="hljs-attr">avg:</span>                                   <span class="hljs-number">38.49</span>         <span class="hljs-attr">max:</span>                                   <span class="hljs-number">38.49</span>         <span class="hljs-attr">95th percentile:</span>                       <span class="hljs-number">38.25</span>         <span class="hljs-attr">sum:</span>                                   <span class="hljs-number">38.49</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">1.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">0.0385</span><span class="hljs-string">/0.00</span></code></pre><h2 id="glances-监控工具"><a href="#glances-监控工具" class="headerlink" title="glances 监控工具"></a>glances 监控工具</h2><p>glances 是一个基于 python 语言开发，可以为 Linux 或者 Unix 性能提供监视和分析性能数据的功能。glances 在用户的终端上显示重要的系统信息，并动态的进行更新，让管理员实时掌握系统资源的使用情况，而动态监控并不会消耗大量的系统资源，比如 CPU 资源，通常消耗小于 2%，glances 默认每两秒更新一次数据。同时 glances 还可以将监控数据导出到文件中，便于以后使用其他可视化工具（例如 grafana）对报告进行分析和图形绘制。</p><p>glances 可以分析系统的：</p><ul><li>CPU 使用率</li><li>内存使用率</li><li>内核统计信息和运行队列信息</li><li>磁盘 I/O 速度、传输和读/写比率</li><li>磁盘适配器</li><li>网络 I/O 速度、传输和读/写比率</li><li>页面监控</li><li>进程监控-消耗资源最多的进程</li><li>计算机信息和系统资源</li></ul><p>当然，也可以用一些专业的服务器监控平台，其包含的信息可能更详细。但是 glances 的一个重要优点是开箱即用，不用专门部署，具体安装方式和参数可参考<a href="https://github.com/nicolargo/glances" target="_blank" rel="noopener">官方 repo</a>，使用示例如下图所示：<br><img src="/operating-system-performance-testing/glances.jpeg" srcset="/img/loading.gif" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客简单介绍了网卡，CPU，内存和磁盘的一些理论知识和我个人认为写的比较好的博客，最后介绍了 Linux 下常用的性能测试工具 sysbench 的使用和性能监控工具 glances 的使用。</p><p>本篇博客涉及了较多硬件和操作系统的知识，其实这些都可以挖的更深，由于水平和时间有限，暂不继续深挖，希望看完本博客之后能对大家的系统调优有所帮助。</p>]]></content>
    
    
    
    <tags>
      
      <tag>操作系统</tag>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开源协议解读</title>
    <link href="/open-source-license/"/>
    <url>/open-source-license/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>越来越多的开发者与设计者希望将自己的产品开源，以便其他人可以在他们的代码基础上做更多事，开源社区也因此充满生机。然而一旦开源，如何为代码选择开源许可证就是一个非常重要的问题。</p><p>相信很多人和我一样，在 Github 创建 repo 时对可以选择的多种 license 很迷糊，不知道该选哪个好，因此本篇博客就简单介绍一下常见的 license 和他们之间的区别，以做记录和学习。</p><h2 id="什么是许可协议"><a href="#什么是许可协议" class="headerlink" title="什么是许可协议"></a>什么是许可协议</h2><p>关于授权的确切含义存在很多困惑。当你授权你的作品时，你并没有放弃你的任何权利，你仍然拥有该作品的原始版权(或专利)。许可协议所做的是授予其他人使用该作品的特定权限。</p><p>不管产品是免费向公众分发还是出售，制定一份许可协议非常有用，否则，对于前者，你相当于放弃了自己所有的权利，任何人都没有义务表明你的原始作者身份，对于后者，你将不得不花费比开发更多的精力用来逐个处理用户的授权问题。</p><p>开源许可使得其他人无需寻求特殊许可就可以很容易地对项目做出贡献。它还保护了你作为原始创造者的身份，确保你至少能从自己的贡献中获得一些荣誉，这也有助于防止别人把你的工作据为己有。</p><h2 id="常见许可协议"><a href="#常见许可协议" class="headerlink" title="常见许可协议"></a>常见许可协议</h2><p>目前，国际公认的开源许可证共有 80 多种。它们的共同特征是，都允许用户免费地使用、修改、共享源码，但是都有各自的使用条件。以下介绍一些常见的许可协议</p><h3 id="GNU-GPL"><a href="#GNU-GPL" class="headerlink" title="GNU GPL"></a>GNU GPL</h3><p>GNU GPL（GNU General Public License）同其它的自由软件许可证一样，许可社会公众享有：运行、复制软件的自由，发行传播软件的自由，获得软件源码的自由，改进软件并将自己作出的改进版本向社会发行传播的自由。 </p><p>GPL 还规定：只要这种修改文本在整体上或者其某个部分来源于遵循 GPL 的程序，该修改文本的整体就必须按照 GPL 流通，不仅该修改文本的源码必须向社会公开，而且对于这种修改文本的流通不准许附加修改者自己作出的限制。因此，一项遵循 GPL 流通的程序不能同非自由的软件合并。GPL 所表达的这种流通规则称为 copyleft，表示与 copyright(版权)的概念“相左”。</p><p>GPL 协议最主要的几个原则：</p><ol><li>确保软件自始至终都以开放源代码形式发布，保护开发成果不被窃取用作商业发售。任何一套软件，只要其中使用了受 GPL 协议保护的第三方软件的源程序，并向非开发人员发布时，软件本身也就自动成为受 GPL 保护并且约束的实体。也就是说，此时它必须开放源代码。</li><li>GPL 大致就是一个左侧版权（Copyleft，或译为“反版权”、“版权属左”、“版权所无”、“版责”等）的体现。你可以去掉所有原作的版权信息，只要你保持开源，并且随源代码、二进制版附上 GPL 的许可证就行，让后人可以很明确地得知此软件的授权信息。GPL 精髓就是，只要使软件在完整开源的情况下，尽可能使使用者得到自由发挥的空间，使软件得到更快更好的发展。</li><li>无论软件以何种形式发布，都必须同时附上源代码。例如在 Web 上提供下载，就必须在二进制版本（如果有的话）下载的同一个页面，清楚地提供源代码下载的链接。如果以光盘形式发布，就必须同时附上源文件的光盘。</li><li>开发或维护遵循 GPL 协议开发的软件的公司或个人，可以对使用者收取一定的服务费用。但还是一句老话——必须无偿提供软件的完整源代码，不得将源代码与服务做捆绑或任何变相捆绑销售。</li></ol><p>GPL 协议和 BSD, Apache Licence 等鼓励代码重用的许可协议很不一样。GPL 的出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但不允许修改后和衍生的代码做为闭源的商业软件发布和销售。这也就是为什么我们能用免费的各种 linux，包括商业公司的 linux 和 linux 上各种各样的由个人，组织，以及商业软件公司开发的免费软件了。</p><h3 id="GNU-LGPL"><a href="#GNU-LGPL" class="headerlink" title="GNU LGPL"></a>GNU LGPL</h3><p>LGPL 是 GPL 的一个为主要为类库使用设计的开源协议。和 GPL 要求任何使用/修改/衍生之 GPL 类库的的软件必须采用 GPL 协议不同。LGPL 允许商业软件通过类库引用(link)方式使用 LGPL 类库而不需要开源商业软件的代码。这使得采用 LGPL 协议的开源代码可以被商业软件作为类库引用并发布和销售。</p><p>但是如果修改 LGPL 协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用 LGPL 协议。因此 LGPL 协议的开源代码很适合作为第三方类库被商业软件引用，但不适合希望以 LGPL 协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。</p><p>GPL/LGPL 都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品。</p><h3 id="GNU-AGPL"><a href="#GNU-AGPL" class="headerlink" title="GNU AGPL"></a>GNU AGPL</h3><p>原有的 GPL 协议，由于现在云服务厂商兴起（如：AWS）产生了一定的漏洞，比如使用 GPL 的自由软件，但是并不发布于网络之中，则可以自由的使用 GPL 协议却不开源自己私有的解决方案。AGPL 增加了对此做法的约束。</p><p>GPL 的约束生效的前提是“发布”软件，即使用了 GPL 成分的软件通过互联网或光盘 release 软件，就必需明示地附上源代码，并且源代码和产品也受 GPL 保护。</p><p>这样如果不“发布”就可以不受约束了。比如使用 GPL 组件编写一个 Web 系统，不发布这个系统，但是用这个系统在线提供服务，同时不开源系统代码。</p><h3 id="GNU-SSPL"><a href="#GNU-SSPL" class="headerlink" title="GNU SSPL"></a>GNU SSPL</h3><p>SSPL (Server Side Public License) 为 MongoDB 基于 GPLv3 上修改并提出的软件授权协议。</p><p>MongoDB 认为 AGPL “Remote Network Interaction” 条款叙述不够明确，容易造成混淆。加上许多云端服务商一直在挑战 AGPL 的底线，大量使用 MongoDB 来进行营利行为却不遵守 AGPL，所以才提出明确定义的 SSPL。</p><p>SSPL服务器端公共授权。许可证更改并不影响当前使用社区服务器的常规用户。根据 MongoDB 之前的 GNU AGPLv3 协议，想要将 MongoDB 作为公共服务运行的公司必须将他们的软件开源，或需要从 MongoDB 获得商业许可，该公司解释说，“然而，MongoDB 的普及使一些组织在违反 GNU AGPLv3 协议的边缘疯狂试探，甚至直接违反了协议。”</p><p>尽管 SSPL 与 GNU GPLv3 没有什么不同，但 SSPL 会明确要求托管 MongoDB 实例的云计算公司要么从 MongoDB 获取商业许可证，要么向社区开源其服务代码。最近闹得特别火的 <a href="https://mp.weixin.qq.com/s/qCJx3sw-Om3y1JwRoCnDbQ" target="_blank" rel="noopener">ElasticSearch 修改开源协议</a>就是从 Apache 2.0 修改到了 SSPL 协议以限制 AWS 厂商。</p><h3 id="BSD"><a href="#BSD" class="headerlink" title="BSD"></a>BSD</h3><p>BSD 是 “Berkeley Software Distribution” 的缩写，意思是”伯克利软件发行版”。</p><p>BSD 在软件分发方面的限制比别的开源协议（如 GNU GPL）要少。该协议有多种版本，最主要的版本有两个，新 BSD 协议与简单 BSD 协议，这两种协议经过修正，都和 GPL 兼容，并为开源组织所认可。</p><p>新 BSD 协议（3 条款协议）在软件分发方面，除需要包含一份版权提示和免责声明之外，没有任何限制。另外，该协议还禁止拿开发者的名义为衍生产品背书，但简单 BSD 协议删除了这一条款。</p><h3 id="MIT"><a href="#MIT" class="headerlink" title="MIT"></a>MIT</h3><p>MIT 是和 BSD 一样宽范的许可协议，源自麻省理工学院（Massachusetts Institute of Technology, MIT），又称 X11 协议。</p><p>MIT 协议可能是几大开源协议中最宽松的一个，核心条款是：该软件及其相关文档对所有人免费，可以任意处置，包括使用，复制，修改，合并，发表，分发，再授权，或者销售。唯一的限制是，软件中必须包含上述版权和许可提示。</p><p>这意味着：你可以自由使用，复制，修改，可以用于自己的项目。可以免费分发或用来盈利。唯一的限制是必须包含许可声明。</p><p>MIT 协议是所有开源许可中最宽松的一个，除了必须包含许可声明外，再无任何限制。</p><h3 id="Apache"><a href="#Apache" class="headerlink" title="Apache"></a>Apache</h3><p>Apache License（Apache 许可证），是 Apache 软件基金会发布的一个自由软件许可证。</p><p>Apache 协议 2.0 和别的开源协议相比，除了为用户提供版权许可之外，还有专利许可，对于那些涉及专利内容的开发者而言，该协议最适合。</p><p>Apache 协议还有以下需要说明的地方:</p><ol><li>永久权利：一旦被授权，永久拥有。</li><li>全球范围的权利：在一个国家获得授权，适用于所有国家。假如你在美国，许可是从印度授权的，也没有问题。</li><li>授权免费，且无版税：前期，后期均无任何费用。</li><li>授权无排他性：任何人都可以获得授权</li><li>授权不可撤消：一旦获得授权，没有任何人可以取消。比如，你基于该产品代码开发了衍生产品，你不用担心会在某一天被禁止使用该代码。</li></ol><p>分发代码方面包含一些要求，主要是，要在声明中对参与开发的人给予认可并包含一份许可协议原文。</p><h3 id="MPL"><a href="#MPL" class="headerlink" title="MPL"></a>MPL</h3><p>MPL（Mozilla Public License 1.1）协议允许免费重发布、免费修改，但要求修改后的代码版权归软件的发起者。这种授权维护了商业软件的利益，它要求基于这种软件的修改无偿贡献版权给该软件。这样，围绕该软件的所有代码的版权都集中在发起开发人的手中。但 MPL 是允许修改，无偿使用得。MPL 软件对链接没有要求。（要求假如你修改了一个基于MPL协议的源代码，则必须列入或公开你所做的修改，假如其他源代码不是基于MPL则不需要公开其源代码）</p><h3 id="Creative-Commons"><a href="#Creative-Commons" class="headerlink" title="Creative Commons"></a>Creative Commons</h3><p>Creative Commons (CC) 并非严格意义上的开源许可，它主要用于设计。Creative Commons 有多种协议，每种都提供了相应授权模式，CC 协议主要包含 4 种基本形式：</p><ol><li>署名权<br>必须为原始作者署名，然后才可以修改，分发，复制。</li><li>保持一致<br>作品同样可以在 CC 协议基础上修改，分发，复制。</li><li>非商业<br>作品可以被修改，分发，复制，但不能用于商业用途。但商业的定义有些模糊，比如，有的人认为非商业用途指的是不能销售，有的认为是甚至不能放在有广告的网站，也有人认为非商业的意思是非盈利。</li><li>不能衍生新作品<br>你可以复制，分发，但不能修改，也不能以此为基础创作自己的作品。</li></ol><p>CC 许可协议的这些条款可以自由组合使用。大多数的比较严格的 CC 协议会声明 “署名权，非商业用途，禁止衍生”条款，这意味着你可以自由的分享这个作品，但你不能改变它和对其收费，而且必须声明作品的归属。这个许可协议非常的有用，它可以让你的作品传播出去，但又可以对作品的使用保留部分或完全的控制。最少限制的 CC 协议类型当属 “署名”协议，这意味着只要人们能维护你的名誉，他们对你的作品怎么使用都行。</p><h2 id="许可协议区别"><a href="#许可协议区别" class="headerlink" title="许可协议区别"></a>许可协议区别</h2><p>根据使用条件的不同，开源许可证分成两大类。</p><ul><li>宽松式许可证</li><li>Copyleft 许可证</li></ul><h3 id="宽松式许可证"><a href="#宽松式许可证" class="headerlink" title="宽松式许可证"></a>宽松式许可证</h3><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>宽松式许可证（permissive license）是最基本的类型，对用户几乎没有限制。用户可以修改代码后闭源。</p><p>它有三个基本特点。</p><ol><li>没有使用限制：用户可以使用代码，做任何想做的事情。</li><li>没有担保：不保证代码质量，用户自担风险。</li><li>披露要求（notice requirement）：用户必须披露原始作者。</li></ol><h4 id="常见许可证"><a href="#常见许可证" class="headerlink" title="常见许可证"></a>常见许可证</h4><p>常见的宽松式许可证有四种。它们都允许用户任意使用代码，区别在于要求用户遵守的条件不同。</p><ol><li>BSD（二条款版）：分发软件时，必须保留原始的许可证声明。</li><li>BSD（三条款版）：分发软件时，必须保留原始的许可证声明。不得使用原始作者的名字为软件促销。</li><li>MIT：分发软件时，必须保留原始的许可证声明，与 BSD（二条款版）基本一致。</li><li>Apache 2：分发软件时，必须保留原始的许可证声明。凡是修改过的文件，必须向用户说明该文件修改过；没有修改过的文件，必须保持许可证不变。</li></ol><h3 id="Copyleft-许可证"><a href="#Copyleft-许可证" class="headerlink" title="Copyleft 许可证"></a>Copyleft 许可证</h3><h4 id="Copyleft-的含义"><a href="#Copyleft-的含义" class="headerlink" title="Copyleft 的含义"></a>Copyleft 的含义</h4><p>Copyleft 是理查德·斯托曼发明的一个词，作为 Copyright （版权）的反义词。</p><p>Copyright 直译是”复制权”，这是版权制度的核心，意为不经许可，用户无权复制。作为反义词，Copyleft 的含义是不经许可，用户可以随意复制。</p><p>但是，它带有前提条件，比宽松式许可证的限制要多。</p><ul><li>如果分发二进制格式，必须提供源码</li><li>修改后的源码，必须与修改前保持许可证一致</li><li>不得在原始许可证以外，附加其他限制</li></ul><p>上面三个条件的核心就是：修改后的 Copyleft 代码不得闭源。</p><h4 id="常见许可证-1"><a href="#常见许可证-1" class="headerlink" title="常见许可证"></a>常见许可证</h4><p>常见的 Copyleft 许可证也有四种（对用户的限制从最强到最弱排序）。</p><ol><li>Affero GPL (AGPL)：如果云服务（即 SAAS）用到的代码是该许可证，那么云服务的代码也必须开源。</li><li>GPL：如果项目包含了 GPL 许可证的代码，那么整个项目都必须使用 GPL 许可证。</li><li>LGPL：如果项目采用动态链接调用该许可证的库，项目可以不用开源。</li><li>Mozilla（MPL）：只要该许可证的代码在单独的文件中，新增的其他文件可以不用开源。</li></ol><h3 id="区别示意图"><a href="#区别示意图" class="headerlink" title="区别示意图"></a>区别示意图</h3><p>图胜千言<br><img src="/open-source-license/license1.png" srcset="/img/loading.gif" alt><br><img src="/open-source-license/license2.jpeg" srcset="/img/loading.gif" alt></p><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="什么叫分发（distribution）"><a href="#什么叫分发（distribution）" class="headerlink" title="什么叫分发（distribution）"></a>什么叫分发（distribution）</h3><p>除了 Affero GPL (AGPL) ，其他许可证都规定只有在”分发”时，才需要遵守许可证。换言之，如果不”分发”，就不需要遵守。</p><p>简单说，分发就是指将版权作品从一个人转移到另一个人。这意味着，如果你是自己使用，不提供给他人，就没有分发。另外，这里的”人”也指”法人”，因此如果使用方是公司，且只在公司内部使用，也不需要遵守许可证。</p><p>云服务（SaaS）是否构成”分发”呢？答案是不构成。所以你使用开源软件提供云服务，不必提供源码。但是，Affero GPL (AGPL) 许可证除外，它规定云服务也必须提供源码。</p><h3 id="开源软件的专利如何处理"><a href="#开源软件的专利如何处理" class="headerlink" title="开源软件的专利如何处理"></a>开源软件的专利如何处理</h3><p>某些许可证（Apache 2 和 GPL v3）包含明确的条款，授予用户许可，使用软件所包含的所有专利。</p><p>另一些许可证（BSD、MIT 和 GPL v2）根本没提到专利。但是一般认为，它们默认给予用户专利许可，不构成侵犯专利。</p><p>总得来说，除非有明确的”保留专利”的条款，使用开源软件都不会构成侵犯专利。</p><h3 id="什么是披露要求"><a href="#什么是披露要求" class="headerlink" title="什么是披露要求"></a>什么是披露要求</h3><p>所有的开源许可证都带有”披露要求”（notice requirement），即要求软件的分发者必须向用户披露，软件里面有开源代码。</p><p>一般来说，你只要在软件里面提供完整的原始许可证文本，并且披露原始作者，就满足了”披露要求”。</p><h3 id="GPL-病毒是真的吗"><a href="#GPL-病毒是真的吗" class="headerlink" title="GPL 病毒是真的吗"></a>GPL 病毒是真的吗</h3><p>GPL 许可证规定，只要你的项目包含了 GPL 代码，整个项目就都变成了 GPL。有人把这种传染性比喻成”GPL 病毒”。</p><p>很多公司希望避开这个条款，既使用 GPL 软件，又不把自己的专有代码开源。理论上，这是做不到的。因为 GPL 的设计目的，就是为了防止出现这种情况。</p><p>但是实际上，不遵守 GPL，最坏情况就是被起诉。如果你向法院表示无法履行 GPL 的条件，法官只会判决你停止使用 GPL 代码（法律上叫做”停止侵害”），而不会强制要求你将源码开源，因为《版权法》里面的”违约救济”没有提到违约者必须开源，只提到可以停止侵害和赔偿损失。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>随着时代的发展，开源协议也一直在迭代进化，希望我们大家都能够了解开源协议，遵守开源协议并捍卫开源协议。</p><p>这篇博客参考了许多相关博客以做一个汇总总结，如有侵权之处可联系我删除~</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html" target="_blank" rel="noopener">如何选择开源许可证？(阮一峰)</a><br><a href="https://zhuanlan.zhihu.com/p/135292511" target="_blank" rel="noopener">简介5大开源许可协议</a><br><a href="https://mp.weixin.qq.com/s/HDNYTwifzGOYTcu3x3wFjg" target="_blank" rel="noopener">拒绝云服务商白嫖，Elasticsearch 和 Kibana 变更开源许可协议</a><br><a href="https://mp.weixin.qq.com/s/wwlAH2MBAsujaPeqKqtjmw" target="_blank" rel="noopener">全球各种开源协议介绍</a><br><a href="https://mp.weixin.qq.com/s/1UXjwKjX22vIvRimr5BLNw" target="_blank" rel="noopener">若你要开源自己的代码，此文带你了解开源协议</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>开源</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Twitter 内存缓存系统分析论文阅读</title>
    <link href="/twitter-cache-analysis-thesis/"/>
    <url>/twitter-cache-analysis-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>基于一个物理现实：内存操作比磁盘操作快若干个数量级。现代网站服务广泛使用了类似 redis ，memcached 的内存缓存系统。其思想很简单：尽管海量数据最终都需要落盘持久化，但如果能够将一些常用的数据在内存中缓存以供读请求直接获取，则不仅能够降低请求时延，还能够提升系统吞吐量，而且也能够减少底层数据管理系统比如关系型数据库的负载。</p><p>内存缓存系统的大规模商用激发了业界对其的研究，现有的研究主要集中在如何提高吞吐量，如何减少缓存缺失率等等。由于用户场景与缓存系统的性能息息相关，之前也出现了生产系统的分析，但其针对的用户场景过少。这就导致了理论和生产环境之间的一个巨大 gap，因此业界需要一个能够包含大量用户场景的内存缓存系统分析；此外，业界对内存缓存系统的前提假设都是写少读多，而且许多理论比如内存管理都是基于数据大小是恒定这一假设来做的，那么实际上线的生产环境也都是理论假设的这样吗？此外，一些看似不重要的特性，比如 TTL 受到了业界极少的关注，那么它对于生产系统的性能影响真的很小吗？带着这些问题，我们开始介绍这篇论文。</p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>首先介绍数据情况，Twitter 内部的内存缓存集群都是单租户单层的容器化实例，这样的架构方便作者直接将集群的分析结果与商业逻辑对应，更能够反映现象的真实原因。可以看到，Twitter 的内存缓存集群很大。</p><p><img src="/twitter-cache-analysis-thesis/1.png" srcset="/img/loading.gif" alt></p><p>为了保证分析不受采样方法的影响，作者未取样的收集了两段区间为一周的集群请求全量元信息，在 80TB 的数据基础上做了详细的分析。而且，作者也将数据适度脱敏后进行了开源，能够让任何人去继续分析。</p><p><img src="/twitter-cache-analysis-thesis/2.png" srcset="/img/loading.gif" alt></p><h2 id="缓存场景"><a href="#缓存场景" class="headerlink" title="缓存场景"></a>缓存场景</h2><p>Twitter 内部的内存缓存系统主要有三种应用场景：</p><p><img src="/twitter-cache-analysis-thesis/3.png" srcset="/img/loading.gif" alt></p><h3 id="存储缓存"><a href="#存储缓存" class="headerlink" title="存储缓存"></a>存储缓存</h3><p>将一些在磁盘上的热点数据缓存在内存中，来减少底层涉及磁盘的数据系统比如关系型数据库的压力。</p><h3 id="计算缓存"><a href="#计算缓存" class="headerlink" title="计算缓存"></a>计算缓存</h3><p>缓存一些计算结果或预结算结果。随着 BI 时代的到来，许多公司都会利用机器学习的方法做用户画像或者是实时流推荐。这些算法的每次计算很难在秒级以下，而且用户的画像也一般不会在几小时内发生变化，因此企业一般会每计算一次用户的画像就将其设置一个几分钟或几小时的 TTL 存储到缓存中，这样既能够大幅度减少计算资源的消耗也能够保持 BI 逻辑，这种使用方式正在逐渐流行。</p><h3 id="瞬态数据缓存"><a href="#瞬态数据缓存" class="headerlink" title="瞬态数据缓存"></a>瞬态数据缓存</h3><p>有一些数据是不需要持久化的，只存在于内存缓存中。比如限速场景会把用户的请求存储到缓存中进行统计，一旦超速即开始限速，过期后隐形删除这些统计日志即可。</p><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><h3 id="写多读少场景广泛存在"><a href="#写多读少场景广泛存在" class="headerlink" title="写多读少场景广泛存在"></a>写多读少场景广泛存在</h3><p>如下图所示，这是一个有关写请求占总请求比率的集群比例概率分布图。<br><img src="/twitter-cache-analysis-thesis/4.png" srcset="/img/loading.gif" alt><br>作者将比率大于 30% 的集群定义为写多读少负载，可以看到 twitter 内部至少 35 % 的集群都是写多读少的负载，这打破了业界对于内存缓存系统的前提假设，也展示了生产和理论之间的 gap。对于写多读少场景，长尾效应，扩展性受限都成了问题，而这一块却几乎没有学者研究过，因此作者为业界指明了一条研究方向：即写多读少场景内存缓存系统的优化。</p><h3 id="对象大小"><a href="#对象大小" class="headerlink" title="对象大小"></a>对象大小</h3><p>下图是集群中有关对象大小的若干集群比例概率分布图。<br><img src="/twitter-cache-analysis-thesis/5.png" srcset="/img/loading.gif" alt></p><h4 id="元数据大小"><a href="#元数据大小" class="headerlink" title="元数据大小"></a>元数据大小</h4><p>从前三张子图中可以看到，大部分对象数据都很小，那相比之下，原本被忽视的元数据消耗就非常大了，比如 memcached 中每个对象的元数据就有 56 字节。现有的研究大多数都是通过增加元数据来提高缓存命中率的。而实际上对于小对象场景来说，最小化元数据的大小能够增大缓存个数，也是一条提升性能的方向。</p><h4 id="key-大小"><a href="#key-大小" class="headerlink" title="key 大小"></a>key 大小</h4><p>在上图的第四张子图中可以观察到，对于 60 % 的 workload，key 和 value 的大小都在一个数量级，这很让人惊异。在观察了许多实际 key 后，作者发现很多业务使用了较为冗长的 key，比如把很长的多段命名空间都扔到了里面，这浪费了许多空间。由于直接让业务团队将 key 改小是不可行的，所以内存缓存系统也可以为 key 增加一个轻量级的压缩，这样也能够有效的增大缓存空间。这个工作实现起来很容易，但之前没有人观察到过这个现象，因此这也是作者的贡献。</p><h3 id="对象大小的动态分布"><a href="#对象大小的动态分布" class="headerlink" title="对象大小的动态分布"></a>对象大小的动态分布</h3><p>当前，内存缓存系统大多假设对象随着时间的推移大小不变，然而在生产系统中，作者观察到大部分时间中，其对象大小的分布并不是恒定的。</p><p><img src="/twitter-cache-analysis-thesis/6.png" srcset="/img/loading.gif" alt></p><p>这个不恒定主要有两种表现形式:</p><ul><li><p>第一种是周期性的变化，比如很多负载都具有白天对象更大的特征。如图所示，亮度发生变化代表分布发生了变化，很多亮度是有周期出现的。</p></li><li><p>第二种是临时的突变，过后即恢复。如图所示，存在个别无规律的亮点。</p></li></ul><p>这些突变给缓存系统的内存管理带来了很多挑战，就如同操作系统的内部碎片和外部碎片一样。作者实际测试发现，现有缓存系统的主流内存管理技术 slab-class ，在面对可变对象大小时表现十分受限，作者认为内存管理技术方面需要更大的创新，比如随着机器学习的发展，动态的预测缓存行为并做出智能的内存管理就是一个可行的方向。</p><h3 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h3><p>TTL，一个限定对象生命周期的内存缓存系统特有参数，作者进行了细致的分析。对应前面提到内存缓存的三种场景，TTL 都有作用。</p><h4 id="TTL-使用场景"><a href="#TTL-使用场景" class="headerlink" title="TTL 使用场景"></a>TTL 使用场景</h4><p><img src="/twitter-cache-analysis-thesis/7.png" srcset="/img/loading.gif" alt></p><ul><li>保证不一致上界：部分对一致性要求较低的业务可能在写缓存失败时并不重试，这可能最终导致缓存与实际数据的不一致，因此用户可以使用 TTL 这个属性给这个最终一致性的区间定一个上界，从而达到性能和不一致上限都得到保证的可控结果。</li><li>定期更新：用户画像随着时间推移可能逐渐变得不准确，不实时，因此需要一个 TTL 属性来隐性指示计算，即哪怕新一轮计算结果还没触发，每过 TTL 时间请求用户的画像就得重新计算一次，这其实是一个实时性和计算资源之间的 trade-off。</li><li>隐形删除：比如限速场景可以把用户每条请求数据的 TTL 设置为限速的时间窗口，这样既能够达到限速的目的又能够在一段时间后隐形删除掉这些数据。</li></ul><h4 id="小-TTL-能够限制工作集合的大小"><a href="#小-TTL-能够限制工作集合的大小" class="headerlink" title="小 TTL 能够限制工作集合的大小"></a>小 TTL 能够限制工作集合的大小</h4><p><img src="/twitter-cache-analysis-thesis/8.png" srcset="/img/loading.gif" alt></p><p>如图所示，作者统计了在有无 TTL 场景下的对象集合总大小和活跃对象集合大小，可以看到活跃的对象集合大小相比总大小始终在一个固定范围内。因此如果能够对过期数据清理得当，那么实际上不需要很大的缓存资源就能提供一个不错的缓存命中率。因此作者得出了结论，有效的主动过期策略比驱逐还要更重要。</p><p>现有的主动驱逐策略主要有两种：一种是 Facebook 提出的环形缓冲区策略，它在 TTL 个数较少或者多但不连续时表现都不好；另一种是当前 redis 的定期删除策略，由于其是遍历实现的，而为了保证及时删除扫表时间至少要和最小的 ttl 在一个量级 ，则对于较大的 ttl，其数据会被扫描多次。这样也一定程度上造成了 cpu 时间的浪费，而且也容易有缓存雪崩问题。因此作者认为有效的主动过期策略也需要进一步的创新。</p><h3 id="更多发现"><a href="#更多发现" class="headerlink" title="更多发现"></a>更多发现</h3><h4 id="生产数据统计"><a href="#生产数据统计" class="headerlink" title="生产数据统计"></a>生产数据统计</h4><p>请求激增不一定就是热点导致的，可能就是均匀的涨了一些。</p><h4 id="对象分布"><a href="#对象分布" class="headerlink" title="对象分布"></a>对象分布</h4><p>尽管有些许偏差，但针对对象的请求基本符合幂率分布</p><h4 id="驱逐策略"><a href="#驱逐策略" class="headerlink" title="驱逐策略"></a>驱逐策略</h4><p>尽管不同负载的表现情况不一样，但 FIFO 与 LRU 策略在大部分负载下的表现近似。这也预示业务可能实际没必要费事费力的去搞 LRU 策略，简单的 FIFO 就能达到类似的性能的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>证明了写多读少内存缓存场景的广泛存在，指明了一个可以研究的领域。</li><li>大多数场景的对象很小，要想提升吞吐量可以从减少元数据大小和适度压缩 key 入手。</li><li>在内存缓存的生产系统中，对象的大小并不是恒定不变的。基于对象大小固定不变进行的理论推理都存在问题。</li><li>有效的主动过期策略比驱逐策略更管用。</li></ul><h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><p>本文作者基于世界上最大的实时内容公司之一 Twitter 的内存缓存统计数据，使用强有力的数据分析纠正了若干业界对于内存缓存的误区，辩驳了若干业界的前提假设和主流思想，并提出了若干研究方向，开创了多个子领域。我个人认为这是一篇很有意义的文章。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://www.usenix.org/system/files/osdi20-yang.pdf" target="_blank" rel="noopener">论文</a><br><a href="https://www.usenix.org/sites/default/files/conference/protected-files/osdi20_slides_yang.pdf" target="_blank" rel="noopener">PPT</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark 论文阅读</title>
    <link href="/spark-thesis/"/>
    <url>/spark-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 Apache Spark 广泛使用以前，业界主要使用 MapReduce 和 Dryad 这样的集群计算框架来对大数据进行分布式处理。这类计算框架，最大的优点旨在帮助程序员专注业务编程，而非花精力分发计算任务和实现程序容错。</p><p>MapReduce 虽然对利用集群中的计算资源做了各类抽象，但还没有实现对集群内存的抽象封装。这样对那些需要重复利用中间结果集的应用就很不友好，比如机器学习和图算法，PageRank, K-means 聚类以及逻辑回归等等。此外，MapReduce 也很难支持高效的交互式数据分析，因涉及大量的即席数据查询，为确保下一次数据集可以被重用，需要借助存储物化结果集，这引发大量写入实体磁盘的操作，导致执行时间拉长。</p><p>意识到这个问题的存在，学者们做了大量尝试，比如 Pregel，它把大量中间数据缓存起来，专为图计算封装了框架；HaLoop 则提供了实现迭代算法的 MapReduce 接口。但这些仅仅对个案有帮助，回到通用的计算上来，毫无优势。比如最常见的数据分析，装载多样化多源头数据，展开即席查询等等。</p><p>综上，MapReduce 的局限可以总结为：</p><ul><li>编程模型的表达能力有限，仅靠 MapReduce 难以实现部分算法。</li><li>对分布式内存资源的使用方式有限，使得其难以满足迭代式分析场景和交互式分析场景，比如迭代式机器学习算法及图算法，交互式数据挖掘等。</li></ul><p>Spark RDD 作为一个分布式内存资源抽象便致力于解决 Hadoop MapReduce 的上述问题：</p><ul><li>通过对分布式集群的内存资源进行抽象，允许程序高效复用已有的中间结果。</li><li>提供比 MapReduce 更灵活的编程模型，兼容更多的高级算法。</li></ul><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset，弹性分布式数据集）本质上是一种只读、分片的记录集合，只能由所支持的数据源或是由其他 RDD 经过一定的转换（Transformation）来产生。通过由用户构建 RDD 间组成的产生关系图，每个 RDD 都能记录到自己是如何由还位于持久化存储中的源数据计算得出的，即其血统（Lineage）。</p><p><img src="/spark-thesis/lineage.jpg" srcset="/img/loading.gif" alt></p><p>Spark 为 RDD 提供了 Transformation 和 Action 两种操作，前者可以从其他数据源读入数据生成 RDD 或利用已有的 RDD 生成新的只读 RDD。后者可对 RDD 进行计算操作并把一个结果值返回给客户端，或是将 RDD 里的数据写出到外部存储。</p><p><img src="/spark-thesis/Transformation_Action.jpg" srcset="/img/loading.gif" alt></p><p>Transformation 与 Action 的区别还在于，对 RDD 进行 Transformation 并不会触发计算：Transformation 方法所产生的 RDD 对象只会记录住该 RDD 所依赖的 RDD 以及计算产生该 RDD 的数据的方式；只有在用户进行 Action 操作时，Spark 才会调度 RDD 计算任务，依次为各个 RDD 计算数据，这是 RDD 典型的惰性计算。</p><h2 id="分布式共享内存模型对比"><a href="#分布式共享内存模型对比" class="headerlink" title="分布式共享内存模型对比"></a>分布式共享内存模型对比</h2><p>相比于 RDD 只能通过粗粒度的”转换”来创建（或是说写入数据），分布式共享内存（Distributed Shared Memory，DSM）是另一种分布式系统常用的分布式内存抽象模型：应用在使用分布式共享内存时可以在一个全局可见的地址空间中进行随机的读写操作。类似的系统包括了一些常见的分布式内存数据库（如 Redis、Memcached）。其最大的优点在于写一次，多机同步。集群中的所有计算机节点，在同一内存位置存储了同一份数据。弊端也很明显，一旦数据损坏，所有数据都要重新还原或重做；同步导致的延迟会很高，因为系统要保障数据的完整性，这在分布式数据库中常见。</p><p><img src="/spark-thesis/comparison.png" srcset="/img/loading.gif" alt></p><p>RDD 产生的方式限制了其只适用于那些只会进行批量数据写入的应用程序，但却使得 RDD 可以使用更为高效的高可用机制。RDD 与 DSM 的区别在于，前者是粗放式写入，通过转换函数生成，而后者在内存任意位置均可写入。RDD 不能很好地支持大批量随机写入，却可以很好的支持批量写入和分区容错。前面也说道，血统依赖是 RDD 容错的利器，丢失分区可重生。</p><p>RDD 的第二大优势在于，备份节点可以迅速的被唤起，去代替那些缓慢节点执行任务。即在缓慢节点执行任务的同时，备份节点同时也执行相同的任务，哪个节点快就用那个节点的结果。而 DSM 则会被备份节点干扰，引起大家同时缓慢，因为共享内存之间会同步状态，互相干扰。</p><p>RDD 还有两大优化点：基于数据存储分发任务和溢出缓存至硬盘。在大量写入的操作中，比如生成 RDD，会选择离数据最近的节点开始任务；而在只读操作中，大量数据没法存入内存时，会自动存到硬盘上而不是报错停止执行。</p><h2 id="计算调度"><a href="#计算调度" class="headerlink" title="计算调度"></a>计算调度</h2><p>前面我们提到，RDD 在物理形式上是分片的，其完整数据被分散在集群内若干机器的内存上。当用户通过 Transformation 创建出新的 RDD 后，新的 RDD 与原本的 RDD 便形成了依赖关系。根据用户所选 Transformation 操作的不同，RDD 间的依赖关系可以被分为两种：</p><ul><li>窄依赖（Narrow Dependency）：父 RDD 的每个分片至多被子 RDD 中的一个分片所依赖</li><li>宽依赖（Wide Dependency）：父 RDD 中的分片可能被子 RDD 中的多个分片所依赖</li></ul><p><img src="/spark-thesis/dependency.jpg" srcset="/img/loading.gif" alt></p><p>通过将窄依赖从宽依赖中区分出来，Spark 便可以针对 RDD 窄依赖进行一定的优化。首先，窄依赖使得位于该依赖链上的 RDD 计算操作可以被安排到同一个集群节点上流水线进行；其次，在节点失效需要恢复 RDD 时，Spark 只需要恢复父 RDD 中的对应分片即可，恢复父分片时还能将不同父分片的恢复任务调度到不同的节点上并发进行。</p><p>总的来说，一个 RDD 由以下几部分组成：</p><ul><li>其分片集合</li><li>其父 RDD 集合</li><li>计算产生该 RDD 的方式</li><li>描述该 RDD 所包含数据的模式、分片方式、存储位置偏好等信息的元数据</li></ul><p>在用户调用 Action 方法触发 RDD 计算时，Spark 会按照定义好的 RDD 依赖关系绘制出完整的 RDD 血统依赖，并根据图中各节点间依赖关系的不同对计算过程进行切分：</p><p><img src="/spark-thesis/stage.jpg" srcset="/img/loading.gif" alt></p><p>简单来说，Spark 会把尽可能多的可以流水线执行的窄依赖 Transformation 放到同一个 Job Stage 中，而 Job Stage 之间则要求集群对数据进行 Shuffle。Job Stage 划分完毕后，Spark 便会为每个 Partition 生成计算任务（Task）并调度到集群节点上运行。</p><p>在调度 Task 时，Spark 也会考虑计算该 Partition 所需的数据的位置：例如，如果 RDD 是从 HDFS 中读出数据，那么 Partition 的计算就会尽可能被分配到持有对应 HDFS Block 的节点上；或者，如果 Spark 已经将父 RDD 持有在内存中，子 Partition 的计算也会被尽可能分配到持有对应父 Partition 的节点上。对于不同 Job Stage 之间的 Data Shuffle，目前 Spark 采取与 MapReduce 相同的策略，会把中间结果持久化到节点的本地存储中，以简化失效恢复的过程。</p><p>当 Task 所在的节点失效时，只要该 Task 所属 Job Stage 的父 Job Stage 数据仍可用，Spark 只要将该 Task 调度到另一个节点上重新运行即可。如果父 Job Stage 的数据也已经不可用了，那么 Spark 就会重新提交一个计算父 Job Stage 数据的 Task，以完成恢复。有趣的是，从论文来看，Spark 当时还没有考虑调度模块本身的高可用，不过调度模块持有的状态只有 RDD 的血统图和 Task 分配情况，通过状态备份的方式实现高可用也是十分直观的。</p><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>Spark 为 RDD 提供了三种存储格式：</p><ul><li>内存中反序列化的 Java 对象；</li><li>内存中序列化的 Java 对象；</li><li>硬盘存储</li></ul><p>访问速度从快到慢，即第一种方式最快，无需任何转换就可以被自由访问。最后一种最慢，因每次使用，需从硬盘抽取数据，有不必要的 IO 开销。</p><p>当内存吃紧，新建的 RDD 分区没有足够内存存储时，Spark 会采用回收分区方式，以给新分区提供空间。回收机制采用的是常规 LRU（Least Recently Used）算法，即最近最少使用的算法。这套回收机制很有用，至少目前来说是。但权值机制也很有用，比如设定 RDD 的权限等级，控制 RDD 分区被回收的可能性。</p><h2 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h2><p>前面已经提到，Spark 可以利用血统依赖来恢复出现故障的 RDD，这样即可不用对中间结果做持久化。然而，在部分长链场景下，做 checkpoint 来持久化也是有必要的。这是因为如果血统依赖足够长，在故障之后，RDD 的恢复需要经历相当多的步骤，会导致时间过多的消耗，此时如果有 checkpoint 即可减少较多的时间消耗。</p><p>Spark 将 checkpoint 的决策留给了用户。实现 checkpoint 的 API 是 persist 的 replicate 开关，即:<br><pre><code class="hljs css"><span class="hljs-selector-tag">rdd</span><span class="hljs-selector-class">.persist</span>(<span class="hljs-selector-tag">REPLICATE</span>)</code></pre></p><p>通过定期将数据暂存至稳定的存储设备，可以保证在性能不大幅度下降的情况下优化 RDD 失效后的过长重算。</p><h2 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h2><p><img src="/spark-thesis/performance.jpg" srcset="/img/loading.gif" alt></p><p>Spark 在性能方面表现出众，对标物是 Hadoop，以下是基于 Amazon EC2 做出的 4 组对比数据：</p><ol><li>在图运算和迭代机器学习方面，优先 Hadoop 20 倍速度。性能的提高得益于无需硬盘 I/O，且在内存中的 Java 对象计算，没有序列化和反序列化的开销。</li><li>性能与扩展性都很好。单测一张分析报表，就比 Hadoop 提高了 40 倍性能</li><li>当有节点故障时，Spark 能自动恢复已丢失的分区</li><li>查询 1TB 的数据，延迟仅在 5-7 秒。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，Spark RDD 的亮点在于如下两点：</p><ul><li>通过对分布式集群的内存资源进行抽象，允许程序高效复用已有的中间结果且保证高可用性。</li><li>提供比 MapReduce 更灵活的编程模型，兼容更多的高级算法。</li></ul><p>比起类似于分布式内存数据库的那种分布式共享内存模型，Spark RDD 巧妙地利用了其不可变和血统依赖的特性实现了对分布式内存资源的抽象，很好地支持了批处理程序的使用场景，同时大大简化了节点失效后的数据恢复过程。</p><p>同时，我们也应该意识到，Spark 是对 MapReduce 的一种补充而不是替代：将那些能够已有的能够很好契合 MapReduce 模型的计算作业迁移到 Spark 上不会收获太多的好处（例如普通的 ETL 作业）。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf" target="_blank" rel="noopener">论文</a><br><a href="https://zhuanlan.zhihu.com/p/36288538" target="_blank" rel="noopener">Spark 博客</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么选择 git 来作为代码版本控制系统</title>
    <link href="/git-or-svn/"/>
    <url>/git-or-svn/</url>
    
    <content type="html"><![CDATA[<p>转载一篇介绍代码版本控制系统的<a href="https://zhuanlan.zhihu.com/p/27374348" target="_blank" rel="noopener">博客</a>，此外也可参考此<a href="https://www.zhihu.com/question/25491925" target="_blank" rel="noopener">博客</a>。</p><h2 id="版本控制系统"><a href="#版本控制系统" class="headerlink" title="版本控制系统"></a>版本控制系统</h2><p>代码作为软件研发的核心产物，在整个开发周期都在递增，不断合入新需求以及解决 bug 的新 patch，这就需要有一款系统，能够存储、追踪文件的修改历史，记录多个版本的开发和维护。于是，版本控制系统（Version Control Systems）应运而生，主要分为两类，集中式和分布式。</p><h2 id="集中式版本控制系统"><a href="#集中式版本控制系统" class="headerlink" title="集中式版本控制系统"></a>集中式版本控制系统</h2><p><img src="https://pic2.zhimg.com/80/v2-974ee843e5b45fa3c81701dcd86ce8e9_1440w.png" srcset="/img/loading.gif" alt></p><p>集中式版本控制系统的特点是只有一台中央服务器，存放着所有研发数据，而其它客户端机器上保存的是中央服务器最新版本的文件快照，不包括项目文件的变更历史。所以，每个相关人员工作开始前，都需要从这台中央服务器同步最新版本，才能开始工作。</p><p><img src="https://pic3.zhimg.com/80/v2-0b59fbfed7cd73abaf5682b88413f062_1440w.png" srcset="/img/loading.gif" alt></p><p>集中式版本控制系统的优点：</p><ol><li>操作简单，使用没有难度，可轻松上手。</li><li>文件夹级权限控制，权限控制粒度小。</li><li>对客户端配置要求不高，无需存储全套代码。</li></ol><p>集中式版本控制系统的缺点：</p><ol><li>网络环境要求高，相关人员必须联网才能工作。</li><li>中央服务器的单点故障影响全局，如果服务器宕机，所有人都无法工作。</li><li>中央服务器在没有备份的情况下，磁盘一旦被损坏，将丢失所有数据。</li></ol><h2 id="分布式版本控制系统"><a href="#分布式版本控制系统" class="headerlink" title="分布式版本控制系统"></a>分布式版本控制系统</h2><p><img src="https://pic2.zhimg.com/80/v2-fb54a44c9918cd2228224b89a64fa7d1_1440w.png" srcset="/img/loading.gif" alt></p><p>分布式版本控制系统的特点是每个客户端都是代码仓库的完整镜像，包括项目文件的变更历史。所有数据分布的存储在每个客户端，不存在中央服务器。可能有人会问，我们公司使用 git 工具，也有”中央服务器”啊？其实，这个所谓的”中央服务器”仅仅是用来方便管理多人协作，任何一台客户端都可以胜任它的工作，它和所有客户端没有本质区别。</p><p><img src="https://pic3.zhimg.com/80/v2-2de2b87f4b39d2a36df4be7a01ebabfa_1440w.png" srcset="/img/loading.gif" alt></p><p>分布式版本控制系统的优点：</p><ol><li>版本库本地化，版本库的完整克隆，包括标签、分支、版本记录等。</li><li>支持离线提交，适合跨地域协同开发。</li><li>分支切换快速高效，创建和销毁分支廉价。</li></ol><p>分布式版本控制系统的缺点：</p><ol><li>学习成本高，不容易上手。</li><li>只能针对整个仓库创建分支，无法根据目录建立层次性的分支。</li></ol><h2 id="svn-or-git"><a href="#svn-or-git" class="headerlink" title="svn or git?"></a>svn or git?</h2><p>svn 和 git 作为集中式和分布式版本控制系统的代表，都有广大的使用群体，两者的优缺点经常被比较。其实，工具对我们来说，就是帮助我们有效提升工作的效率与质量，最适合的就是最好的。我们引用几个开发场景来看看两个版本控制工具的适用范围。</p><h3 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h3><p>公司 A，非纯技术开发，项目包含大量媒体设计文件，相关人员只需下载自己关注的部分文件；员工 PC 电脑配置不高，没有空间拷贝整个项目资料。</p><p>适用：svn</p><p>分析：只需公司有一个足够大的服务器硬盘，员工本地只存储自己相关的文件夹，不必下载不想关的媒体文件，避免浪费文件传输时间。</p><h3 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h3><p>公司 B，嵌入式底层开发，项目人员较多并且分布在两个城市，代码庞大；用分支管理多机种并行开发，机种间经常相互合并新特性，新 patch。</p><p>适用：git</p><p>分析：</p><ol><li>git 有能力高效管理类似 Linux 内核一样的超大规模项目；</li><li>git 实现了离线开发、代码审核特性，解决了跨地域协同开发中代码质量和编码协同的问题；</li><li>分支管理功能强大，便于查询和追溯分支间的提交历史；</li><li>git 基于 DAG（有向非环图）的设计比 svn 的线性提交提供更好的合并追踪，避免不必要的冲突，提高工作效率。</li></ol><p><img src="https://pic2.zhimg.com/80/v2-6349574ea7ed79d1e9aa65fdd84bb141_1440w.png" srcset="/img/loading.gif" alt></p><h3 id="场景三"><a href="#场景三" class="headerlink" title="场景三"></a>场景三</h3><p>公司 C，某行业软件开发，包含敏感重要数据，代码仓库和版本发布权限掌握在客户手中，代码安全要求高，公司开发人员先将代码提交到本地仓库，只有在客户审核通过才能提交到发布仓库。</p><p>适用：git</p><p>分析：</p><ol><li>git 通过哈希加密保证数据的完整性，防止恶意篡改；</li><li>代码分布存储，异地容灾，保证数据安全；</li><li>git 支持团队成员自建本地版本库和分支，只有客户发出合并请求，开发人员才能提交代码，客户可以对提交说明、代码规范等方面逐一审核。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不难看出，git 凭借自身的优势，完美解决了大多数公司对版本控制工具的诉求。在当今敏捷开发成为主流，研发周期短，跨地域协同开发多的大形势下，选择 git 是大势所趋。</p>]]></content>
    
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>30 条常用 git 命令详解</title>
    <link href="/git-common-command/"/>
    <url>/git-common-command/</url>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>git 是一个强大的分布式版本管理工具，也是计算机专业找工作的必备技能之一。关于 git 的教程网上已经有很多很多，其大致工作流程如下图，此处不再赘述。</p><p><img src="/git-common-command/workload.png" srcset="/img/loading.gif" alt></p><p>本文主要介绍常用的 30 条 git 命令，希望能够大家使用 git 带来一些帮助。</p><h2 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h2><p>git version 2.17.2 (Apple Git-113)</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>以下每条命令也可使用<code>git &lt;command&gt; -h</code>来查看对应官方文档。</p><h3 id="init"><a href="#init" class="headerlink" title="init"></a>init</h3><p>可以使用<code>git init</code>在当前目录创建 git 仓库，也可使用<code>git init &lt;path&gt;</code>在 path 路径下创建目录并创建 git 仓库。注意有个<code>--bare</code>参数可以生成一般作为远程仓库的裸仓库，其不包含工作区，具体可参考此<a href="https://www.jianshu.com/p/5b7ff91c5338" target="_blank" rel="noopener">博客</a>。</p><h3 id="clone"><a href="#clone" class="headerlink" title="clone"></a>clone</h3><p>可以直接使用<code>git clone &lt;url&gt;</code>来克隆对应仓库代码，本地默认存储目录为当前目录下的仓库同名目录，也可在 url 之后指定本地的存储目录名。例如：<br><pre><code class="hljs crmsh">git <span class="hljs-keyword">clone</span> <span class="hljs-title">&lt;url</span>&gt;git <span class="hljs-keyword">clone</span> <span class="hljs-title">&lt;url</span>&gt; <span class="hljs-tag">&lt;dir_name&gt;</span></code></pre></p><p>具体的 url 有四种协议格式，分别是 local，git，ssh，https 协议，具体区别可参考此<a href="https://juejin.im/post/6844903536749182989" target="_blank" rel="noopener">博客</a>和<a href="https://cloud.tencent.com/developer/article/1347791" target="_blank" rel="noopener">博客</a>。</p><p>此外在克隆某些存在较大文件的项目时，可以使用<code>git clone --depth=n &lt;url&gt;</code>来下载该项目 master 分支最近 n 次 commit 的相关文件和引用；使用<code>git clone --single-branch --branch=dev &lt;url&gt;</code>来下载该项目 dev 分支的相关文件和引用。这样可以大大加快克隆的速度。</p><h3 id="config"><a href="#config" class="headerlink" title="config"></a>config</h3><p>通常只用来设置邮箱和用户名。注意有三个级别<code>local</code>，<code>global</code>和<code>system</code>，分别对应项目级别，用户级别和机器级别。默认不指定是<code>local</code>级别，具体区别可参考此<a href="https://www.daixiaorui.com/read/240.html" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs routeros">git<span class="hljs-built_in"> config </span>user.email <span class="hljs-string">"TXYPotato@gmail.com"</span>git<span class="hljs-built_in"> config </span>user.name <span class="hljs-string">"LebronAl"</span></code></pre></p><p>此外，在单机配置多个 git 账号时，除了在目录<code>~/.ssh/</code>下生成 config 文件外，一般也需要在对应仓库重新设置非全局的正确用户名和邮箱以避免身份混淆，具体配置方式可参考此<a href="http://www.chenyp.com/2017/08/11/multiple-git-account/" target="_blank" rel="noopener">博客</a>。</p><h3 id="remote"><a href="#remote" class="headerlink" title="remote"></a>remote</h3><p>通常用来管理上游仓库，可以通过<code>git remote -v</code>来查看相关信息，其相关命令如下：<br><pre><code class="hljs dts"><span class="hljs-comment">// 添加远程仓库关联</span>git remote add <span class="hljs-params">&lt;name&gt;</span> <span class="hljs-params">&lt;url&gt;</span><span class="hljs-comment">// 删除远程仓库关联</span>git remote remove <span class="hljs-params">&lt;name&gt;</span><span class="hljs-comment">// 更名远程仓库关联</span>git remote rename <span class="hljs-params">&lt;old_name&gt;</span> <span class="hljs-params">&lt;new_name&gt;</span><span class="hljs-comment">// 显示某个远程仓库的信息</span>git remote show <span class="hljs-params">&lt;name&gt;</span><span class="hljs-comment">// 更新远程仓库 url</span>git remote set-url <span class="hljs-params">&lt;name&gt;</span> <span class="hljs-params">&lt;new_url&gt;</span></code></pre></p><p>此外，remote 命令也可以为一个远程仓库设置多个 url 地址，这样一次 push 就可以同时对该远程仓库的所有 url 进行推送，比如同时往 github 和 gitee 上推送，具体可参考此<a href="https://www.cnblogs.com/hsd1727728211/p/5331651.html" target="_blank" rel="noopener">博客</a>，相关命令如下：<br><pre><code class="hljs dsconfig"><span class="hljs-string">git </span><span class="hljs-string">remote </span><span class="hljs-built_in">set-url</span> <span class="hljs-built_in">--add</span> &lt;<span class="hljs-string">name&gt;</span> &lt;<span class="hljs-string">url&gt;</span><span class="hljs-string">git </span><span class="hljs-string">remote </span><span class="hljs-built_in">set-url</span> <span class="hljs-built_in">--delete</span> &lt;<span class="hljs-string">name&gt;</span> &lt;<span class="hljs-string">url&gt;</span></code></pre></p><p>此外，remote 命令也可以跟踪远程的所有分支和清理无效的远程分支跟踪，具体可参考此<a href="https://www.jianshu.com/p/884ff6252be5" target="_blank" rel="noopener">博客</a>：<br><pre><code class="hljs delphi"><span class="hljs-comment">// 本地创建远程追踪分支</span>git remote update &lt;<span class="hljs-keyword">name</span>&gt;<span class="hljs-comment">// 本地清理无效的远程追踪分支</span>git remote prune &lt;<span class="hljs-keyword">name</span>&gt;<span class="hljs-comment">// 查看无效的远程追踪分支</span>git remote prune --dry-run &lt;<span class="hljs-keyword">name</span>&gt;</code></pre></p><h3 id="add"><a href="#add" class="headerlink" title="add"></a>add</h3><p>用来从工作区向暂存区添加变更。可以使用<code>git add -e &lt;file&gt;</code>或者<code>git add -p &lt;file&gt;</code>来交互性的添加工作区文件的部分变更到暂存区中去，具体可查看此<a href="https://www.cnblogs.com/zqb-all/p/13020293.html" target="_blank" rel="noopener">博客</a>。</p><p>add 命令支持添加单个文件，带有通配符路径的所有对应文件和所有文件。添加所有文件对应三种命令，有细微的区别，介绍如下：<br><pre><code class="hljs cs"><span class="hljs-comment">// 提交被修改(modified)和被删除(deleted)文件，不包括新文件(new)</span>git <span class="hljs-keyword">add</span> -u<span class="hljs-comment">// 提交当前目录下的所有变化</span>git <span class="hljs-keyword">add</span> .<span class="hljs-comment">// 提交所有变化</span>git <span class="hljs-keyword">add</span> -A</code></pre></p><h3 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h3><p>用于删除工作区文件，并将此次删除放入到暂存区。（注：要删除的文件没有修改过，就是说和当前版本库文件的内容相同。）<br><pre><code class="hljs stata">git <span class="hljs-keyword">rm</span> &lt;<span class="hljs-keyword">file</span>&gt;</code></pre></p><p>用于删除工作区和暂存区文件，并将此次删除放入暂存区。（注：要删除的文件已经修改过，就是说和当前版本库文件的内容不同）<br><pre><code class="hljs stata">git <span class="hljs-keyword">rm</span> -f &lt;<span class="hljs-keyword">file</span>&gt;<span class="hljs-comment">// 对所有文件进行操作</span>git <span class="hljs-keyword">rm</span> -f -r .</code></pre></p><p>用于删除暂存区文件，并将此次删除放入暂存区，但会保留工作区的文件。<br><pre><code class="hljs stata">git <span class="hljs-keyword">rm</span> --cached &lt;<span class="hljs-keyword">file</span>&gt;<span class="hljs-comment">// 对所有文件进行操作</span>git <span class="hljs-keyword">rm</span> --cached -r .</code></pre></p><p>更多例子可参考此<a href="https://blog.csdn.net/qq_42780289/article/details/98353792" target="_blank" rel="noopener">博客</a>。</p><h3 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h3><p>用于移动或重命名一个文件、目录或软链接。<br><pre><code class="hljs xml">git mv <span class="hljs-tag">&lt;<span class="hljs-name">old_file</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">new_file</span>&gt;</span></code></pre><br>相当于<code>mv old_file new_file</code>，<code>git rm old_file</code>，<code>git add new_file</code>这三条命令一起运行，具体可参考此<a href="https://www.cnblogs.com/mf001/p/8663386.html" target="_blank" rel="noopener">博客</a>。</p><p>新文件名已经存在，若想强制覆盖则可以使用 -f 参数：<br><pre><code class="hljs xml">git mv -f <span class="hljs-tag">&lt;<span class="hljs-name">old_file</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">new_file</span>&gt;</span></code></pre></p><h3 id="reset"><a href="#reset" class="headerlink" title="reset"></a>reset</h3><p>用于将指定 commit 和 branch 的文件替换暂存区的文件。有三个常用参数，分别是<code>--hard</code>，<code>--soft</code>，<code>--mixed</code>，默认是<code>--mixed</code>。具体细节和应用场景可参考此<a href="https://www.jianshu.com/p/c2ec5f06cf1a" target="_blank" rel="noopener">博客</a>。</p><p>定义三种动作：</p><ol><li>替换引用的指向，指向新的提交。</li><li>替换暂存区。暂存区内容将和指定的提交内容一样。</li><li>替换工作区。工作区和指定的提交内容一样。</li></ol><pre><code class="hljs pgsql">// 执行 <span class="hljs-number">1</span>。常用于合并多个 <span class="hljs-keyword">commit</span>，类似于 squashgit <span class="hljs-keyword">reset</span> <span class="hljs-comment">--soft [commitId]/&lt;branch&gt;</span>// 执行 <span class="hljs-number">12</span>。常用于移出暂存区的文件以作为 <span class="hljs-keyword">add</span> 命令的反动作git <span class="hljs-keyword">reset</span> (<span class="hljs-comment">--mixed) &lt;commitId&gt;/&lt;branch&gt;</span>// 执行 <span class="hljs-number">123</span>。常用于无条件放弃本地所有变更以向远程分支同步git <span class="hljs-keyword">reset</span> <span class="hljs-comment">--hard &lt;commitId&gt;/&lt;branch&gt;</span>// 将暂存区的所有改动撤销到工作区git <span class="hljs-keyword">reset</span> (<span class="hljs-comment">--mixed) HEAD</span></code></pre><h3 id="stash"><a href="#stash" class="headerlink" title="stash"></a>stash</h3><p>常用来保存和恢复工作进度。注意该命令只对被 git 跟踪的文件有效。这是一个非常有用的命令，具体相关用法可查看此<a href="https://blog.csdn.net/andyzhaojianhui/article/details/80586695" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs sql">// 保存当前工作进度，将工作区和暂存区恢复到修改之前。默认 message 为当前分支最后一次 <span class="hljs-keyword">commit</span> 的 messagegit stash// 作用同上，message 为此次进度保存的说明git stash <span class="hljs-keyword">save</span> message// 显示此次工作进度做了哪些文件行数的变化，此命令的 stash@&#123;<span class="hljs-keyword">num</span>&#125; 是可选项，不带此项则默认显示最近一次的工作进度相当于 git stash <span class="hljs-keyword">show</span> stash@&#123;<span class="hljs-number">0</span>&#125;git stash <span class="hljs-keyword">show</span> stash@&#123;<span class="hljs-keyword">num</span>&#125;// 显示此次工作进度做了哪些具体的代码改动，此命令的 stash@&#123;<span class="hljs-keyword">num</span>&#125; 是可选项，不带此项则默认显示最近一次的工作进度相当于 git stash <span class="hljs-keyword">show</span> stash@&#123;<span class="hljs-number">0</span>&#125; -pgit stash <span class="hljs-keyword">show</span> stash@&#123;<span class="hljs-keyword">num</span>&#125; -p// 显示保存的工作进度列表，编号越小代表保存进度的时间越近git stash <span class="hljs-keyword">list</span>// 恢复工作进度到工作区，此命令的 stash@&#123;<span class="hljs-keyword">num</span>&#125; 是可选项，在多个工作进度中可以选择恢复，不带此项则默认恢复最近的一次进度相当于 git stash pop stash@&#123;<span class="hljs-number">0</span>&#125;git stash pop stash@&#123;<span class="hljs-keyword">num</span>&#125;// 恢复工作进度到工作区且该工作进度可重复恢复，此命令的 stash@&#123;<span class="hljs-keyword">num</span>&#125; 是可选项，在多个工作进度中可以选择恢复，不带此项则默认恢复最近的一次进度相当于 git stash <span class="hljs-keyword">apply</span> stash@&#123;<span class="hljs-number">0</span>&#125;git stash <span class="hljs-keyword">apply</span> stash@&#123;<span class="hljs-keyword">num</span>&#125;// 删除一条保存的工作进度，此命令的 stash@&#123;<span class="hljs-keyword">num</span>&#125; 是可选项，在多个工作进度中可以选择删除，不带此项则默认删除最近的一次进度相当于 git stash <span class="hljs-keyword">drop</span> stash@&#123;<span class="hljs-number">0</span>&#125;git stash <span class="hljs-keyword">drop</span> stash@&#123;<span class="hljs-keyword">num</span>&#125;// 删除所有保存的工作进度git stash <span class="hljs-keyword">clear</span></code></pre><p>此外有时我们只想保存部分文件，可以通过 add 一些不想保存的文件到暂存区去，然后使用<code>git stash --keep-index</code>来将工作区的工作进度保存，从而保留暂存区的进度并保存工作区的进度。具体可参考此<a href="https://juejin.im/post/6844903961770606605" target="_blank" rel="noopener">博客</a>。</p><p>此外虽然 stash 命令默认不包含未跟踪的文件，但我们可以通过显示声明<code>-u</code>的方式来保存未被 git 追踪的文件。即<code>git stash -u</code>。</p><h3 id="status"><a href="#status" class="headerlink" title="status"></a>status</h3><p>用于显示文件和文件夹在工作区和暂存区的状态。<br><pre><code class="hljs lua">git <span class="hljs-built_in">status</span>// 展示中包含被忽略的文件git <span class="hljs-built_in">status</span> <span class="hljs-comment">--ignored</span></code></pre></p><h3 id="log"><a href="#log" class="headerlink" title="log"></a>log</h3><p>用来回顾提交历史。可参考此<a href="https://git-scm.com/book/zh/v2/Git-%E5%9F%BA%E7%A1%80-%E6%9F%A5%E7%9C%8B%E6%8F%90%E4%BA%A4%E5%8E%86%E5%8F%B2" target="_blank" rel="noopener">文档</a>和此<a href="https://www.cnblogs.com/lenomirei/p/8379457.html" target="_blank" rel="noopener">博客</a>。以下只列出几个常用的打印格式：<br><pre><code class="hljs cos">git loggit log --onelinegit log --graph --pretty=format:'<span class="hljs-built_in">%Cred</span><span class="hljs-built_in">%h</span><span class="hljs-built_in">%Creset</span> -<span class="hljs-built_in">%C</span>(yellow)<span class="hljs-built_in">%d</span><span class="hljs-built_in">%Creset</span> <span class="hljs-built_in">%s</span> <span class="hljs-built_in">%Cgreen</span>(<span class="hljs-built_in">%cr</span>) <span class="hljs-built_in">%C</span>(bold blue)&lt;<span class="hljs-built_in">%an</span>&gt;<span class="hljs-built_in">%Creset</span>' --abbrev-commit --date=relative<span class="hljs-comment">// 从 commit 信息中查找与关键字相关的 commit</span>git log --grep &lt;key&gt;<span class="hljs-comment">// 从代码中查找与关键字相关的 commit</span>git log -<span class="hljs-keyword">S</span> &lt;key&gt;</code></pre></p><h3 id="shortlog"><a href="#shortlog" class="headerlink" title="shortlog"></a>shortlog</h3><p>用于汇总 git 日志输出。非常人性化的一个命令。<br><pre><code class="hljs pgsql">// 按照用户列出其 <span class="hljs-keyword">commit</span> 的次数以及每次 <span class="hljs-keyword">commit</span> 的注释git shortlog// 按照 <span class="hljs-keyword">commit</span> 数量从多到少的顺序列出本仓库的贡献者并省略注释git shortlog -sn</code></pre></p><h3 id="diff"><a href="#diff" class="headerlink" title="diff"></a>diff</h3><p>用来比较文件之间的不同。具体用法可参考此<a href="https://blog.csdn.net/wq6ylg08/article/details/88798254" target="_blank" rel="noopener">博客</a>。diff 结果的格式可参考此<a href="http://www.ruanyifeng.com/blog/2012/08/how_to_read_diff.html" target="_blank" rel="noopener">博客</a>了解。<br><pre><code class="hljs dts"><span class="hljs-comment">// 查看工作区与暂存区所有文件的变更</span>git diff<span class="hljs-comment">// 查看暂存区与最后一次 commit 之间所有文件的变更</span>git diff --cached<span class="hljs-comment">// 查看工作区与最后一次 commit 之间所有文件的变更</span>git diff HEAD<span class="hljs-comment">// 查看两次 commit 之间的变动</span>git diff <span class="hljs-params">&lt;commitId&gt;</span>...<span class="hljs-params">&lt;commitId&gt;</span><span class="hljs-comment">// 查看两个分支上最后一次 commit 的内容差别</span>git diff <span class="hljs-params">&lt;branch&gt;</span>...<span class="hljs-params">&lt;branch&gt;</span></code></pre></p><h3 id="branch"><a href="#branch" class="headerlink" title="branch"></a>branch</h3><p>用于分支的操作，比如创建分支，查看分支等等。<br><pre><code class="hljs dts"><span class="hljs-comment">// 可以查看本地分支对应的远程分支</span>git branch -vv <span class="hljs-comment">// 查看远程版本库分支列表</span>git branch -r<span class="hljs-comment">// 查看所有分支列表，包括本地和远程</span>git branch -a<span class="hljs-comment">// 在当前位置新建 name 分支</span>git branch <span class="hljs-params">&lt;name&gt;</span><span class="hljs-comment">// 在指定 commit 上新建 name 分支</span>git branch <span class="hljs-params">&lt;name&gt;</span> <span class="hljs-params">&lt;commitId&gt;</span><span class="hljs-comment">// 强制创建 name 分支，原来的分支会被新建分支覆盖，从而迁移 name 分支指针</span>git branch -f <span class="hljs-params">&lt;name&gt;</span><span class="hljs-comment">// 删除 dev 分支，如果在分支中有一些未 merge 的提交则会失败</span>git branch -d dev<span class="hljs-comment">// 强制删除 dev 分支</span>git branch -D dev<span class="hljs-comment">// 重命名分支</span>git branch -m <span class="hljs-params">&lt;oldName&gt;</span> <span class="hljs-params">&lt;newName&gt;</span></code></pre></p><h3 id="checkout"><a href="#checkout" class="headerlink" title="checkout"></a>checkout</h3><p>用于切换分支或更新工作树文件以匹配索引或指定树中的版本，具体可参考此<a href="https://www.jianshu.com/p/cad4d2ec4da5" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs dts"><span class="hljs-comment">// 切换到指定分支</span>git checkout <span class="hljs-params">&lt;branch&gt;</span><span class="hljs-comment">// 在当前位置新建分支并切换</span>git checkout -b <span class="hljs-params">&lt;branch&gt;</span><span class="hljs-comment">// 在指定 commit 上新建分支并切换</span>git checkout -b <span class="hljs-params">&lt;branch&gt;</span> <span class="hljs-params">&lt;commitId&gt;</span><span class="hljs-comment">// 切换到指定 v1.2 的标签</span>git checkout tags/v1<span class="hljs-number">.2</span><span class="hljs-comment">// 切换到指定 commit</span>git checkout <span class="hljs-params">&lt;commitId&gt;</span><span class="hljs-comment">// 基于当前所在分支新建一个赤裸分支，该分支没有任何的提交历史但包含当前分支的所有内容，相当于 git checkout -b &lt;new_branch&gt; 和 git reset --soft &lt;firstCommitId&gt; 两条命令 </span>git checkout --orphan <span class="hljs-params">&lt;new_branch&gt;</span><span class="hljs-comment">// 放弃工作区单个文件的变更，默认会从暂存区检出该文件，如果暂存区为空，则该文件会回滚到最近一次的提交状态</span>git checkout -- <span class="hljs-params">&lt;filepath&gt;</span><span class="hljs-comment">// 放弃工作区所有文件的变更（不包含未跟踪的）</span>git checkout .</code></pre></p><h3 id="commit"><a href="#commit" class="headerlink" title="commit"></a>commit</h3><p>用于将暂存区里的改动给提交到本地的版本库。<br><pre><code class="hljs pgsql">// 提交一个描述为 message 的 <span class="hljs-keyword">commit</span>git <span class="hljs-keyword">commit</span> -m "message"// 相当于 git <span class="hljs-keyword">add</span> -a 和 git <span class="hljs-keyword">commit</span> -m "message" 两条命令git <span class="hljs-keyword">commit</span> -am "message"// 在不增加一个新 <span class="hljs-keyword">commit</span> 的情况下将新修改的代码追加到前一次的 <span class="hljs-keyword">commit</span> 中，会弹出一个编辑器界面重新编辑 message 信息git <span class="hljs-keyword">commit</span> <span class="hljs-comment">--amend</span>// 在不增加一个新 <span class="hljs-keyword">commit</span> 的情况下将新修改的代码追加到前一次的 <span class="hljs-keyword">commit</span> 中，不需要再修改 message 信息git <span class="hljs-keyword">commit</span> <span class="hljs-comment">--amend --no-edit</span>// 提交一次没有任何改动的空提交，常用于触发远程 cigit <span class="hljs-keyword">commit</span> <span class="hljs-comment">----allow-empty -m "message"</span>// 修改 <span class="hljs-keyword">commit</span> 时间git <span class="hljs-keyword">commit</span> -m "message" <span class="hljs-comment">--date=" Wed May 27 00:35:36 2020 +0800"</span></code></pre></p><h3 id="revert"><a href="#revert" class="headerlink" title="revert"></a>revert</h3><p>撤销某个 commit 的改动。在多人协作中如果某个 commit 已经被推到了远程，此时使用 revert 相比 reset + force-update 是更优雅的撤销变更方式。可参考此<a href="https://www.cnblogs.com/0616--ataozhijia/p/3709917.html" target="_blank" rel="noopener">博客</a>和<a href="https://www.jianshu.com/p/5e7ee87241e4" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs jboss-cli"><span class="hljs-string">//</span> 可以撤销指定的提交git revert &lt;commitId&gt;<span class="hljs-string">//</span> 可以撤销不包括 commit1，但包括 commit2 的一串提交git revert &lt;commit1&gt;<span class="hljs-string">...</span>&lt;commit2&gt;<span class="hljs-string">//</span> revert 命令会对每个撤销的 commit 进行一次提交，<span class="hljs-params">--no-commit</span> 后可以最后一起手动提交git revert <span class="hljs-params">--no-commit</span> &lt;commit1&gt;<span class="hljs-string">...</span>&lt;commit2&gt;<span class="hljs-string">//</span> 退出 revert 过程，常在处理冲突出错时使用git revert <span class="hljs-params">--abort</span><span class="hljs-string">//</span> 继续 revert 过程，常在处理完冲突时使用git revert <span class="hljs-params">--continue</span></code></pre><h3 id="merge"><a href="#merge" class="headerlink" title="merge"></a>merge</h3><p>用于将两个或两个以上的开发历史合并在一起的操作，具体详细命令及介绍可看此<a href="https://www.jianshu.com/p/58a166f24c81" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs cos"><span class="hljs-comment">// 合并某个分支</span>git <span class="hljs-keyword">merge</span> &lt;branch&gt;<span class="hljs-comment">// 退出 merge 过程，常在处理冲突出错时使用</span>git <span class="hljs-keyword">merge</span> --abort<span class="hljs-comment">// 继续 merge 过程，常在处理完冲突时使用</span>git <span class="hljs-keyword">merge</span> --<span class="hljs-keyword">continue</span></code></pre><p>此外，关于在可以 fast-forward 合并时是否再提交一个 commit 有一些争议，具体可看此<a href="https://www.jianshu.com/p/b357df6794e3" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs livecodeserver">git <span class="hljs-built_in">merge</span> <span class="hljs-comment">--no-ff &lt;branch&gt;</span></code></pre></p><p>此外可以在 merge 时也可以使用 —squash 参数，该命令能够将合并分支上的多个 commit 合并成一个 commit 放在当前分支上。相比 rebase 的 squash，其主要区别是该 commit 的 author 不一样。具体可查看此<a href="https://www.jianshu.com/p/684a8ae9dcf1" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs livecodeserver">git <span class="hljs-built_in">merge</span> <span class="hljs-comment">--squash &lt;branch&gt;</span></code></pre></p><h3 id="rebase"><a href="#rebase" class="headerlink" title="rebase"></a>rebase</h3><p>相比 merge，合并分支历史的另一种管理方式，区别可查看此<a href="https://www.jianshu.com/p/6960811ac89c" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs cpp"><span class="hljs-comment">// 变基某个分支</span>git rebase &lt;branch&gt;<span class="hljs-comment">// 退出 rebase 过程，常在处理冲突出错时使用</span>git rebase --<span class="hljs-built_in">abort</span><span class="hljs-comment">// 继续 rebase 过程，常在处理完冲突时使用</span>git rebase --<span class="hljs-keyword">continue</span></code></pre></p><p>此外也可以使用<code>git rebase -i HEAD~n</code> 来合并 n 个 commit，具体过程可参考此<a href="https://www.jianshu.com/p/6960811ac89c" target="_blank" rel="noopener">博客</a>。切记不要在共用的分支上进行 rebase，具体原因可查看此<a href="https://segmentfault.com/a/1190000005937408" target="_blank" rel="noopener">博客</a>。</p><p>此外也可以使用<code>git rebase --onto &lt;branch&gt; &lt;fromCommitId&gt; &lt;toCommitId&gt;</code>来将 (fromCommitId,toCommitId] 上的所有 commit 合并到指定分支上，具体可参考此<a href="https://www.cnblogs.com/rickyk/p/3848768.html" target="_blank" rel="noopener">博客</a>。其实类似于 cherry-pick 多个提交。</p><h3 id="cherry-pick"><a href="#cherry-pick" class="headerlink" title="cherry-pick"></a>cherry-pick</h3><p>用于将指定的 commit 应用于其他分支，具体可参考此<a href="http://www.ruanyifeng.com/blog/2020/04/git-cherry-pick.html" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs dts"><span class="hljs-comment">// 将指定的提交应用于当前分支</span>git cherry-pick <span class="hljs-params">&lt;commitId&gt;</span><span class="hljs-comment">// 将指定分支的最近一次提交应用于当前分支</span>git cherry-pick <span class="hljs-params">&lt;branch&gt;</span><span class="hljs-comment">// 将指定的两个提交先后应用于当前分支</span>git cherry-pick <span class="hljs-params">&lt;commitId1&gt;</span> <span class="hljs-params">&lt;commitId2&gt;</span><span class="hljs-comment">// 将指定范围内的多个提交(不包含 commitId1，包含 commitId2)先后应用于当前分支</span>git cherry-pick <span class="hljs-params">&lt;commitId1&gt;</span>..<span class="hljs-params">&lt;commitId2&gt;</span><span class="hljs-comment">// 将指定范围内的多个提交(包含 commitId1，包含 commitId2)先后应用于当前分支</span>git cherry-pick <span class="hljs-params">&lt;commitId1&gt;</span>^..<span class="hljs-params">&lt;commitId2&gt;</span><span class="hljs-comment">// 退出 cherry-pick 过程，常在处理冲突出错时使用</span>git cherry-pick --abort<span class="hljs-comment">// 继续 cherry-pick 过程，常在处理完冲突时使用</span>git cherry-pick --continue</code></pre></p><h3 id="tag"><a href="#tag" class="headerlink" title="tag"></a>tag</h3><p>常用于发布版本的管理，是指向某个 commit 的指针，具体可查看此<a href="https://blog.csdn.net/jdsjlzx/article/details/98654951" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs crmsh">// 查看所有 <span class="hljs-keyword">tag</span><span class="hljs-title">git</span> <span class="hljs-keyword">tag</span><span class="hljs-title">// 基于本地当前分支最后一个 commit</span> 创建 <span class="hljs-keyword">tag</span><span class="hljs-title">git</span> <span class="hljs-keyword">tag</span> <span class="hljs-title">&lt;tagName</span>&gt; // 基于指定 commit 创建 <span class="hljs-keyword">tag</span><span class="hljs-title">git</span> <span class="hljs-keyword">tag</span> <span class="hljs-title">&lt;tagName</span>&gt; <span class="hljs-tag">&lt;commitId&gt;</span>// 基于指定 commit 创建 <span class="hljs-keyword">tag</span> <span class="hljs-title">并指定 message</span>git <span class="hljs-keyword">tag</span> <span class="hljs-title">-a</span> <span class="hljs-tag">&lt;tagName&gt;</span> -m <span class="hljs-string">"message"</span>// 删除本地指定 <span class="hljs-keyword">tag</span><span class="hljs-title">git</span> <span class="hljs-keyword">tag</span> <span class="hljs-title">-d</span> <span class="hljs-tag">&lt;tagName&gt;</span></code></pre><h3 id="push"><a href="#push" class="headerlink" title="push"></a>push</h3><p>用于将本地版本库的分支推送到远程服务器上对应的分支，具体用法可查看此<a href="https://www.cnblogs.com/qianqiannian/p/6008140.html" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs dts"><span class="hljs-comment">// 向远程推送指定分支</span>git push <span class="hljs-params">&lt;remote&gt;</span> <span class="hljs-params">&lt;branch&gt;</span><span class="hljs-comment">// 向远程推送指定分支并绑定</span>git push -u <span class="hljs-params">&lt;remote&gt;</span> <span class="hljs-params">&lt;branch&gt;</span><span class="hljs-comment">// 删除远程分支</span>git push <span class="hljs-params">&lt;remote&gt;</span> :<span class="hljs-params">&lt;branch&gt;</span><span class="hljs-comment">// 向远程推送 tag</span>git push <span class="hljs-params">&lt;remote&gt;</span> <span class="hljs-params">&lt;tagName&gt;</span><span class="hljs-comment">// 删除远程 tag</span>git push <span class="hljs-params">&lt;remote&gt;</span> :<span class="hljs-params">&lt;tagName&gt;</span><span class="hljs-comment">// 向远程推送本地所有 tag</span>git push --tags <span class="hljs-params">&lt;remote&gt;</span></code></pre></p><p>此外，当本地版本库的分支和远程版本库的对应分支有冲突时，如果想要强制覆盖，千万不要用<code>git push -f</code>，而是应该用<code>git push --force-with-lease</code>，具体原因可查看此<a href="https://www.cnblogs.com/walterlv/p/10236461.html" target="_blank" rel="noopener">博客</a>。</p><h3 id="fetch"><a href="#fetch" class="headerlink" title="fetch"></a>fetch</h3><p>用于从远程获取对象和引用到本地，具体可参考此<a href="https://www.yiibai.com/git/git_fetch.html" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs dts"><span class="hljs-comment">// 取回指定远程仓库的所有分支</span>git fetch <span class="hljs-params">&lt;remote&gt;</span> <span class="hljs-comment">// 取回指定远程仓库的指定分支</span>git fetch <span class="hljs-params">&lt;remote&gt;</span> <span class="hljs-params">&lt;branch&gt;</span><span class="hljs-comment">// 取回所有远程仓库的所有分支</span>git fetch --all</code></pre><h3 id="pull"><a href="#pull" class="headerlink" title="pull"></a>pull</h3><p>用于从远程获取代码并合并本地的版本，其实就是<code>git fetch</code>和<code>git merge FETCH_HEAD</code>的简写。有关其与 fetch 的区别，可以参考此<a href="https://www.cnblogs.com/ruiyang-/p/10764711.html" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs dts"><span class="hljs-comment">// 相当于 fetch + merge</span>git pull <span class="hljs-params">&lt;remote&gt;</span> <span class="hljs-params">&lt;remote_branch&gt;</span>:<span class="hljs-params">&lt;local_branch&gt;</span><span class="hljs-comment">// 相当于 fetch + rebase</span>git pull --rebase <span class="hljs-params">&lt;remote&gt;</span> <span class="hljs-params">&lt;remote_branch&gt;</span>:<span class="hljs-params">&lt;local_branch&gt;</span></code></pre><h3 id="bisect"><a href="#bisect" class="headerlink" title="bisect"></a>bisect</h3><p>用于在 git 历史里面二分查找有 bug 的 commit。这是找 bug 时一个非常有用的命令，可以快速定位出引入 bug 的 commit。具体使用方式可查看此<a href="http://www.ruanyifeng.com/blog/2018/12/git-bisect.html" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs less"><span class="hljs-comment">// 开始查找</span><span class="hljs-selector-tag">git</span> <span class="hljs-selector-tag">bisect</span> <span class="hljs-selector-tag">start</span> <span class="hljs-selector-attr">[endCommitId]</span> <span class="hljs-selector-attr">[startCommitId]</span><span class="hljs-comment">// 版本正确</span><span class="hljs-selector-tag">git</span> <span class="hljs-selector-tag">bisect</span> <span class="hljs-selector-tag">good</span><span class="hljs-comment">// 版本错误</span><span class="hljs-selector-tag">git</span> <span class="hljs-selector-tag">bisect</span> <span class="hljs-selector-tag">bad</span><span class="hljs-comment">// 退出查找</span><span class="hljs-selector-tag">git</span> <span class="hljs-selector-tag">bisect</span> <span class="hljs-selector-tag">reset</span></code></pre></p><h3 id="reflog"><a href="#reflog" class="headerlink" title="reflog"></a>reflog</h3><p>常用来恢复本地错误操作。用户每次使用更新 HEAD 的 git 命令比如 commit、amend、cherry-pick、reset、revert 等都会被记录下来（不限分支），就像 shell 的 history 一样。 这样用户如果发生误操作删除了某个 commit 找不到时，调用此命令就可以查找到对应的 commit，从而回到误删前的状态了。具体使用场景可查看此<a href="https://cloud.tencent.com/developer/article/1413097" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs angelscript">git <span class="hljs-built_in">ref</span>log</code></pre><h3 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h3><p>用于检索文件中的文本内容，更多用法可参考此<a href="https://www.softwhy.com/article-8652-1.html" target="_blank" rel="noopener">博客</a>。<br><pre><code class="hljs arduino"><span class="hljs-comment">// 检索所有包含指定关键字的文件</span>git grep <span class="hljs-built_in">text</span><span class="hljs-comment">// 检索关键字出现在哪一行</span>git grep -n <span class="hljs-built_in">text</span><span class="hljs-comment">// 统计每一个文件中检索到指定关键字的行数</span>git grep -c <span class="hljs-built_in">text</span></code></pre></p><h3 id="blame"><a href="#blame" class="headerlink" title="blame"></a>blame</h3><p>用于查看某个文件的每一行内容由谁所写，类似于 IDEA 的 annotate 功能，具体可查看此<a href="http://www.zhai14.com/blog/git-blame-command-help-u-find-out-who-made-the-serious-mistake.html" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs applescript">git blame &lt;<span class="hljs-built_in">file</span>&gt;</code></pre><h3 id="gc"><a href="#gc" class="headerlink" title="gc"></a>gc</h3><p>用来清理 git 目录，删除掉无效的应用和文件，节约存储空间和传输大小。具体功能可参考此<a href="https://blog.csdn.net/lihuanshuai/article/details/37345565" target="_blank" rel="noopener">博客</a>。</p><pre><code class="hljs ebnf"><span class="hljs-attribute">git gc</span></code></pre><h3 id="submodule"><a href="#submodule" class="headerlink" title="submodule"></a>submodule</h3><p>用于管理多个子项目。该命令允许一个 git 仓库作为另一个 git 仓库的子目录，并且保持父项目和子项目相互独立。具体用法可参考此<a href="http://www.ayqy.net/blog/%E7%90%86%E8%A7%A3git-submodules/" target="_blank" rel="noopener">博客</a>。</p><h2 id="我的-alias-设置"><a href="#我的-alias-设置" class="headerlink" title="我的 alias 设置"></a>我的 alias 设置</h2><pre><code class="hljs nix"><span class="hljs-attr">a</span> = add <span class="hljs-attr">aa</span> = add -A   <span class="hljs-attr">au</span> = add -u<span class="hljs-attr">ae</span> = add -e<span class="hljs-attr">ap</span> = add -p<span class="hljs-attr">bm</span> = blame<span class="hljs-attr">br</span> = branch -vv<span class="hljs-attr">bra</span> = branch -a   <span class="hljs-attr">brr</span> = branch -r<span class="hljs-attr">brd</span> = branch -d<span class="hljs-attr">brdd</span> = branch -D   <span class="hljs-attr">brf</span> = branch -f<span class="hljs-attr">brm</span> = branch -m<span class="hljs-attr">bs</span> = bisect   <span class="hljs-attr">bss</span> = bisect start<span class="hljs-attr">bsb</span> = bisect bad<span class="hljs-attr">bsg</span> = bisect good<span class="hljs-attr">bsr</span> = bisect reset<span class="hljs-attr">cf</span> = config   <span class="hljs-attr">cfl</span> = config --list<span class="hljs-attr">cfg</span> = config --global<span class="hljs-attr">cfgl</span> = config --global --list<span class="hljs-attr">cfs</span> = config --system<span class="hljs-attr">cfsl</span> = config --system --list<span class="hljs-attr">cl</span> = clone<span class="hljs-attr">clb</span> = clone --single-branch --branch<span class="hljs-attr">cld</span> = clone --depth<span class="hljs-attr">cm</span> = commit   <span class="hljs-attr">cmm</span> = commit -m   <span class="hljs-attr">cmem</span> = commit --allow-empty -m<span class="hljs-attr">cma</span> = commit --amend<span class="hljs-attr">cman</span> = commit --amend --no-edit<span class="hljs-attr">co</span> = checkout<span class="hljs-attr">cob</span> = checkout -b<span class="hljs-attr">cod</span> = checkout -- <span class="hljs-attr">coda</span> = checkout .<span class="hljs-attr">coo</span> = checkout --orphan<span class="hljs-attr">cp</span> = cherry-pick<span class="hljs-attr">cpa</span> = cherry-pick --<span class="hljs-built_in">abort</span><span class="hljs-attr">cpc</span> = cherry-pick --continue<span class="hljs-attr">df</span> = diff<span class="hljs-attr">dfc</span> = diff --cached<span class="hljs-attr">dfh</span> = diff HEAD<span class="hljs-attr">fh</span> = fetch<span class="hljs-attr">fha</span> = fetch --all<span class="hljs-attr">ge</span> = grep -n <span class="hljs-attr">gec</span> = grep -c <span class="hljs-attr">lg</span> = log<span class="hljs-attr">lgo</span> = log --oneline<span class="hljs-attr">lgga</span> = log --graph <span class="hljs-attr">--pretty=format:'%Cred%h%Creset</span> -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit <span class="hljs-attr">--date=relative</span><span class="hljs-attr">lgge</span> = log --grep<span class="hljs-attr">lgs</span> = log -S<span class="hljs-attr">me</span> = merge<span class="hljs-attr">mea</span> = merge --<span class="hljs-built_in">abort</span><span class="hljs-attr">mec</span> = merge --continue<span class="hljs-attr">men</span> = merge --no-ff<span class="hljs-attr">mvf</span> = mv -f<span class="hljs-attr">pl</span> = pull<span class="hljs-attr">plr</span> = pull --rebase<span class="hljs-attr">ps</span> = push<span class="hljs-attr">psu</span> = push -u<span class="hljs-attr">psf</span> = push --force-<span class="hljs-keyword">with</span>-lease<span class="hljs-attr">pst</span> = push --tags<span class="hljs-attr">rb</span> = rebase<span class="hljs-attr">rba</span> = rebase --<span class="hljs-built_in">abort</span><span class="hljs-attr">rbc</span> = rebase --continue<span class="hljs-attr">rbi</span> = rebase -i <span class="hljs-attr">rbo</span> = rebase --onto<span class="hljs-attr">rmc</span> = rm --cached<span class="hljs-attr">rmca</span> = rm --cached -r .<span class="hljs-attr">rmf</span> = rm -f<span class="hljs-attr">rmfa</span> = rm -f -r .<span class="hljs-attr">rf</span> = reflog<span class="hljs-attr">rs</span> = reset<span class="hljs-attr">rse</span> = reset HEAD <span class="hljs-attr">rsh</span> = reset --hard<span class="hljs-attr">rss</span> = reset --soft<span class="hljs-attr">rv</span> = revert<span class="hljs-attr">rva</span> = revert --<span class="hljs-built_in">abort</span><span class="hljs-attr">rvc</span> = revert --continue<span class="hljs-attr">rvn</span> = revert --no-commit<span class="hljs-attr">rt</span> = remote<span class="hljs-attr">rta</span> = remote add<span class="hljs-attr">rtp</span> = remote prune<span class="hljs-attr">rtpd</span> = remote prune --dry-run<span class="hljs-attr">rtrm</span> = remote remove<span class="hljs-attr">rtrn</span> = remote rename<span class="hljs-attr">rtsu</span> = remote set-url<span class="hljs-attr">rtsua</span> = remote set-url --add<span class="hljs-attr">rtsud</span> = remote set-url --delete<span class="hljs-attr">rts</span> = remote show<span class="hljs-attr">rtu</span> = remote update<span class="hljs-attr">rtv</span> = remote -v<span class="hljs-attr">sh</span> = stash<span class="hljs-attr">shu</span> = stash -u<span class="hljs-attr">shk</span> = stash --keep-index<span class="hljs-attr">sha</span> = stash apply<span class="hljs-attr">shc</span> = stash clear<span class="hljs-attr">shd</span> = stash drop<span class="hljs-attr">shl</span> = stash list<span class="hljs-attr">shp</span> = stash pop<span class="hljs-attr">shsh</span> = stash show<span class="hljs-attr">shshp</span> = stash show -p<span class="hljs-attr">shsa</span> = stash save<span class="hljs-attr">shsak</span> = stash save --keep-index<span class="hljs-attr">sl</span> = shortlog<span class="hljs-attr">slsn</span> = shortlog -s -n<span class="hljs-attr">st</span> = status--show-stash<span class="hljs-attr">sti</span> = stash ---ignored --show-stash<span class="hljs-attr">tg</span> = tag<span class="hljs-attr">tga</span> = tag -a<span class="hljs-attr">tgd</span> = tag -d</code></pre><h2 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h2><p>以下两个仓库是我刷过的比较有趣的两个教程，十分推荐。</p><ul><li><a href="https://github.com/Gazler/githug" target="_blank" rel="noopener">githug</a></li><li><a href="https://github.com/pcottle/learnGitBranching" target="_blank" rel="noopener">learnGitBranching</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce 论文阅读</title>
    <link href="/mapreduce-thesis/"/>
    <url>/mapreduce-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>在 20 世纪初，包括本文作者在内的 Google 的很多程序员，为了处理海量的原始数据，已经实现了数以百计的、专用的计算方法。这些计算方法用来处理大量的原始数据，比如，文档抓取（类似网络爬虫的程序）、Web 请求日志等等；也为了计算处理各种类型的衍生数据，比如倒排索引、Web 文档的图结构的各种表示形势、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合等等。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>大多数以上提到的数据处理运算在概念上很容易理解。然而由于输入的数据量巨大，因此要想在可接受的时间内完成运算，只有将这些计算分布在成百上千的主机上。如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为了解决上述复杂的问题，本文设计一个新的抽象模型，使用这个抽象模型，用户只要表述想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面：利用一个输入 key/value pair 集合来产生一个输出的 key/value pair 集合。</p><p>MapReduce 库的用户可以用两个函数表达这个计算：Map 和 Reduce。</p><ul><li>用户自定义的 Map 函数接受一个输入的 key/value pair 值，然后产生一个中间 key/value pair 值的集合。MapReduce 库把所有具有相同中间 key 值 I 的中间 value 值集合在一起后传递给 reduce 函数。</li><li>用户自定义的 Reduce 函数接受一个中间 key 的值 I 和相关的一个 value 值的集合。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般的，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通常 Map 通过一个迭代器把中间 value 值提供给 Reduce 函数，这样 Reduce Worker 就可以处理无法全部放入内存中的大量的 value 值的集合。</li></ul><p>在概念上，用户定义的 Map 和 Reduce 函数都有相关联的类型：<br><pre><code class="hljs livescript"><span class="hljs-keyword">map</span><span class="hljs-function"><span class="hljs-params">(k1,v1)</span> -&gt;</span><span class="hljs-keyword">list</span>(k2,v2)reduce<span class="hljs-function"><span class="hljs-params">(k2,<span class="hljs-keyword">list</span>(v2))</span> -&gt;</span><span class="hljs-keyword">list</span>(v2)</code></pre><br>比如，输入的 key 和 value 值与输出的 key 和 value 值在类型上推导的域不同。此外，中间 key 和 value 值与输出 key 和 value 值在类型上推导的域相同。</p><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><p>通过将 Map 调用的输入数据自动分割为 M 个数据片段的集合，Map 调用被分布到多台机器上执行。输入的数据片段能够在不同的机器上并行处理。使用分区函数将 Map 调用产生的中间 key 值分成 R 个不同分区（例如，hash(key) mod R），Reduce 调用也被分布到多台机器上执行。分区数量（R）和分区函数由用户来指定。</p><p><img src="/mapreduce-thesis/mapreduce.png" srcset="/img/loading.gif" alt></p><p>上图展示了 MapReduce 实现中操作的全部流程。当用户调用 MapReduce 函数时，将发生下面的一系列动作：</p><ol><li>用户程序首先调用的 MapReduce 库将输入文件分成 M 个数据片度，每个数据片段的大小一般从 16MB 到 64MB(可以通过可选的参数来控制每个数据片段的大小)。然后用户程序在机群中创建大量的程序副本。</li><li>这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 任务或 Reduce 任务分配给一个空闲的 worker。</li><li>被分配了 map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。</li><li>缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的 key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker</li><li>当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。</li><li>Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。</li><li>当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对 MapReduce 调用才返回。</li></ol><h3 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h3><h4 id="worker-故障"><a href="#worker-故障" class="headerlink" title="worker 故障"></a>worker 故障</h4><p>master 与 worker 之间同步心跳，对于失效的 worker，根据其类型来做进一步处理：</p><ul><li>Map worker 故障：由于 Map 任务将数据临时存储在本地，所以需要重新执行。</li><li>Reduce worker 故障：由于 Reduce 任务将数据存储在全局文件系统中 ，所以不需要重新执行。</li></ul><h4 id="master-故障"><a href="#master-故障" class="headerlink" title="master 故障"></a>master 故障</h4><p>MapReduce 任务重新执行</p><h4 id="故障语义保证"><a href="#故障语义保证" class="headerlink" title="故障语义保证"></a>故障语义保证</h4><p>当用户提供的 Map 和 Reduce 操作是输入确定性函数（即相同的输入产生相同的输出）时，MapReduce 的分布式实现在任何情况下的输出都和所有程序没有出现任何错误、顺序的执行产生的输出是一样的。</p><ul><li>Map worker 任务的原子提交：每个 Map 任务生成 R 个本地临时文件，当一个 Map 任务完成的时，worker 发送一个包含 R 个临时文件名的完成消息给 master。如果 master 从一个已经完成的 Map 任务再次接收到一个完成消息，master 将忽略这个消息；</li><li>Reduce worker 任务的原子提交：当 Reduce 任务完成时，Reduce worker 进程以原子的方式把临时文件重命名为最终的输出文件。如果同一个 Reduce 任务在多台机器上执行，针对同一个最终的输出文件将有多个重命名操作执行。MapReduce 依赖底层文件系统提供的重命名操作的原子性来保证最终的文件系统状态仅仅包含一个 Reduce 任务产生的数据。</li></ul><h3 id="存储位置优化"><a href="#存储位置优化" class="headerlink" title="存储位置优化"></a>存储位置优化</h3><p>核心思想：本地读文件以减少流量消耗</p><p>MapReduce 的 master 在调度 Map 任务时会考虑输入文件的位置信息，尽量将一个 Map 任务调度在包含相关输入数据拷贝的机器上执行；如果上述努力失败了，master 将尝试在保存有输入数据拷贝的机器附近的机器上执行 Map 任务(例如，分配到一个和包含输入数据的机器在一个交换机里的 worker 机器上执行)。</p><h3 id="任务粒度"><a href="#任务粒度" class="headerlink" title="任务粒度"></a>任务粒度</h3><p>理想情况下，M 和 R 应当比集群中 worker 的机器数量要多得多。在每台 worker 机器都执行大量的不同任务能够提高集群的动态的负载均衡能力，并且能够加快故障恢复的速度：失效机器上执行的大量 Map 任务都可以分布到所有其他的 worker 机器上去执行。</p><p>实际使用时建议用户选择合适的 M 值，以使得每一个独立任务都是处理大约 16M 到 64M 的输入数据（这样，上面描写的输入数据本地存储优化策略才最有效），另外，也建议把 R 值设置使用的 worker 机器数量的小倍数。比如：M=200000，R=5000，使用 2000 台 worker 机器。</p><h3 id="备用任务"><a href="#备用任务" class="headerlink" title="备用任务"></a>备用任务</h3><p>影响一个 MapReduce 的总执行时间最通常的因素是“落伍者”：在运算过程中，如果有一台机器花了很长的时间才完成最后几个 Map 或 Reduce 任务，导致 MapReduce 操作总的执行时间超过预期。</p><p>为了解决落伍者的问题，当一个 MapReduce 操作接近完成的时候，master 调度备用（backup）任务进程来执行剩下的、处于处理中状态（in-progress）的任务。无论是最初的执行进程、还是备用（backup）任务进程完成了任务，MapReduce 都把这个任务标记成为已经完成。此个机制通常只会占用比正常操作多几个百分点的计算资源。但能减少近 50% 的任务完成总时间。</p><h3 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h3><h4 id="分区函数"><a href="#分区函数" class="headerlink" title="分区函数"></a>分区函数</h4><p>MapReduce 缺省的分区函数是使用 hash 方法(比如，hash(key) mod R)进行分区。hash 方法能产生非常平衡的分区。然而，有的时候，其它的一些分区函数对 key 值进行的分区将非常有用。比如，输出的 key 值是 URLs，有的用户希望每个主机的所有条目保持在同一个输出文件中。为了支持类似的情况，MapReduce 库的用户需要提供专门的分区函数。例如，使用“hash(Hostname(urlkey))mod R”作为分区函数就可以把所有来自同一个主机的 URLs 保存在同一个输出文件中。</p><h4 id="顺序保证"><a href="#顺序保证" class="headerlink" title="顺序保证"></a>顺序保证</h4><p>MapReduce 确保在给定的分区中，中间 key/value pair 数据的处理顺序是按照 key 值增量顺序处理的。这样的顺序保证对每个分成生成一个有序的输出文件，这对于需要对输出文件按 key 值随机存取的应用非常有意义，对在排序输出的数据集也很有帮助。</p><h4 id="Combiner-函数"><a href="#Combiner-函数" class="headerlink" title="Combiner 函数"></a>Combiner 函数</h4><p>在某些情况下，Map 函数产生的中间 key 值的重复数据会占很大的比重，并且，用户自定义的 Reduce 函数满足结合律和交换律。在 2.1 节的词数统计程序是个很好的例子。由于词频率倾向于一个 zipf 分布(齐夫分布)，每个 Map 任务将产生成千上万个这样的记录<the,1>。所有的这些记录将通过网络被发送到一个单独的 Reduce 任务，然后由这个 Reduce 任务把所有这些记录累加起来产生一个数字。MapReduce 允许用户指定一个可选的 combiner 函数，combiner 函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。</the,1></p><p>Combiner 函数在每台执行 Map 任务的机器上都会被执行一次。一般情况下，Combiner 和 Reduce 函数是一样的。Combiner 函数和 Reduce 函数之间唯一的区别是 MapReduce 库怎样控制函数的输出。Reduce 函数的输出被保存在最终的输出文件里，而 Combiner 函数的输出被写到中间文件里，然后被发送给 Reduce 任务。</p><p>部分的合并中间结果可以显著的提高一些 MapReduce 操作的速度。</p><h4 id="输入和输出的类型"><a href="#输入和输出的类型" class="headerlink" title="输入和输出的类型"></a>输入和输出的类型</h4><p>支持常用的类型，可以通过提供一个简单的 Reader 接口实现来支持一个新的输入类型。Reader 并非一定要从文件中读取数据，比如可以很容易的实现一个从数据库里读记录的 Reader，或者从内存中的数据结构读取数据的 Reader。</p><h4 id="副作用"><a href="#副作用" class="headerlink" title="副作用"></a>副作用</h4><p>在某些情况下，MapReduce 的使用者发现，如果在 Map 或 Reduce 操作过程中增加辅助的输出文件会比较省事。MapReduce 依靠程序 writer 把这种“副作用”变成原子的和幂等的。通常应用程序首先把输出结果写到一个临时文件中，在输出全部数据之后，在使用系统级的原子操作 rename 重新命名这个临时文件。</p><h4 id="跳过损坏的记录"><a href="#跳过损坏的记录" class="headerlink" title="跳过损坏的记录"></a>跳过损坏的记录</h4><p>每个 worker 进程都设置了信号处理函数捕获内存段异常（segmentation violation）和总线错误（bus error）。 在执行 Map 或者 Reduce 操作之前，MapReduce 库通过全局变量保存记录序号。如果用户程序触发了一个系统信号，消息处理函数将用“最后一口气”通过 UDP 包向 master 发送处理的最后一条记录的序号。当 master 看到在处理某条特定记录不止失败一次时，master 就标志着条记录需要被跳过，并且在下次重新执行相关的 Map 或者 Reduce 任务的时候跳过这条记录。</p><h4 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h4><p>支持本地串行执行以方便调试</p><h4 id="状态信息"><a href="#状态信息" class="headerlink" title="状态信息"></a>状态信息</h4><p>master 支持嵌入 HTTP 服务器以显示一组状态信息页面，用户可以监控各种执行状态。状态信息页面显示了包括计算执行的进度，比如已经完成了多少任务、有多少任务正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理百分比等等</p><h4 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h4><p>MapReduce 库使用计数器统计不同事件发生次数。比如，用户可能想统计已经处理了多少个单词、已经索引的多少篇 German 文档等等。</p><p>这些计数器的值周期性的从各个单独的 worker 机器上传递给 master（附加在 ping 的应答包中传递）。master 把执行成功的 Map 和 Reduce 任务的计数器值进行累计，当 MapReduce 操作完成之后，返回给用户代码。 </p><p>计数器当前的值也会显示在 master 的状态页面上，这样用户就可以看到当前计算的进度。当累加计数器的值的时候，master 要检查重复运行的 Map 或者 Reduce 任务，避免重复累加（之前提到的备用任务和失效后重新执行任务这两种情况会导致相同的任务被多次执行）。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li>分布式的 Grep：Map 函数输出匹配某个模式的一行，Reduce 函数是一个恒等函数，即把中间数据复制到输出。</li><li>计算 URL 访问频率：Map 函数处理日志中 web 页面请求的记录，然后输出(URL,1)。Reduce 函数把相同 URL 的 value 值都累加起来，产生(URL,记录总数)结果。</li><li>网络链接倒排：Map 函数在源页面（source）中搜索所有的链接目标（target）并输出为(target,source)。Reduce 函数把给定链接目标（target）的链接组合成一个列表，输出(target,list(source))。</li><li>每个主机的检索词向量：检索词向量用一个(词,频率)列表来概述出现在文档或文档集中的最重要的一些词。Map 函数为每一个输入文档输出(主机名,检索词向量)，其中主机名来自文档的 URL。Reduce 函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，输出一个最终的(主机名,检索词向量)。</li><li>倒排索引：Map 函数分析每个文档输出一个(词,文档号)的列表，Reduce 函数的输入是一个给定词的所有（词，文档号），排序所有的文档号，输出(词,list（文档号）)。所有的输出集合形成一个简单的倒排索引，它以一种简单的算法跟踪词在文档中的位置。</li><li>分布式排序：Map 函数从每个记录提取 key，输出(key,record)。Reduce 函数不改变任何的值。这个运算依赖分区机制和排序属性。</li></ul><h2 id="经验分享"><a href="#经验分享" class="headerlink" title="经验分享"></a>经验分享</h2><ul><li>约束编程模式使得并行和分布式计算非常容易，也易于构造容错的计算环境；</li><li>网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽。</li><li>多次执行相同的任务可以减少硬件配置不平衡带来的负面影响，同时解决了由于机器失效导致的数据丢失问题。</li></ul><h2 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h2><ul><li>通过简单的接口实现了自动的并行化和大规模的分布式计算，通过使用 MapReduce 模型接口实现了在大量普通 PC 机上的高性能计算。</li><li>向工业界证明了 MapReduce 模型在分布式计算上的可行性，拉开了分布式计算的序幕并影响了其后所有的计算框架，包括现在流行的批处理框架 Spark 和流处理框架 Flink 都很受其影响。</li></ul><h2 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h2><ul><li>基于历史局限性和当时的成本考虑，没有利用内存去更高效的处理数据，不过也为 Spark 提供了思路。</li><li>没有将资料调度和计算调度分离，使得 MapReduce 系统看起来较为冗杂。在开源的 Hadoop 生态中，MapReduce 现只关注于计算，具体的资源调度由 Yarn 管理。 </li></ul><h2 id="相关系统"><a href="#相关系统" class="headerlink" title="相关系统"></a>相关系统</h2><ul><li>分布式存储系统：GFS/Colossus/HDFS</li><li>批处理框架：Spark</li><li>流处理框架：Flink</li><li>高可用机制：Chubby/ZooKeeper</li></ul><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://pdos.csail.mit.edu/6.824/notes/l01.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="https://www.bilibili.com/video/BV1R7411t71W?p=1" target="_blank" rel="noopener">6.824 双语视频</a><br><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf" target="_blank" rel="noopener">论文</a><br><a href="https://github.com/Cxka/paper/blob/0a72fe0b354b65bac25e45163163eb2573f1faf2/map-reduce/map-reduce-cn.pdf" target="_blank" rel="noopener">中文翻译</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Google</tag>
      
      <tag>分布式计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MaxCompute 跨域流量优化论文阅读</title>
    <link href="/maxcompute-yugong-thesis/"/>
    <url>/maxcompute-yugong-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>随着大数据技术的长足发展，大公司和云供应商在全球创建了数十个地理上分散的数据中心（分布式数据中心）。一个典型的数据中心可包含数万台计算机，这些数据中心为许多大规模的 IT 企业提供了计算和存储能力。在管理这种大规模、分布式数据中心的过程中，减少跨数据中心流量是提高整体性能的核心瓶颈之一。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>MaxCompute 是阿里巴巴的大型数据管理和分析平台，管理着数十个分布式数据中心。每个数据中心都包含成千上万台服务器，并且通过广域网（WAN）相互连接。这些数据中心每天新增 500 万张数据表，并为阿里巴巴的各种业务应用程序（例如淘宝，天猫等）执行多达 700 万次的分析工作。 MaxCompute 中的作业每天生产和使用大量数据，形成了复杂的数据依赖关系。尽管这些依赖关系大多都在本地数据中心内部，随着业务的增长，来自非本地数据中心的依赖关系也在迅速增加。由于跨数据中心的依赖关系，使得 MaxCompute 中大约产生了数百 PB 通过广域网传输的数据。</p><p>日益增长的跨数据中心传输需求带来了多个方面的问题。WAN 的带宽约为 Tbps，而数据中心内网络的聚合带宽则大得多；另外，WAN 延迟是数据中心内部网络延迟的 10-100 倍。除网速之外， WAN 的成本也十分昂贵。如今，跨 DC 的带宽已成为一种非常宝贵的资源，同时也是 MaxCompute 运营的性能瓶颈。在优化之前，WAN 的成本占 MaxCompute 总体运营成本的很大一部分——考虑到 MaxCompute 的日常运营规模庞大，这是巨大的财务负担。正因如此，减少跨地区带宽的使用已经逐渐成为阿里巴巴数据中心业务的一大挑战。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>MaxCompute 研发了一个跨域流量优化系统 Yugong（意译：愚公），其通过与 MaxCompute 在大范围生产环境下进行协同运算，极大地降低了由项目迁移（project migration）、副本拷贝（table replication）以及计算调度（job outsourcing）所产生的大量跨数据中心带宽流量，由此有效地降低了运营成本。</p><h3 id="业务数据分析"><a href="#业务数据分析" class="headerlink" title="业务数据分析"></a>业务数据分析</h3><p>MaxCompute 业务场景中的数据有 project，table 和 partition 三个级别。project 可以类比于关系数据库中 database 的概念，通常是业务功能上相近的表的集合。table 是用户数据的某一张关系型数据表，partition 是 table 根据时间进行分区的子表，通常每张表每天会产生一个分区。 </p><p>其特性如下：</p><ul><li>project，job 和 table 遵循幂律分布。作业和表之间的跨 DC 依赖关系呈现出长尾现象。相对较少的热表和大型作业在跨 DC 依赖关系中占很大一部分。</li><li>不论是数据大小还是分区数量，大部分作业的输入都要比输出大得多。</li><li>连续几天创建的表分区具有相似的大小。一个分区的大小比它的表的大小小数百倍。大多数作业是周期性的，连续几天的表访问模式是稳定的。</li><li>最近的表分区被更频繁地访问，其他作业对其依赖的数量随着访问偏移量的增加呈指数级下降。</li><li>一些表经常被远程 DC 中的作业读取，因此复制这些表可以节省跨 DC 的带宽。另外，调度一些作业在它们的非默认 DC 中运行可以减少跨 DC 带宽的使用。</li><li>DC 具有动态且不可预测的资源利用模式，在同一时间段内可能存在不同的资源瓶颈。</li></ul><h3 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h3><p>下图为下文常用到的一些符号及其解释。<br><img src="/maxcompute-yugong-thesis/notation.png" srcset="/img/loading.gif" alt></p><p>在下文中，我们将尝试最小化一天中跨集群的总带宽使用。我们假设连接每对 DC 的广域网具有相同的单位成本。我们假设来自同一个分时表的所有分区的大小是相同的，作业每天重复出现，并且它们的表访问模式连续几天都是相同的。因此，我们只需要考虑一天内运行的作业。我们使用“所有作业”来表示“在当前日期 t<sub>cur</sub> 上运行的所有作业”。表访问模式仅由在 t<sub>cur</sub> 上运行的作业生成。作业和访问模式实际上在几天内缓慢变化，我们将在 table replication 节再介绍如何应用于生产环境。</p><p><img src="/maxcompute-yugong-thesis/model_1.png" srcset="/img/loading.gif" alt><br><img src="/maxcompute-yugong-thesis/model_2.png" srcset="/img/loading.gif" alt></p><p>该分析模型在计算上是棘手的，因为它是（| T | + | P |）×| DC | 的整数规划问题，其中 | T | 是表的总数，| P | 是项目总数，并且 | DC | 是数据中心的数量。</p><p>根据前文讲述的业务数据分析，我们可以利用我们的发现来简化问题。由于作业倾向于读取最近的分区，并且它们对分区的依赖关系的大小随访问偏移的增加而呈指数下降，因此有理由首先假设我们有足够/无限的项目复制存储大小迁移，然后使用启发式方法确定固定存储预算下的寿命。这种简化极大地降低了模型的复杂性，因为我们实际上删除了存储空间的限制。</p><p>通过这种简化，我们可以将问题分解为两个问题：</p><ol><li>一个项目迁移问题，该问题首先在假定复制存储预算不受限制的情况下找到项目放置计划 P，</li><li>一个表复制问题，该问题在给定项目放置计划 P 后生成表复制计划 R，同时满足存储空间的约束。这种分离对于我们的生产环境也是很自然的，因为由于较高的迁移成本，项目放置/迁移不能频繁执行，而表复制计划可以更频繁地更新。</li></ol><h3 id="项目迁移"><a href="#项目迁移" class="headerlink" title="项目迁移"></a>项目迁移</h3><p>在简化的项目迁移模型中，我们删除了存储空间约束。根据给定的项目布置计划，可以通过以下方法获得 DC d 的最小跨 DC 带宽成本 BW<sup>opt（d）</sup>：</p><ol><li>从表 i 的 DC 中远程读取每个表 i 的所有必需数据。 如果从表 i 读取的数据量小于其分区的总大小； </li><li>否则将表 i 的所有分区存储在 DC d 中，并每天复制其最新分区。 </li></ol><p>因此，BW<sup>opt（d）</sup> 由以下公式 12 给出：</p><p><img src="/maxcompute-yugong-thesis/project_migration_1.png" srcset="/img/loading.gif" alt></p><p>上面关于 R<sub>i</sub><sup>t</sup>（d）沿时间维度 t 的总和有助于消除为每个时间分区共同考虑复制策略的复杂性。我们的目标是找到一个项目放置计划 P，以使总 BW<sup>opt（d）</sup> 最小化：</p><p><img src="/maxcompute-yugong-thesis/project_migration_2.png" srcset="/img/loading.gif" alt></p><p>即使进行了这种简化，项目和表的数量仍然很大，因为我们有成千上万个项目和数百万个表。我们利用跨 DC 依赖关系的幂律分布来进一步减小问题的大小，即少量表构成了大部分跨 DC 的依赖。因此，我们仅考虑项目迁移模型中具有最大依赖项大小的少数表。此外，在解决问题时，我们还发现，对少量项目进行迁移可以显着改善我们当前的项目布局，而随着影响力较大的项目已经放置在合适的 DC 中，随着进一步的迁移，这种改进会迅速降低。</p><p>我们的项目迁移策略还可用于解决新的项目放置问题。我们首先根据 DC的负载将新项目放置在 DC 中，然后通过将 MigCount 设置为新项目的数量来解决项目迁移问题。</p><h3 id="数据拷贝"><a href="#数据拷贝" class="headerlink" title="数据拷贝"></a>数据拷贝</h3><p>给定上一节计算出的项目放置计划 P，我们然后找到一个表复制计划（即，找到每个 DC d 中每个表 i 的寿命 L<sub>i</sub>（d））以最小化跨DC带宽的总成本， 同时满足存储空间约束。 我们首先假设连续两天的表访问模式相同，针对不同 DC 中所有表的寿命设计一种启发式方法。然后我们删除该假设，并考虑动态维护表副本的生命周期，因为表访问模式实际上随着时间逐渐变化。 请注意，我们的解决方案中的表访问矩阵 R<sub>i</sub><sup>t</sup>（d）仅包含不属于 DC d 中项目的表，即 X<sub>p（i），d</sub> = 0，因为我们仅关心远程读取。为简单起见，我们在随后的讨论中省略了 DC d。</p><h4 id="DP-解法"><a href="#DP-解法" class="headerlink" title="DP 解法"></a>DP 解法</h4><p>我们可以通过 DP 算法来获得给定复制存储大小和给定项目放置计划下的最佳表复制计划。 我们将 dp（i，s）表示为可以通过考虑复制存储大小约束 s 下的前 i 个表获得的最小跨 DC 带宽成本, 定义表 i 的寿命是 L<sub>i</sub>。 用于存储表 i 的副本的存储大小为 L<sub>i</sub>×S<sub>i</sub>。 则读取表 i 的分区所产生的跨 DC 带宽成本为：</p><ol><li>寿命未涵盖的部分的所有远程读取的总成本和。</li><li>寿命涵盖部分的复制成本。</li></ol><p><img src="/maxcompute-yugong-thesis/dp_1.png" srcset="/img/loading.gif" alt></p><p>因此 dp 算法的转移函数可以定义如下：</p><p><img src="/maxcompute-yugong-thesis/dp_2.png" srcset="/img/loading.gif" alt></p><p>存储预算为 s 的前 i 个表的最小跨 DC 带宽成本是枚举表 i 的所有可能寿命 L<sub>i</sub> 并取其中的最小值。</p><p>DP 算法的时间复杂度为 O（| T | | L | | storage |），其中 | T | 是表的数量（从几万到几百万），| L | 是每个表的可能寿命（通常为几百个），并且 | storage | 是 DP 公式中使用的复制存储单位的数量（大约十亿：大约是存储总预算（PB 级别）除以分区大小（MB 级别））。因此，DP 算法太昂贵了。</p><h4 id="贪心解法"><a href="#贪心解法" class="headerlink" title="贪心解法"></a>贪心解法</h4><p>作为 DP 算法的替代方法，我们提出了一种有效的贪心算法。 在每一步中，算法都会将表 i 的当前寿命 L<sub>i</sub> 最多提高 k 个单位，这由边际增益贪心地确定，其定义（公式 15）如下：</p><p><img src="/maxcompute-yugong-thesis/greedy.png" srcset="/img/loading.gif" alt></p><p>直观地，分子是通过将 L<sub>i</sub> 提前 k 个单位可以节省的跨 DC 带宽总成本，而分母是存储额外的 k 个分区副本所需的存储空间。 如果 Gain<sub>i</sub><sup>k</sup> &gt; 0，则意味着复制多余的 k 个分区可以进一步节省跨 DC 带宽。 请注意，当 L<sub>i</sub> = 0 时，我们需要从 Gain<sub>i</sub><sup>k</sup> 中减去 S<sub>i</sub>，因为我们需要使用跨 DC 带宽来复制分区 tp<sub>i</sub><sup>t<sub>cur</sub></sup>，而对于 L<sub>i</sub> &gt; 0，此成本 i 已经被 L<sub>i</sub>  = 0 时所覆盖。</p><p>下图就是提出的贪心算法（算法 1）。 该算法首先初始化最大优先队列（maxPQ），以在所有表 i 的 L<sub>i</sub> = 0 时，如果 Gain<sub>i</sub><sup>k</sup> &gt; 0，则保留所有可能的 Gain<sub>i</sub>。 然后，它将继续使 maxPQ 的最大增益出队，直到队列变空。 假设对于某个表 i，当前的最大增益为 Gain<sub>i</sub><sup>k’</sup> ，如果存储预算仍允许其他 k’ 个副本，则将 L<sub>i</sub> 提前 k’ 个单位。”l<sub>i</sub> = L<sub>i</sub>“ 条件是确保对于所有基于当前 L<sub>i</sub> 计算的 Gain<sub>i</sub><sup>k’</sup>，其中 0 ≤ k’&lt; k，i 只能使用一个 k’ 来推进 L<sub>i</sub>。 在L<sub>i</sub> 前进之后，将基于更新的 L<sub>i</sub> 来计算新增益 Gain<sub>i</sub><sup>k</sup> 并将其放入 maxPQ 中。</p><p><img src="/maxcompute-yugong-thesis/kprobe.png" srcset="/img/loading.gif" alt></p><p>以上算法的时间复杂度为 O（k | T || L | log（k | T || L |）），由于 k 仅为数百个数量级，因此它的耗费时间大大小于 DP 算法。 此外，我们证明了贪心算法在给定足够的存储预算的情况下可以获得最佳带宽成本，如下所示。</p><p>定理 1:</p><blockquote><p>将 STO<sub>rep</sub> 设置为达到等式 12 中的最佳带宽成本且 k 为最大寿命时使用的实际复制存储大小，算法 1 计算出的表复制计划能够给出与等式 12 相同的最佳带宽成本。</p></blockquote><p>当达到公式 12 中的最佳带宽成本时，令 L<sub>i</sub><sup>opt</sup> 为表 i 的寿命。 考虑算法 1 中表 i 的当前寿命 L<sub>i</sub>。我们有 L<sub>i</sub> &lt; L<sub>i</sub><sup>opt</sup>，并且将 L<sub>i</sub> 提升到 L<sub>i</sub><sup>opt</sup> 的收益高于任何 l &gt; L<sub>i</sub><sup>opt</sup> 的收益，因为 L<sub>i</sub><sup>opt</sup> 需要较少的存储空间，并且产生相同的读取次数（ 请注意，在给定无限复制存储预算的情况下，因此只要从该分区的远程读取大小大于该分区的大小，就可以复制任何分区，如公式 12 所示）因此，如果我们的存储预算与等式 12 中用于获得最佳带宽成本的实际复制存储大小相同，则在某一点上表 i 将从 maxPQ 出队，并得到 L<sub>i</sub> 到 L<sub>opt</sub> 的增益。 之后进一步提升 L<sub>i</sub> 的增益将变为 0，从而不会再入队。</p><h4 id="动态维护"><a href="#动态维护" class="headerlink" title="动态维护"></a>动态维护</h4><p>我们的贪心解决方案目前仅考虑固定的表访问模式。实际上，由于业务的增长和偶尔的临时工作，表访问模式实际上在随时间变化（尽管缓慢）。 因此，我们需要定期更新表复制计划。 假设计划每 δ 天更新一次。 现在的问题是，给定当前的复制计划 R，我们需要找到一个新的复制计划 R’，以便它可以用作接下来 δ 天的良好复制计划，以及最小化从 R 至 R’ 状态迁移的带宽消耗。 作为过渡成本的示例，假设表 i 的寿命是 R 中的 L<sub>i</sub> 和 R’ 中的 L’<sub>i</sub>，并且 L<sub>i</sub> &lt; L’<sub>i</sub>，这意味着 R’ 对表 i 的覆盖范围比 R 多。 为了将较旧的分区从（t<sub>cur</sub> -L’<sub>i</sub>）复制到（t<sub>cur</sub> -L<sub>i</sub>），需要额外的带宽以保证从 R 过渡到 R’。</p><p>更新复制计划的最简单方法是每 δ 天重新运行一次算法 1，但这可能会导致相当大的过渡成本。考虑到复制较旧分区所产生的成本，我们建议对增益函数进行简单的修改。 设 I<sub>i，t</sub> 为指标，如果 tp<sub>i</sub><sup>t</sup> 被 R 覆盖，则 I<sub>i，t</sub>  = 1，否则，I<sub>i，t</sub> = 0。 我们将 G<sub>i，t</sub> 定义为如果新计划涵盖 tp<sub>i</sub><sup>t</sup> 可以节省的带宽量。</p><p><img src="/maxcompute-yugong-thesis/incremental_maintenance_1.png" srcset="/img/loading.gif" alt></p><p>直觉是，如果分区 tp<sub>i</sub><sup>t</sup> 不在复制计划中，则将其包括在计划中需要付出一定的代价，该损失等于在 δ 天内复制 tp<sub>i</sub><sup>t</sup> 的摊余带宽成本。也就是说，除非收益很大，否则我们不鼓励复制较旧的分区。通过在公式 15 中用 G<sub>i</sub><sup>t</sup> 代替 R<sub>i</sub><sup>t</sup>，我们获得了一个新的增益函数：</p><p><img src="/maxcompute-yugong-thesis/incremental_maintenance_2.png" srcset="/img/loading.gif" alt></p><p>除了使用新的增益函数外，我们还取了前 δ 天的表访问矩阵 R<sub>i</sub><sup>t</sup> 的平均值以减少表访问模式中振荡的影响。</p><h3 id="计算调度"><a href="#计算调度" class="headerlink" title="计算调度"></a>计算调度</h3><p>计算调度，即将作业调度到非默认 DC 进行处理，该 DC 可能包含作业所需的全部或部分输入表。当输入数据较大时，计算调度可以减少跨 DC 的带宽使用。这也可以用于在分布式控制系统之间平衡负载和各种资源（例如，中央处理器、内存、磁盘、网络等）的利用率来提高整体资源利用率（从而节省生产成本）。</p><p>对比前两种离线方式，因为调度决策需要考虑分布式控制系统的负载和资源利用率，所以计算调度需要在线解决方案。此外我们还需要考虑远程分布式控制系统是否有空闲资源来运行该作业，以及预期的作业完成时间(包括远程分布式控制系统中的等待时间)是否短于作业的默认 DC 时间。因此，我们设计了一个简单的评分函数来决定是否将工作 j 调度给 DC d：</p><p><img src="/maxcompute-yugong-thesis/job_outsourcing.png" srcset="/img/loading.gif" alt></p><p>其中 Cost（j，d）和 WaitT（j，d）是跨 DC 带宽的总成本以及如果将工作 j 外包给 DC d 的估计等待时间，而 AvailResrc（d）是DC d 中的可用资源量。请注意，Cost（j，d）包括将所有必要的信息/数据发送到 DC d 以执行作业，并将作业输出传回默认的 DC。我们仔细调整了参数 α 和 β，以降低跨 DC 带宽的成本。</p><h3 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h3><p>首先报告愚公在阿里巴巴投入生产的整体表现。下图显示了在典型的一天中每个 DC 传入的跨 DC 带宽使用量的减少。 愚公将不同 DC 的跨 DC 带宽使用率从 14％ 降低到 88％ 。 DC2 具有最大的减少量，因为它具有最大的远程依赖性。当天，愚公总共减少了总带宽使用量的 76％。</p><p><img src="/maxcompute-yugong-thesis/performance.png" srcset="/img/loading.gif" alt></p><h2 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h2><ul><li>根据实际业务的工作流来将如何减少跨域流量的难题解耦为三个容易分开解决的子问题。</li><li>在数据拷贝部分进行了系统的分析与设计并就此过程中的取舍进行了讨论。</li></ul><h2 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h2><ul><li>很多简化部分很直接，这可能是容易想到的最直接最容易实现的方法，但也可能还有一定的可优化空间。</li><li>如果能够将存储空间成本和带宽成本量化似乎模型会更准确。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="地理分布式调度"><a href="#地理分布式调度" class="headerlink" title="地理分布式调度"></a>地理分布式调度</h3><p>最近有关分析工作负载的地理分布调度工作仅考虑了少量作业，并假设数据在地理分布的 DC 之间进行了分区，并且一项任务可以在多个 DC 中运行。 Iridium 优化了任务调度和数据放置，以实现分析查询的低延迟。 Geode 和 WANalytics 使查询计划了解 WAN，并为整个 DC 的数据分析提供以网络为中心的优化。 Clarinet 通过考虑网络带宽，任务位置，网络传输调度和多个并发查询，提出了一种支持 WAN 的查询优化器。Tetrium 考虑了地理分布的 DC 中用于任务放置和作业调度的计算和网络资源。Pixida 应用图分区来最大程度地减少数据分析作业中的跨 DC 任务依赖性。Hung 提出了地理分布的作业调度算法，以最大程度地减少整体作业的运行时间，但并未考虑 DC 之间的 WAN 带宽使用情况。 Bohr 利用地理分布的 OLAP 数据立方体展示了不同 DC 中数据之间的相似性。 Lube 实时检测和缓解地理分布数据分析查询中的瓶颈。</p><p>还有其他使数据流分析，分布式机器学习和图形分析可感知 WAN 的工作。 JetStream 提出了显式的编程模型，以减少分析流数据集所需的带宽。对于机器学习工作负载，Gaia 和 GDML 开发了地理分布式解决方案，以有效利用稀缺的 WAN 带宽，同时保留 ML 算法的正确性。 Monarch 和 ASAP 提出了一种地理分布图模式挖掘的近似解决方案。</p><h3 id="云原生数据仓库"><a href="#云原生数据仓库" class="headerlink" title="云原生数据仓库"></a>云原生数据仓库</h3><p>Google BigQuery，Amazon Redshift，Microsoft Azure Cosmos DB 和 Alibaba MaxCompute 是大型数据仓库产品。 MaxCompute 中的 “project” 概念对应于 Redshift 中的 “database” 和 BigQuery 中的 “project”。 尽管愚公在此工作中主要是用作 MaxCompute 的插件构建的，但类似的想法也可以应用于其他地理分布的数据仓库平台。</p><h3 id="缓存和打包"><a href="#缓存和打包" class="headerlink" title="缓存和打包"></a>缓存和打包</h3><p>从 CPU 高速缓存，内存高速缓存到应用程序级高速缓存，高速缓存管理是在不同级别的计算机体系结构上经过充分研究的主题。 Memcached 和 Redis 是高度可用的分布式键值存储，可在磁盘上提供内存缓存。EC-Cache 和 SP-Cache 为数据密集型群集和对象存储提供了内存中缓存。Piccolo，Spark，PACMan 和 Tachyon 结合了用于集群计算框架的内存缓存。在这项工作中，我们使用磁盘存储作为远程分区的缓存，以减少跨 DC 带宽。 表复制问题是背包问题的变体，项目放置问题是装箱问题的变体。 Tetris 将多资源分配问题与多维 bin 打包问题进行了类比，以进行任务调度。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="http://www.vldb.org/pvldb/vol12/p2155-huang.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式调度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JProfile 远程配置</title>
    <link href="/jprofile-remote/"/>
    <url>/jprofile-remote/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>JProfile 是 Java 常用的性能分析工具，常被用来查看内存占用，CPU 时间消耗等数据。本地配置 JProfile 较为简单，可以参考此<a href="https://www.javatt.com/p/48237" target="_blank" rel="noopener">博客</a>。在一些情况下，监控的 Java 服务跑在服务器上，因此不能使用本地的 JProfile 来监控。不过 JProfile 能够支持远程连接监控，只不过需要一些额外的配置。此博客的内容就是介绍如何使用 JProfile 监控远程的 Java 应用。</p><h2 id="配置方案一：ssh-免密连接"><a href="#配置方案一：ssh-免密连接" class="headerlink" title="配置方案一：ssh 免密连接"></a>配置方案一：ssh 免密连接</h2><ol><li>从<a href="https://www.ej-technologies.com/download/jprofiler/files" target="_blank" rel="noopener">官方网站</a>上根据操作系统下载对应本地和服务器的 JProfile（需要确保两者版本一致）。注意服务端选择下载 <code>Setup Executable</code> 的可执行脚本。</li><li>客户端安装配置好本地的 JProfile， 然后将服务器的 JProfile 利用 scp 等工具传到服务器上去（也可在上述网页中直接复制链接在服务器上 wget 或者 curl -O 下载）。</li><li>服务端执行 <code>sh ./jprofiler_linux_11_1_4.sh</code> 来安装 JProfile，无脑同意 OK 和 Enter 就能够让其正确安装并且后台运行起来。</li><li>服务端启动要监控的应用程序，如 IoTDB。</li><li>客户端打开 JProfile，点击 <code>Profile a demo session or a saved session</code>，选择 <code>New Session</code> 栏的 <code>New Session</code> 手动创建一个远程连接。<br><img src="/jprofile-remote/jprofile_0.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_1.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_2.png" srcset="/img/loading.gif" alt></li><li>填好 <code>Session name</code>，<code>Attach type</code> 选择 <code>Attach to remote JVM</code>，连接方式选择 <code>SSH tunnel</code>，然后点击 <code>Edit</code> 开始配置服务器 ip, host, ssh 秘钥等信息。<br><img src="/jprofile-remote/jprofile_3.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_4.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_5.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_6.png" srcset="/img/loading.gif" alt></li><li>配置完后点击 finish 再点击 ok 即可连接到远程的 JProfile，接着选择对应要监控的 Java 应用程序。<br><img src="/jprofile-remote/jprofile_7.png" srcset="/img/loading.gif" alt></li><li>配置监控模式，一般选择 <code>Async sampling</code>，该模式对性能影响最小。<br><img src="/jprofile-remote/jprofile_8.png" srcset="/img/loading.gif" alt></li><li><p>配置调用树的过滤器，该参数一般用来指示 jprofile 来记录 cpu view 时的函数耗时统计。加上要统计的包的前缀即可，比如 <code>org.apache.iotdb</code>。<br><img src="/jprofile-remote/jprofile_9.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_10.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_11.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_12.png" srcset="/img/loading.gif" alt></p></li><li><p>可以开始监控分析了！<br><img src="/jprofile-remote/jprofile_13.png" srcset="/img/loading.gif" alt></p></li><li>之后每次远程连接都可以直接到 <code>Open Session</code> 栏选择这次建好的 session 即可，不用再进行配置了。<br><img src="/jprofile-remote/jprofile_14.png" srcset="/img/loading.gif" alt></li></ol><h2 id="配置方案二：http-端口连接"><a href="#配置方案二：http-端口连接" class="headerlink" title="配置方案二：http 端口连接"></a>配置方案二：http 端口连接</h2><ol><li>从<a href="https://www.ej-technologies.com/download/jprofiler/files" target="_blank" rel="noopener">官方网站</a>上根据操作系统下载对应本地和服务器的 JProfile（需要确保两者版本一致）。注意服务端选择下载 <code>TAR.GZ Archive</code> 的压缩包。</li><li>客户端安装配置好本地的 JProfile， 然后将服务器的 JProfile 利用 scp 等工具传到服务器上去（也可在上述网页中直接复制链接在服务器上 wget 或者 curl -O 下载）。</li><li>服务端启动要监控的应用程序，如 IoTDB。</li><li>服务端执行<code>tar -zxvf jprofiler_linux_11_1_4.tar.gz</code>解压压缩包，然后执行 <code>jprofiler11.1.4/bin/jpenable -p 10000</code> 手动指定 10000 端口来启动 JProfile。（注，必须有运行的 JVM 才可以启动，并且程序一旦关闭重启服务端的 jprofile 也需要重新启动。）</li><li>客户端打开 JProfile，点击 <code>Profile a demo session or a saved session</code>，选择 <code>New Session</code> 栏的 <code>New Session</code> 手动创建一个远程连接。<br><img src="/jprofile-remote/jprofile_0.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_1.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_2.png" srcset="/img/loading.gif" alt></li><li>填好 <code>Session name</code>，<code>Attach type</code> 选择 <code>Attach to remote JVM</code>，连接方式选择 <code>Directly connection to</code>，配置好 host 并指定 <code>Profiling port</code> 为服务端手动指定的端口 10000。<br><img src="/jprofile-remote/jprofile_15.png" srcset="/img/loading.gif" alt></li><li>配置完后点击 ok 即可连接到远程的 JProfile，接着选择对应要监控的 Java 应用程序。<br><img src="/jprofile-remote/jprofile_7.png" srcset="/img/loading.gif" alt></li><li>配置监控模式，一般选择 <code>Async sampling</code>，该模式对性能影响最小。<br><img src="/jprofile-remote/jprofile_8.png" srcset="/img/loading.gif" alt></li><li>配置调用树的过滤器，该参数一般用来指示 jprofile 来记录 cpu view 时的函数耗时统计。加上要统计的包的前缀即可，比如 <code>org.apache.iotdb</code>。<br><img src="/jprofile-remote/jprofile_9.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_10.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_11.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_12.png" srcset="/img/loading.gif" alt></li><li>可以开始监控分析了！<br><img src="/jprofile-remote/jprofile_13.png" srcset="/img/loading.gif" alt></li><li>之后每次远程连接都可以直接到 <code>Open Session</code> 栏选择这次建好的 session 即可，不用再进行配置了。<br><img src="/jprofile-remote/jprofile_14.png" srcset="/img/loading.gif" alt></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本博客傻瓜式介绍了使用 JProfile 来对远程服务器上运行的 Java 服务进行监控分析的两种方式。其中 ssh 免密连接方式更方便，其启动时不需要有启动的 JVM 且能够自动检测随后启动的 JVM 让客户端在连接时再挑选。 http 连接方式在监控的 JVM 关闭重启后服务端的 jprofile 也需要手动重启，较为麻烦。因此建议大家都使用第一种连接方式。</p>]]></content>
    
    
    
    <tags>
      
      <tag>开发工具配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BASE 定理介绍</title>
    <link href="/base-theory/"/>
    <url>/base-theory/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>CAP 理论表明，对于一个分布式系统而言，它是无法同时满足 Consistency（强一致性）、Availability（可用性）和 Partition tolerance（分区容忍性）这三个条件的，最多只能满足其中两个。</p><p>对于绝大多数互联网应用来说，由于网络环境是不可信的，所以分区容错性（P）必须满足。</p><p>如果只能在一致性和可用性之间做出选择的话，大部分情况下大家都会选择牺牲一部分一致性来保证可用性。因为如果不返回给用户数据，用户的体验会十分差，因此很多应用宁肯拒绝服务也不会能访问却没有数据。当然，在一些较为严格的场景比如支付场景下，强一致性是必须要满足的。</p><p>好了，我们只能放弃一致性，但是我们真这样做了，将一致性放弃了，现在这个系统返回的数据你敢信吗？没有一致性，系统中的数据也就从根本上变得不可信了，那这数据拿来有什么用，那这个系统也就没有任何价值，根本没用。</p><p>如上所述，由于我们三者都无法抛弃，但 CAP 定理限制了我们三者无法同时满足。在这种情况下，我们只能选择尽量靠近 CAP 定理，即尽量让 C、A、P 都满足。在此大势所趋下，eBay 的架构师 Dan Pritchett 源于对大规模分布式系统的实践总结，在 ACM 上发表文章提出 BASE 理论，其是基于 CAP 定理逐步演化而来的。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>BASE 理论是 Basically Available(基本可用)，Soft State（软状态）和 Eventually Consistent（最终一致性）三个短语的缩写。</p><p>其核心思想是：</p><blockquote><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。</p></blockquote><h3 id="Basically-Available"><a href="#Basically-Available" class="headerlink" title="Basically Available"></a>Basically Available</h3><p>基本可用是相对于正常的系统来说的，常见如下情况：</p><ul><li><p>响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而基本可用的搜索引擎可以在 2 秒作用返回结果。</p></li><li><p>功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单。但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。</p></li></ul><h3 id="Soft-state"><a href="#Soft-state" class="headerlink" title="Soft state"></a>Soft state</h3><p>软状态是相对原子性来说的：</p><ul><li>原子性（硬状态）：要求多个节点的数据副本都是一致的，这是一种”硬状态”。</li><li>软状态（弱状态）：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延迟。</li></ul><h3 id="Eventually-Consistent"><a href="#Eventually-Consistent" class="headerlink" title="Eventually Consistent"></a>Eventually Consistent</h3><p>最终一致性是相对强一致性来说的：</p><ul><li>系统并不保证连续进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。最终一致性是弱一致性的特定形式。</li><li>对于软状态，我们允许中间状态存在，但不可能一直是中间状态，必须要有个期限，系统保证在没有后续更新的前提下，在这个期限后，系统最终返回上一次更新操作的值，从而达到数据的最终一致性，这个容忍期限（不一致窗口的时间）取决于通信延迟，系统负载，数据复制方案设计，复制副本个数等。DNS 就是一个典型的最终一致性系统。</li></ul><h2 id="最终一致性的种类"><a href="#最终一致性的种类" class="headerlink" title="最终一致性的种类"></a>最终一致性的种类</h2><p>在实际工程实践中，最终一致性常被分为 5 种：</p><h3 id="因果一致性（Causal-consistency）"><a href="#因果一致性（Causal-consistency）" class="headerlink" title="因果一致性（Causal consistency）"></a>因果一致性（Causal consistency）</h3><p>如果节点 A 在更新完某个数据后通知了节点 B，那么节点 B 之后对该数据的访问和修改都是基于 A 更新后的值。与此同时，和节点 A 无因果关系的节点 C 的数据访问则没有这样的限制。</p><h3 id="读己之所写（Read-your-writes）"><a href="#读己之所写（Read-your-writes）" class="headerlink" title="读己之所写（Read your writes）"></a>读己之所写（Read your writes）</h3><p>节点 A 更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性。</p><h3 id="会话一致性（Session-consistency）"><a href="#会话一致性（Session-consistency）" class="headerlink" title="会话一致性（Session consistency）"></a>会话一致性（Session consistency）</h3><p>系统能保证在同一个有效的会话中实现 “读己之所写” 的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。</p><h3 id="单调读一致性（Monotonic-read-consistency）"><a href="#单调读一致性（Monotonic-read-consistency）" class="headerlink" title="单调读一致性（Monotonic read consistency）"></a>单调读一致性（Monotonic read consistency）</h3><p>如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。</p><h3 id="单调写一致性（Monotonic-write-consistency）"><a href="#单调写一致性（Monotonic-write-consistency）" class="headerlink" title="单调写一致性（Monotonic write consistency）"></a>单调写一致性（Monotonic write consistency）</h3><p>一个系统要能够保证来自同一个节点的写操作被顺序的执行。</p><p><img src="/base-theory/consistency_level.png" srcset="/img/loading.gif" alt></p><blockquote><p>在实际的实践中，这 5 种系统往往会结合使用，以构建一个具有最终一致性的分布式系统。</p></blockquote><p>事实上，最终一致性并不是只有那些大型分布式系统才设计的特性，许多现代的关系型数据库都采用了最终一致性模型。在现代关系型数据库中，大多都会采用同步和异步方式来实现主备数据复制技术。在同步方式中，数据的复制通常是更新事务的一部分，因此在事务完成后，主备数据库的数据就会达到一致。而在异步方式中，备库的更新往往存在延时，这取决于事务日志在主备数据库之间传输的时间长短，如果传输时间过长或者甚至在日志传输过程中出现异常导致无法及时将事务应用到备库上，那么很显然，从备库中读取的的数据将是旧的，因此就出现了不一致的情况。当然，无论是采用多次重试还是将数据修正，关系型数据库还是能够保证最终数据达到一致——这就是关系数据库提供最终一致性保证的经典案例。</p><h2 id="ACID-与-BASE"><a href="#ACID-与-BASE" class="headerlink" title="ACID 与 BASE"></a>ACID 与 BASE</h2><h3 id="ACID-是什么"><a href="#ACID-是什么" class="headerlink" title="ACID 是什么"></a>ACID 是什么</h3><p>ACID，是指数据库管理系统在写入或更新数据的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：</p><ul><li>原子性（atomicity，或称不可分割性）</li><li>一致性（consistency）</li><li>隔离性（isolation，又称独立性）</li><li>持久性（durability）</li></ul><h4 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h4><p>一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。</p><h4 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h4><p>在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。</p><h4 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h4><p>数据库允许多个并发事务同时对齐数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</p><h4 id="持久性"><a href="#持久性" class="headerlink" title="持久性"></a>持久性</h4><p>事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</p><h3 id="区别和联系"><a href="#区别和联系" class="headerlink" title="区别和联系"></a>区别和联系</h3><ul><li>ACID 是传统数据库常用的设计理念, 追求强一致性模型。</li><li>BASE 支持的是大型分布式系统，提出通过牺牲强一致性获得高可用性。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说 BASE 理论面向的是大型高可用、可扩展的分布式系统。与传统 ACID 特性相反，不同于 ACID 的强一致性模型，BASE 理论基于 CAP 定理提出通过牺牲强一致性来获得可用性，并允许数据段时间内的不一致，但是最终达到一致状态。同时，在实际分布式场景中，不同业务对数据的一致性要求不一样。因此在设计中，ACID 和 BASE 理论往往又会结合使用。</p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Monarch 论文阅读</title>
    <link href="/monarch/"/>
    <url>/monarch/</url>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Monarch 是谷歌的一个全球分布的内存时间序列数据库系统，它被广泛用在监控谷歌上十亿用户规模的应用程序和系统的可用性、正确性、性能、负载和其他方面。</p><p>Monarch 自 2010 年开始持续运行，收集、组织、存储、查询大量全球范围内快速增长的时间序列数据。它目前在内存中存储近 PB 的压缩时间序列数据，每秒摄入 TB 的数据，每秒处理数百万次查询。</p><p>本文介绍了系统的结构，以及在区域分布式架构下实现可靠、灵活的统一系统的新机制。我们也分享了十年来在谷歌中开发和运行 Monarch 作为服务的经验教训。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>谷歌有大量的计算机系统监控需求。数以千计的团队正在运营面向全球用户的服务(如 YouTube、GMail 和 Google Maps)，或者为这些服务(如 Spanner、Borg 和 F1)提供硬件和软件基础设施。这些团队需要监视不断增长和变化的异构实体(例如设备、虚拟机和容器)集合，这些实体数量达数十亿，分布在全球各地。</p><p>谷歌必须从这些实体中收集度量，以时间序列的形式存储，并进行查询，以支持以下用例:</p><ol><li>当被监视的服务没有正确执行时检测和警报;</li><li>显示显示服务状态和运行状况的图形仪表板;</li><li>针对问题的不可知性执行特别查询，探索性能和资源使用情况。</li></ol><p>Borgmon 是谷歌内部的初代监控系统。随着大数据时代的到来，Borgmon 的部署规模逐渐扩大，暴露了一些弊端:</p><ol><li>Borgmon 常被鼓励使用成一种分散的架构，即每个团队都建立并管理自己的 Borgmon 实例，这导致一些人力资源的浪费。此外，用户经常需要跨应用程序和基础设施边界检查和关联监控数据以解决问题，而跨多个 Borgmon 实例来操作是很难或不可能实现的;</li><li>Borgmon 缺乏度量符号和度量值的图表化，导致了查询的语义歧义，限制了查询语言在数据分析过程中的表达能力;</li><li>Borgmon 不支持分布(即柱状图)值类型，这是一种强大的数据结构，能够进行复杂的统计分析(例如，计算跨多个服务器的请求延迟的99%);</li><li>Borgmon 要求用户手动跨多个实例对大量被监视的全局服务实体进行切分，并建立查询评估树。</li></ol><p>考虑到这些经验教训，Monarch 成为了谷歌的下一代大规模监控系统。它的设计是为了适应持续增长的流量，并支持不断扩展的用例集。它为所有团队提供单一的统一服务，从而最大限度地减少了操作的工作量。它有一个简化的数据模型，便于复杂的查询和分布式类型时间序列的全面支持。</p><h2 id="系统概述"><a href="#系统概述" class="headerlink" title="系统概述"></a>系统概述</h2><p>Monarch 的设计理念：</p><ul><li>Monarch 技术上被设计成了一个 AP 系统而不是 CP 系统。他们认为在监控报警的场景下可用性比一致性重要得多，因为这能够显著增加检测到异常情况和减轻异常影响的平均时间。</li><li>Monarch 的关键报警路径上不能产生循环依赖，即不能使用谷歌内部的 Bigtable, Colossus (the successor to GFS), Spanner, Blobstore, and F1 等存储系统再去监控这些存储系统的集群（非关键路径可以适当使用）。</li><li>结合全局管理和查询对区域 zone 进行本地监控。在降低延迟，减少可靠性问题的同时能够提供全球视角。</li><li>为了可靠性，Monarch 的全局组件在地理上进行复制，并使用最接近的副本与地域性组件交互以利用局部性，其区域 zone 中的组件将跨集群复制。</li></ul><p><img src="/monarch/system_overview.png" srcset="/img/loading.gif" alt></p><p>从功能上讲，为了可靠性，Monarch 组件可以分为三类：持有状态组件、数据存储组件和查询执行组件。</p><p>持有状态组件：</p><ul><li>叶子（Leaves）将监视数据存储在内存中的时间序列存储中。</li><li>恢复日志（Recover Logs）将与叶子相同的监视数据存储在磁盘上，这些数据最终会被重写到一个长期时间序列存储库中。</li><li>全局配置服务器及其分区镜将配置数据保存在 Spanner 数据库中。</li></ul><p>数据存储组件：</p><ul><li>摄取路由器（Ingestion Routers）使用时间序列键中的信息来路由数据到适当的 Monarch 区域中的叶路由器。</li><li>叶路由器（Leaf Routers）接受数据存储在一个区域 zone，并路由到叶子存储。</li><li>范围分配管理器（Range Assigner）将数据分配到叶子，以平衡在一个区域的叶子之间的负载。</li></ul><p>查询执行组件：</p><ul><li>混合器（Mixers）将查询划分为多个子查询，将其路由到叶子去执行并合并子查询结果。查询可以在根级别(由根混合器)或在区域 zone 级别(由区域混合器)发出。根级查询同时涉及根和区域 zone 混合器。</li><li>索引服务器（Index Server）为每个区域 zone 和叶节点索引数据，并指导分布式查询执行。</li><li>评估器（Evaluator）定期向混合器发出长期查询并将结果写回叶子。</li></ul><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p><img src="/monarch/data_model.png" srcset="/img/loading.gif" alt><br>从概念上讲，Monarch 将监控数据存储为结构化表中的时间序列。每个表由多个键列组成时间序列键和一个值列组成时间序列点的历史，如图 2 所示。键列也称为字段，有两个源：目标（Target）和指标（Metric）。</p><h3 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h3><p>Monarch 使用 Target 将每个时间序列与它的源实体(或监视实体)相关联。例如，源实体就是生成时间序列的进程或 VM。每个 Target 表示一个受监视的实体，并符合一个 Target Schema，该模式定义了一组有序的 Target 字段名和相关字段类型。上图显示了一个名为 ComputeTask 的流行目标模式：每个 ComputeTask 目标标识 Borg 集群中正在运行的任务，具有四个字段：user、job、clsuter 和 task_num。</p><p>Monarch 将数据存储在离生成数据很近的位置。每个 Target Schema 都有一个标注为 location 的字段；这个位置字段的值决定了时间序列被路由和存储到的特定 Moranch zone。</p><p>在每个 zone 内，Monarch 将同一目标的时间序列存储在同一叶子中，因为它们来自同一实体，并且更有可能在一个 join 中被一起查询。Monarch 还将目标以<code>[S_start,S_end]</code>的形式分组为不同的目标范围，其中 S_start 和 S_end 是开始和结束目标字符串。目标字符串通过有序连接 Target Schema 名称来表示。</p><p>Target 范围用于字典分片和叶节点间的负载均衡；这允许在查询中更有效地跨邻近目标聚合。</p><h3 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h3><p>Metric 测量被监视目标的一个方面，如任务所服务的 RPC 数量、VM 的内存使用情况等。与 Target 类似，Metric 也有 Metric Schema，该模式定义时间序列值类型和一组指标字段。指标的命名类似于文件。图 2 显示了一个称为 /rpc/server/latency 的示例度量，它测量 RPC 到服务器的延迟；它有两个度量字段，通过 service 和 command 来区分 RPC。</p><p><img src="/monarch/metric.png" srcset="/img/loading.gif" alt><br>值类型可以是 bool、int64、double、string、分布类型或其他类型的元组。除了分布类型之外，它们都是标准类型，分布类型是一种紧凑类型，表示大量双精度值。分布包括一个直方图，该直方图将一组 double 值划分到称为 bucket 的子集中，并使用总体统计信息(如平均值、计数和标准差)汇总每个 bucket 中的值。桶边界是可配置的，可以在数据粒度(即精度)和存储成本之间进行权衡：用户可以为更流行的值范围指定更细的桶。图 3 显示了一个测试组分布类型的 /rpc/server/latency 时间序列，它测量服务器在处理 RPC 时的延迟；它有一个固定的桶大小为 10ms。</p><p>Exemplars：分布中的每个桶可以包含该桶中的值的一个范例。这会便于用户轻易的从直方图中找到慢 RPC 的详细信息。</p><p>Metric types：可以是度量类型或累积类型。累积 Metric 对于支持由许多服务器组成的分布式系统非常重要，这些服务器可能由于作业调度而定期重新启动，在重新启动期间可能会丢失一些点。</p><h2 id="可扩展收集"><a href="#可扩展收集" class="headerlink" title="可扩展收集"></a>可扩展收集</h2><p>为了实时摄取大量的时间序列数据，Monarch 采用了两种分治策略和一种关键的优化，在收集过程中对数据进行聚合。</p><h3 id="数据收集概述"><a href="#数据收集概述" class="headerlink" title="数据收集概述"></a>数据收集概述</h3><p>摄取路由器根据位置字段将时间序列数据划分给 zone，叶路由器根据范围分配管理器将数据分布在叶子之间。</p><p>一些细节：</p><ul><li>客户端会按照特定频率将数据发送到离其最近的摄取路由器（全球分布）。</li><li>location-to-zone 的映射配置在摄取路由器中且可以动态更新。</li><li>每个叶路由器维护一个持续更新的 range map，该映射将每个目标范围映射到三个叶子副本。注意叶路由器从叶子而不是从范围分配管理器获得范围映射的更新。</li><li>最小化内存碎片和分配混乱。为了在 CPU 和内存之间实现平衡，内存存储只执行少量压缩，比如时间戳共享和增量编码。时间戳共享非常有效:一个时间戳序列平均被大约 10 个时间序列共享。</li><li>尽最大努力异步向分布式文件系统写恢复日志但不需要确认来隔离分布式文件系统的故障。 </li><li>叶子收集的数据还会触发用于约束读放大的 zone 和 root 索引服务器中的更新。</li></ul><h3 id="zone-内部的负载均衡"><a href="#zone-内部的负载均衡" class="headerlink" title="zone 内部的负载均衡"></a>zone 内部的负载均衡</h3><p>一个表模式由一个 target 模式和 schema 模式组成。target 模式的编码是 Monarch 用来分片的依据，这减少了写放大，即一次 RPC 可以携带一个 target 和几百个 metric，且这些数据最终最多被送到 3 个叶副本（不是有个 range map 吗，如果刚好在边界呢？）。这样的分配有利于查询操作的下推。</p><p>一些细节：</p><ul><li>副本数（1-3）可自由配置。</li><li>通常，Monarch zone 包含多个故障域(集群)中的叶子；分配者会将范围的副本分配到不同的故障域。</li><li>范围分配管理器会根据 CPU 负载，内存使用量等变量来对不同的 Range 进行分裂或合并。</li><li>迁移时新节点利用恢复日志来重新构建数据，此外迁移过程中是新旧双写的，这样就不影响上层应用的可用性。</li></ul><h3 id="收集聚合"><a href="#收集聚合" class="headerlink" title="收集聚合"></a>收集聚合</h3><p>在有些监控场景下，用户只想知道整个人或某个集群的某个监控信息一段时间内的统计值而不是每个值。<br><img src="/monarch/collection_aggregation.png" srcset="/img/loading.gif" alt></p><p>Monarch 利用分组连续聚合的方式收集数据并抛弃过时的数据，还用了 TrueTime 大杀器来确保时间分组的正确，实现了实时收集聚合。</p><h2 id="可扩展查询"><a href="#可扩展查询" class="headerlink" title="可扩展查询"></a>可扩展查询</h2><p>为了查询时间序列数据，Monarch 提供了一种由分布式引擎支持的表达语言，该分布式引擎使用静态不变量和新的索引来本地化查询执行。</p><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p><img src="/monarch/query_language.png" srcset="/img/loading.gif" alt><br>听他吹的很牛逼就完事了。类似于 SQL，表达能力比较强吧。</p><h3 id="读流程概述"><a href="#读流程概述" class="headerlink" title="读流程概述"></a>读流程概述</h3><p>系统中有两种查询:临时查询和长期查询。前者是系统外的用户偶尔执行的，后者是 Monarch 周期性执行的物化视图，还会被重新写入到 Monarch 中来提高查询效率和报警。</p><p>查询时也会分三层，即先从根混合器分配到 zone 混合器，然后再被分配到叶子中去。当然为了减少读放大，混合器都会向同一级别的索引服务器请教来剪枝（类似于布隆过滤器）。</p><p>Monarch 叶子的不同副本之间不是完全实时一致（最终一致），其会有一个质量参数，查询时混合器会对该时间序列不同 range 的分片挑选其质量最高的副本来读数据。</p><p>Monarch 做了租户隔离，会跟踪每个用户在集群中查询时使用的总内存，并在越界时取消查询，同时也为每个用户分配了公平的 CPU 时间。</p><h3 id="查询下推"><a href="#查询下推" class="headerlink" title="查询下推"></a>查询下推</h3><p>查询下推增加了可评估的查询规模，并减少了查询延迟，原因如下：</p><ul><li>更低级别的评估越多，意味着更稳定和均匀分布的负载。</li><li>在较低级别计算的全部或部分聚合大大减少传输到较高级别节点的数据量。</li></ul><h3 id="提示字段索引"><a href="#提示字段索引" class="headerlink" title="提示字段索引"></a>提示字段索引</h3><p>为了实现高可伸缩性，Monarch 使用存储在索引服务器中的字段提示索引(FHI)来限制从父节点向子节点发送查询时放大，方法是跳过不相关的子节点(那些没有向特定查询输入数据的子节点)。一个 FHI 是一个简明的，持续更新的索引时间序列字段值。FHIs 通过分析查询中的字段谓词跳过不相关的子字段，并有效地处理正则表达式谓词，而无需遍历精确的字段值。FHI 工作的区域有数万亿个时间序列键和超过 10000 个叶子，同时保持足够小的大小以存储在内存中。</p><h3 id="可靠查询"><a href="#可靠查询" class="headerlink" title="可靠查询"></a>可靠查询</h3><ul><li>Monarch 的查询能够在文件系统或全局组件失效时继续工作。</li><li>Monarch 时间序列的不同分片之间存在重叠。即适度的冗余存储有助于读性能的提升。</li><li>Monarch 查询时还会有一些回退叶子来防止某些慢叶子对查询的影响。即 zone 混合器会在主叶和回退叶之间并行地继续进行，并从两个中较快的叶中提取和删除响应。</li></ul><h2 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h2><p>由于 Monarch 作为分布式的、多租户的服务运行的特性，需要一个集中的配置管理系统来为用户提供方便的、细粒度的控制，以便在整个系统中对其监视和分布配置进行控制。用户与影响所有 Monarch zone 的单一全局配置视图交互。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p><img src="/monarch/evalution.png" srcset="/img/loading.gif" alt></p><p>数据量很大，增速很快，节点很多，我很牛逼…</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>目前有许多开源时间序列数据库：Graphite，InfluxDB， OpenTSDB， Prometheus，和 tsdb 是最受欢迎的。它们将数据存储在辅助存储上(本地或分布式存储，如HBase)；辅助存储的使用使它们在关键监视时的可使用性降低。它们通过类似于 Monarch zone 的水平扩展来支持分布式部署，但是它们缺乏 Monarch 所提供的全局配置管理和查询聚合。</p><h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><ul><li>时间序列键的字典序分片改进了摄取和查询的可伸缩性，使 Monarch zone 能够扩展到数以万计的叶子。</li><li>基于推的数据收集模式提高了系统的健壮性，同时简化了系统架构。</li><li>系统化的数据模型提高了健壮性和性能。</li><li>系统扩展是一个连续的过程。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Monarch 可以在很大规模下高效可靠地运行，这是由于它将自治 zone 监控子系统通过全局配置和查询平面整合成一个连贯的整体。它采用了一种新颖的、类型丰富的关系时间序列数据模型，允许高效和可伸缩的数据存储，同时为数据分析提供了一种表达性查询语言。为了适应这种大规模，Monarch 在数据收集和查询执行方面使用了各种优化技术。对于数据收集，Monarch 服务器执行区域内负载平衡和收集聚合，以提高可靠性和效率。对于查询执行，Monarch 以分布式、分层的方式执行每个查询，执行积极的过滤和聚合下推以提高性能和吞吐量，并利用紧凑而强大的分布式索引进行高效的数据修剪。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="http://www.vldb.org/pvldb/vol13/p3181-adams.pdf" target="_blank" rel="noopener">论文</a></p><p><a href="https://www.datacouncil.ai/talks/monarch-googles-planet-scale-streaming-monitoring-infrastructure" target="_blank" rel="noopener">PPT</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 博士论文翻译</title>
    <link href="/raft-thesis-translate/"/>
    <url>/raft-thesis-translate/</url>
    
    <content type="html"><![CDATA[<p>翻译原文在<a href="https://github.com/LebronAl/raft-thesis-zh_cn" target="_blank" rel="noopener">这里</a>，欢迎提 PR！</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>当今的数据中心系统和应用程序在高度动态的环境中运行。它们通过利用额外服务器的资源横向扩展，并根据需求扩展和收缩。服务器和网络故障也很常见：每年约有 2-4% 的磁盘驱动器发生故障，服务器崩溃的频率与之相当，在现代数据中心中，每天都有数十个网络链接失败。</p><p>因此，系统必须在正常操作期间处理服务器的上下线。他们必须对变故做出反应并在几秒钟内自动适应；对客户来说明显的中断通常是不可接受的。这是当今系统中的主要挑战；在这种动态环境中，故障处理，协调，服务发现和配置管理都很困难。</p><p>幸运的是，分布式共识可以帮助应对这些挑战。共识允许一组机器作为一个一致的组来工作，这些组可以承受某些成员的故障。在一个达成共识的团队中，故障是用一种有原则的、行之有效的方式来处理的。由于共识组具有很高的可用性和可靠性，所以其他系统组件可以使用共识组作为自身容错的基础。因此，共识在构建可靠的大规模软件系统中起着关键作用。</p><p>当我们开始这项工作时，达成共识的需求变得越来越明显，但是许多系统仍在努力解决共识可以解决的问题。一些大型系统仍然受到单个协调服务器作为单点故障的限制（例如，HDFS）。许多其他方法包括不安全地处理故障的特殊复制算法（例如，MongoDB 和 Redis）。新系统几乎没有可供选择的现成共识实现方式（ZooKeeper 是最受欢迎的），这迫使系统构建者要么遵从一个，要么构建一个自己的一致实现。</p><p>那些选择实现共识的人通常会参考 Paxos。在过去的二十年中，Paxos 主宰了共识的讨论：共识的大多数实现都基于 Paxos 或受其影响，并且 Paxos 已成为用于向学生传授共识的主要工具。</p><p>不幸的是，尽管有很多尝试使 Paxos 变得更加平易近人，但它还是很难理解。此外，其体系结构需要进行复杂的更改以支持实际系统，并且基于 Paxos 构建完整的系统需要开发一些扩展，而这些扩展的详细信息尚未发布或未得到商定。结果，系统构建者和学生都在Paxos 中挣扎。</p><p>另外两个著名的共识算法是 Viewstamped Replication 和 Zab，ZooKeeper 中使用的算法。尽管我们认为这两种算法在结构上都比 Paxos 更好，但都没有明确提出这一论点。它们的设计并非以简单或易于理解为主要目标。理解和实现这些算法的负担仍然过高。</p><p>这些共识算法中的每一个都难以理解且难以实施。不幸的是，当用公认的算法来实现共识的成本太高时，系统构建者就不得不做出艰难的决定。他们可能会完全避免达成共识，从而牺牲其系统的容错性或一致性，或者他们可能会开发自己的特殊算法，从而可能导致不安全的行为。而且，当解释和理解共识的成本太高时，并不是所有的教师都试图教授它，也不是所有的学生都能成功地学习它。共识与两阶段提交一样重要。理想情况下，应有尽可能多的学生学习（即使从根本上来说，学习共识更为困难）。</p><p>在与 Paxos 斗争之后，我们着手寻找一种新的共识算法，该算法可以为系统构建和教育提供更好的基础。我们的方法与众不同，因为我们的主要目标是可理解性：我们可以为实际系统定义一个一致的算法，并以一种比 Paxos 更容易学习的方式来描述它吗？此外，我们希望该算法能够帮助系统构建者开发至关重要的直觉。重要的不仅是算法能起作用，还有它为什么很明显能起作用。</p><p>该算法还必须足够完整，以解决构建实际系统的所有方面，并且必须在实际部署时表现良好。核心算法不仅必须指定接收消息的效果，还必须描述应该发生什么以及何时发生。这些对于系统构建者同样重要。同样，它必须保证一致性，并且还必须尽可能提供可用性。它还必须解决超出达成共识系统的许多方面，例如更改共识组的成员。这在实践中是必要的，将这个负担留给系统构建者将会有特别的、次优的甚至不正确的解决方案的风险。</p><p>这项工作的结果是一个称为 Raft 的共识算法。在设计 Raft 时，我们应用了特定的技术来提高可理解性，包括分解（Raft 分离了领导者选举，日志复制和安全性）和状态空间缩减（Raft 减少了不确定性的程度以及服务器彼此之间不一致的方式）。我们还解决了构建基于共识的完整系统所需的所有问题。我们仔细考虑了每个设计选择，不仅是为了自己的实现，还为了我们希望实现的许多其他设计。</p><p>我们认为，无论是出于教育目的还是作为实现的基础，Raft 都优于 Paxos 和其他共识算法。它比其他算法更简单，更容易理解；它的描述完全能够满足实际系统的需要；它有几个开源实现并被一些公司使用；其安全性能已被正式规定和证明；其效率与其他算法相当。</p><p>本文的主要贡献如下：</p><ul><li>Raft 共识算法的设计，实现和评估。 Raft 在许多方面都与现有的共识算法（最著名的是 Oki 和 Liskov 的 Viewstamped复制）相似，但其设计目的是为了易于理解。这导致了几种新颖的功能。例如，与其他共识算法相比，Raft 使用了一种更强大的领导形式。这简化了日志复制的管理，并使 Raft 更易于理解。</li><li>对 Raft 的可理解性进行评估。一项对两所大学的 43 名学生进行的用户研究表明，Raft 比 Paxos 更容易理解：学习了这两种算法后，其中 33 名学生能够更好地回答有关 Raft 而不是 Paxos 的问题。我们相信这是第一个基于教学来评估一致性算法的科学研究。</li><li>Raft 领导者选举机制的设计，实现和评估。尽管许多共识算法未规定特定的领导者选举算法，但 Raft 包含了一种涉及随机计时器的特定算法。这可以为任何共识算法已经要求的心跳添加少量机制，同时可以轻松，快速地解决冲突。对领导者选举的评估考察了它的行为和性能，得出结论，这种简单的方法在广泛的实际环境中是足够的。它通常在集群的单向网络延迟的 20 倍以内选出一个领导者。</li><li>Raft 集群成员更改机制的设计和实现。Raft 允许一次添加或删除单个服务器；这些操作简单地保持了安全性，因为更改期间至少有一台服务器与大多数服务器重叠。成员中更复杂的更改可以通过一系列单服务器更改来实现。Raft 允许集群在更改期间继续正常运行，并且成员更改可以通过对基本共识算法的一些扩展来实现。</li><li>一个完整的基于共识的系统所需的其他组件的详细讨论和实现，包括客户端交互和日志压缩。尽管我们不认为 Raft 的这些方面特别新颖，但是完整的描述对于可理解性和使他人能够构建实际系统很重要。我们已经实现了一个完整的基于共识的服务，以探索和处理所有涉及的设计决策。</li><li>Raft 算法的安全性和正式规范的证明。正式规范中的精度级别有助于对算法进行仔细的推理，并在算法的非正式描述中澄清细节。安全性的证明有助于建立对 Raft 正确性的信心。它还帮助那些希望扩展 Raft 的人，阐明扩展对安全性的影响。</li></ul><p>我们已经在一个称为 LogCabin 的 Raft 开源实现中实现了本文中的许多设计。LogCabin 是我们对于 Raft 新想法的测试平台，并且是一种验证我们是否了解构建完整且实用的系统的方法。该实现在第 10 章中有更详细的描述。</p><p>本文的其余部分介绍了复制状态机问题，并讨论了 Paxos 的优缺点（第 2 章）。 介绍了 Raft 共识算法，它对集群成员更改和日志压缩的扩展，以及客户端如何与 Raft 交互（第 3-6 章）；评估 Raft 的可理解性，正确性，领导者选举和日志复制性能（第 7-10 章）；并讨论相关工作（第 11 章）。</p><h2 id="2-动机"><a href="#2-动机" class="headerlink" title="2 动机"></a>2 动机</h2><p>共识是可容错系统中的一个基本问题：即使面对故障，服务器如何在共享状态上达成一致？ 这个问题出现在需要提供高可用性且不能在一致性上妥协的各种系统中。因此，共识实际上几乎用于所有一致的大型存储系统中。第 2.1 节描述了通常如何使用共识来创建复制状态机，而复制状态机是容错系统的通用模块。第 2.2 节讨论了在大型系统中使用复制状态机的各种方式。第 2.3 节讨论了 Raft 旨在解决的 Paxos 共识协议的问题。</p><h3 id="2-1-使用复制状态机实现容错"><a href="#2-1-使用复制状态机实现容错" class="headerlink" title="2.1 使用复制状态机实现容错"></a>2.1 使用复制状态机实现容错</h3><p>共识算法通常出现在复制状态机的环境中。在此过程中，服务器集合上的状态机计算相同状态的相同副本，即使某些服务器宕机，状态机也可以继续运行。复制状态机用于解决分布式系统中的各种容错问题，如第 2.2 节所述。复制状态机的示例包括 Chubby 和 ZooKeeper，它们都为少量的配置数据提供了分层的键值存储。除了诸如 get 和 put 之类的基本操作之外，它们还提供了诸如 compare-and-swap 之类的同步原语，从而使并发的客户端能够安全地协调工作。</p><p>复制状态机通常使用复制日志来实现，如图 2.1 所示。每个服务器存储一个包含一系列命令的日志，其状态机按顺序执行这些命令。每个日志以相同的顺序包含相同的命令，因此每个状态机处理相同的命令序列。由于状态机是确定性的，因此每个状态机都计算相同的状态和相同的输出序列。</p><blockquote><p><img src="/raft-thesis-translate/2_1.png" srcset="/img/loading.gif" alt><br>图 2.1：复制状态机架构。共识算法管理包含来自客户端的状态机命令的复制日志。 状态机处理来自日志的相同命令序列，因此它们产生相同的输出。</p></blockquote><p>保持复制日志的一致性是共识算法的工作。 服务器上的共识模块从客户端接收命令并将它们添加到其日志中。它与其他服务器上的共识模块通信，以确保即使某些服务器发生故障，每个日志最终仍将以相同顺序包含相同的请求。一旦正确复制了命令，就称它们已提交。每个服务器的状态机均以日志顺序处理已提交的命令，并将输出返回给客户端。因此，服务器似乎形成了一个单独的，高度可靠的状态机。</p><p>实际系统的共识算法通常具有以下属性：</p><ul><li>它们可确保在所有非拜占庭条件下的安全性（绝不会返回错误的结果），包括网络延迟，分区，数据包丢失，重复和重新排序。</li><li>只要任何大多数服务器都可以运行并且可以相互通信以及与客户端进行通信，它们就可以正常运行（可用）。因此，由五个服务器组成的典型集群可以容忍任何两个服务器的故障。假定服务器因停止而发生故障；它们稍后可能会从稳定存储上的状态中恢复并重新加入集群。</li><li>它们不依赖于时间来确保日志的一致性：错误的时钟和极端的消息延迟，在最坏的情况下，只可能会导致可用性问题。也就是说，它们可以在消息和处理器都以任意速度运行的异步模型下保持安全性。</li><li>在通常情况下，只要集群中的大多数服务器都响应了一次远程过程调用，命令就可以完成。少数运行缓慢的服务器不影响整体系统性能。</li></ul><h3 id="2-2-复制状态机的常见用例"><a href="#2-2-复制状态机的常见用例" class="headerlink" title="2.2 复制状态机的常见用例"></a>2.2 复制状态机的常见用例</h3><p>复制状态机是使系统具有容错能力的通用构建块。它们可以以多种方式使用，本节讨论一些典型的使用模式。</p><p>大多数常见的共识部署都只用三到五台服务器形成一个复制状态机。然后其他服务器可以使用此状态机来协调其活动，如图 2.2（a）所示。这些系统通常使用复制的状态机来提供成员身份，配置管理或锁。作为一个更具体的示例，复制状态机可以提供容错工作队列，其他服务器可以使用复制状态机进行协调，以将工作分配给自己。</p><blockquote><p><img src="/raft-thesis-translate/2_2.png" srcset="/img/loading.gif" alt><br>图 2.2：使用单个复制状态机的常见模式。</p></blockquote><p>图 2.2（b）中显示了对此用法的常见简化。在这种模式下，一台服务器充当领导者，管理其余服务器。领导者将其关键数据存储在共识系统中。万一发生故障，其他备用服务器将争夺领导者的位置，如果成功，它们将使用共识系统中的数据继续操作。许多具有单个集群领导者的大型存储系统，例如 GFS，HDFS 和 RAMCloud，都使用这种方法。</p><p>有时也使用共识来复制大量数据，如图 2.3 所示。大型存储系统（例如 Megastore，Spanner 和 Scatter）存储的数据太多，无法容纳在一组服务器中。他们跨许多复制状态机对数据进行分区，并且对跨越多个分区的操作使用两阶段提交协议（2PC）来保持一致性。</p><blockquote><p><img src="/raft-thesis-translate/2_3.png" srcset="/img/loading.gif" alt><br>图 2.3：使用共识的大型分区存储系统。为了扩展，数据在许多复制状态机之间进行了分区。跨越分区的操作使用两阶段提交协议。</p></blockquote><h3 id="2-3-Paxos-怎么了"><a href="#2-3-Paxos-怎么了" class="headerlink" title="2.3 Paxos 怎么了"></a>2.3 Paxos 怎么了</h3><p>在过去的 10 年里，Leslie Lamport 的 Paxos 算法几乎已经成为一致性的代名词：Paxos 是在课程教学中最经常使用的算法，同时也是大多数一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，比如单条的复制日志项。我们把这一子集叫做单决策 Paxos。然后通过组合多个 Paxos 协议的实例来促进一系列决策的达成。图 2.4 总结了单决策 Paxos，图 A.5 总结了多决策 Paxos。Paxos 保证安全性和活性（假设使用了足够的故障检测器以避免提议者发生活锁，它最终会达成共识），并且其正确性得到了证明。在正常情况下，多决策 Paxos 是有效的，并且 Paxos 支持集群成员更改。</p><p>不幸的是，Paxos 有两个明显的缺点。第一个缺点是 Paxos 算法特别的难以理解。完整的解释是出了名的不透明；通过极大的努力之后，也只有少数人成功理解了这个算法。因此，有了几次用更简单的术语来解释 Paxos 的尝试。尽管这些解释都只关注了单决策的子集问题，但依然很具有挑战性。在 2012 年 NSDI 的会议中的一次调查显示，很少有人对 Paxos 算法感到满意，甚至在经验老道的研究者中也是如此。我们自己也尝试去理解 Paxos；我们一直没能理解 Paxos 直到我们读了很多对 Paxos 的简化解释并且设计了我们自己的算法之后，这一过程花了近一年时间。</p><blockquote><p><img src="/raft-thesis-translate/2_4.png" srcset="/img/loading.gif" alt><br>图 2.4：单决策 Paxos 共识协议摘要。详细说明请参见后文。</p></blockquote><p>我们假设 Paxos 的不透明性来自它选择单决策问题作为它的基础。单决策 Paxos 是晦涩微妙的，它被划分成了两种没有简单直观解释和无法独立理解的情景。因此，这导致了很难建立起直观的感受为什么单决策 Paxos 算法能够工作。构成多决策 Paxos 增加了很多错综复杂的规则。我们相信，在多决策上达成一致性的问题（一份日志而不是单一的日志记录）能够被分解成其他的方式并且更加直接和明显。</p><p>Paxos算法的第二个问题就是它没有提供一个足够好的用来构建一个现实系统的基础。一个原因是还没有一种被广泛认同的多决策问题的算法。Lamport 的描述基本上都是关于单决策 Paxos 的；他简要描述了实施多决策 Paxos 的方法，但是缺乏很多细节。当然也有很多具体化 Paxos 的尝试，但是他们都互相不一样，和 Paxos 的概述也不同。例如 Chubby 这样的系统实现了一个类似于 Paxos 的算法，但是大多数的细节并没有被公开。</p><p>而且，Paxos 算法的结构也不是十分易于构建实践的系统；单决策分解也会产生其他的结果。例如，独立的选择一组日志条目然后合并成一个序列化的日志并没有带来太多的好处，仅仅增加了不少复杂性。围绕着日志来设计一个系统是更加简单高效的；新日志条目以严格限制的顺序增添到日志中去。另一个问题是，Paxos 使用了一种对等的点对点的方式作为它的核心（尽管它最终提议了一种弱领导者的方法来优化性能）。在只有一个决策会被制定的简化世界中是很有意义的，但是很少有现实的系统使用这种方式。如果有一系列的决策需要被制定，首先选择一个领导者，然后让他去协调所有的决议，会更加简单快速。（第 11 章讨论了 Egalitarian Paxos，这是 Paxos 的最新变体，它不使用领导者，但在某些情况下可能比使用领导者的算法更有效；但是，此算法比基于领导者的算法要复杂得多。）</p><p>因此，实际的系统中很少有和 Paxos 相似的实践。每一种实现都是从 Paxos 开始研究，然后发现很多实现上的难题，再然后开发了一种和 Paxos 明显不一样的结构。这样是非常费时和容易出错的，并且理解 Paxos 的难度使得这个问题更加糟糕。Paxos 算法在理论上被证明是正确可行的，但是现实的系统和 Paxos 差别是如此的大，以至于这些证明没有什么太大的价值。下面来自 Chubby 实现非常典型：</p><blockquote><p>在 Paxos 算法描述和实现现实系统中间有着巨大的鸿沟。最终的系统建立在一种没有经过证明的算法之上。</p></blockquote><p>由于以上问题，我们认为 Paxos 算法既没有提供一个良好的基础给实践的系统，也没有给教学很好的帮助。基于一致性问题在大规模软件系统中的重要性，我们决定看看我们是否可以设计一个拥有更好特性的替代 Paxos 的一致性算法。Raft算法就是这次实验的结果。</p><h2 id="3-Raft-算法基础"><a href="#3-Raft-算法基础" class="headerlink" title="3 Raft 算法基础"></a>3 Raft 算法基础</h2><p>本章介绍了 Raft 算法。 我们将 Raft 设计尽可能容易理解。 第一节描述了我们为可理解性而设计的方法。以下各节描述了算法本身，并包括为便于理解而做出的设计选择示例。</p><h3 id="3-1-为可理解性设计"><a href="#3-1-为可理解性设计" class="headerlink" title="3.1 为可理解性设计"></a>3.1 为可理解性设计</h3><p>在设计 Raft 时，我们有几个目标：它要能为系统构建提供完整可实施的方案，以便显著减少开发者的设计工作量；它要在各种条件下保证安全性，在典型的操作下保证可用性；并且要对常规操作是有效的。但我们最重要的目标（也是最困难的挑战）就是可理解性，要尽可能的保证让大部分读者理解这个算法是舒服的。此外，要尽可能的使得开发这个算法直观，以便系统构建者可以在现实世界中进行不可避免的扩展。</p><p>在 Raft 的设计中有很多要点，我们必须在其他方法中做选择。这种情况下我们通过可理解性对可供选择的方案作出评估：解释每个可供选择的方法有多难（例如它的状态空间有多复杂，是不是有精妙的实现），并且对于读者来说完整理解方法并实现容易吗？</p><p>我们承认这样的分析具有很高的主观性；尽管如此，我们使用了两种通常适用的技术。第一种技术是众所周知的问题分解的方法：我们尽可能的分解问题为分离的，可解决的，可解释的，容易理解的独立部分。例如，在 Raft 中，我们分离了领导者选举、日志复制和安全性。</p><p>我们的第二种方法是通过减少要考虑的状态数量来简化状态空间，使得系统更加协调一致，并尽可能消除不确定性。特别地，日志不允许有空洞，Raft 限制了节点间会导致日志不一致的可能。尽管在大多数情况下，我们试图消除不确定性，但在某些情况下，不确定性实际上提高了可理解性。特别地，随机方法引入了不确定性，但是它们倾向于通过以相似的方式处理所有可能的选择来缩小状态空间（“随便选择；这无关紧要”）。我们使用随机的方式去简化 Raft 领导者选举算法。</p><h3 id="3-2-Raft-概述"><a href="#3-2-Raft-概述" class="headerlink" title="3.2 Raft 概述"></a>3.2 Raft 概述</h3><p>Raft 是一种用于管理第 2.1 节中所述形式的复制日志的算法。图 3.1 概要地总结了算法以供参考，图 3.2 列举了该算法的关键属性；在本章的剩余部分将对这些图形中的元素进行分段讨论。</p><blockquote><p><img src="/raft-thesis-translate/3_1.png" srcset="/img/loading.gif" alt><br>图 3.1：Raft 共识算法的精简摘要（不包括成员资格更改，日志压缩和客户端交互）。 右下角框中的服务器行为被描述为一组独立且重复触发的规则。 诸如 3.4 之类的节号指出了讨论特殊功能的地方。附录 B 中的正式规范更精确地描述了该算法。</p><p><img src="/raft-thesis-translate/3_2.png" srcset="/img/loading.gif" alt><br>图 3.2：Raft 保证所有这些属性在任何时候都是正确的。节号指示了每个属性的讨论位置。</p></blockquote><p>Raft通过首先选举一个领导者，然后让领导者完全负责管理复制的日志来实现一致性。领导者从客户端接收日志条目，再把其复制到其他服务器上，并告诉服务器何时可以安全地将日志条目应用于其状态机。拥有领导者大大简化了复制日志的管理。例如，领导者可以决定新的日志条目需要放在日志中的什么位置而不需要和其他服务器商议，并且数据都以简单的方式从领导者流向其他服务器。领导者可能会失败或与其他服务器断开连接，在这种情况下，将选举出新的领导者。</p><p>通过领导者的方式，Raft 将一致性问题分解成了三个相对独立的子问题，以下各小节对此进行了讨论：</p><ul><li>领导者选举：启动集群时以及现有领导者失败时必须选出新的领导者（第 3.4 节）。</li><li>日志复制：领导者必须接受来自客户端的日志条目，并在整个集群中复制它们，迫使其他日志与其自己的日志一致（第 3.5 节）。</li><li>安全性：Raft 的安全属性的关键是图 3.2 中的状态机安全性属性：如果任何服务器已将特定的日志条目应用于其状态机，则其他服务器都不可以对同一日志索引应用不同的命令。第 3.6 节介绍了 Raft 如何确保此属性；这个解决方案涉及对第 3.4 节中所述的选举机制的额外限制。</li></ul><p>在介绍了共识算法之后，本章讨论可用性问题和定时在系统中的作用（第 3.9 节），以及在服务器之间转移领导权的可选扩展（第 3.10 节）。</p><h3 id="3-3-Raft-基础"><a href="#3-3-Raft-基础" class="headerlink" title="3.3 Raft 基础"></a>3.3 Raft 基础</h3><p>一个 Raft 集群包含若干个服务器节点；通常是 5 个，这允许整个系统容忍 2 个节点的失效。在任何时刻，每一个服务器节点都处于这三个状态之一：领导者、跟随者或者候选人。在通常情况下，系统中只有一个领导者并且其他的节点全部都是跟随者。跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导者或者候选人的请求。领导者处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导者）。第三种状态，候选人，是用来在第 3.4 节描述的选举新领导者时使用。图 3.3 展示了这些状态和他们之间的转换关系；这些转换关系会在接下来进行讨论。</p><blockquote><p><img src="/raft-thesis-translate/3_3.png" srcset="/img/loading.gif" alt><br>图 3.3：服务器状态。跟随者只响应来自其他服务器的请求。如果跟随者接收不到消息，那么他就会变成候选人并发起一次选举。获得集群中大多数选票的候选人将成为领导者。在一个任期内，领导者一直都会是领导者直到自己宕机了。</p></blockquote><p>Raft 把时间分割成任意长度的任期，如图 3.4。任期用连续的整数标记。每一段任期从一次选举开始，就像第 3.4 节描述的一样，一个或者多个候选人尝试成为领导者。如果一个候选人赢得选举，然后他就在接下来的任期内充当领导者的职责。在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有领导者结束；一个新的任期（和一次新的选举）会很快重新开始。Raft 保证了在一个给定的任期内，最多只有一个领导者。</p><blockquote><p><img src="/raft-thesis-translate/3_4.png" srcset="/img/loading.gif" alt><br>图 3.4：时间被划分成一个个的任期，每个任期开始都是一次选举。在选举成功后，领导者会管理整个集群直到任期结束。有时候选举会失败，那么这个任期就会没有领导者而结束。任期之间的切换可以在不同的时间不同的服务器上观察到。</p></blockquote><p>不同的服务器节点可能多次观察到任期之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个任期全程。任期在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如陈旧的领导者。每一个节点存储一个当前任期号，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换当前任期号；如果一个服务器的当前任期号比其他人小，那么他会更新自己的编号到较大的编号值。如果一个候选人或者领导者发现自己的任期号过期了，那么他会立即恢复成跟随者状态。如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求。</p><p>Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。请求投票（RequestVote） RPCs 由候选人在选举期间发起（第 3.4 节），然后附加条目（AppendEntries）RPCs 由领导者发起，用来复制日志和提供一种心跳机制（第 3.5 节）。领导者禅让（第 3.10 节）和后续章节中介绍的机制在核心共识算法中引入了除这两种之外的其他 RPCs。</p><p>我们选择结构化的 RPCs 交流以简化服务间的沟通。每个请求类型都有一个对应的的响应类型。Raft 假定 RPC 的请求和响应可能会在网络中丢失；如果未及时收到响应，则请求者有责任重试 RPC。服务器并行发出 RPCs 以获得最佳性能，并且 Raft 不会假定网络保留了 RPCs 之间的顺序。</p><h3 id="3-4-领导者选举"><a href="#3-4-领导者选举" class="headerlink" title="3.4 领导者选举"></a>3.4 领导者选举</h3><p>Raft 使用一种心跳机制来触发领导者选举。当服务器程序启动时，他们都是跟随者身份。一个服务器节点继续保持着跟随者状态只要他从领导者或者候选人处接收到有效的 RPCs。领导者周期性的向所有跟随者发送心跳包（即不包含日志项内容的附加日志项 RPCs）来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是选举超时，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。</p><p>要开始一次选举过程，跟随者先要增加自己的当前任期号并且转换到候选人状态。然后他会并行的向集群中的其他服务器节点发送请求投票的 RPCs 来给自己投票。候选人会继续保持着当前状态直到以下三件事情之一发生：(a) 他自己赢得了这次的选举，(b) 其他的服务器成为领导者，(c) 一段时间之后没有任何一个获胜的人。这些结果会分别的在下面的段落里进行讨论。</p><p>当一个候选人从整个集群的大多数服务器节点获得了针对同一个任期号的选票，那么他就赢得了这次选举并成为领导者。每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则（注意：第 3.6 节在投票上增加了一点额外的限制）。要求大多数选票的规则确保了最多只会有一个候选人赢得此次选举（图 3.2 中的选举安全性）。一旦候选人赢得选举，他就立即成为领导者。然后他会向其他的服务器发送心跳消息来建立自己的权威并且阻止新的领导者的产生。</p><p>在等待投票的时候，候选人可能会从其他的服务器接收到声明它是领导者的附加日志项 RPC。如果这个领导者的任期号（包含在此次的 RPC中）不小于候选人当前的任期号，那么候选人会承认领导者合法并回到跟随者状态。 如果此次 RPC 中的任期号比自己小，那么候选人就会拒绝这次的 RPC 并且继续保持候选人状态。</p><p>第三种可能的结果是候选人既没有赢得选举也没有输：如果有多个跟随者同时成为候选人，那么选票可能会被瓜分以至于没有候选人可以赢得大多数人的支持。当这种情况发生的时候，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分。</p><p>Raft 算法使用随机选举超时时间的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。为了阻止选票起初就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后他赢得选举并在其他服务器超时之前发送心跳包。同样的机制被用在选票瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。第 9 节展示了这种方案能够快速的选出一个领导者。</p><p>领导者选举这个例子，体现了可理解性原则是如何指导我们进行方案设计的。起初我们计划使用一种排名系统：每一个候选人都被赋予一个唯一的排名，供候选人之间竞争时进行选择。如果一个候选人发现另一个候选人拥有更高的排名，那么他就会回到跟随者状态，这样高排名的候选人能够更加容易的赢得下一次选举。但是我们发现这种方法在可用性方面会有一点问题（如果高排名的服务器宕机了，那么低排名的服务器可能会超时并再次进入候选人状态。而且如果这个行为发生得足够快，则可能会导致整个选举过程都被重置掉）。我们针对算法进行了多次调整，但是每次调整之后都会有新的问题。最终我们认为随机重试的方法是更加明显和易于理解的。</p><h3 id="3-5-日志复制"><a href="#3-5-日志复制" class="headerlink" title="3.5 日志复制"></a>3.5 日志复制</h3><p>一旦一个领导者被选举出来，他就开始为客户端提供服务。客户端的每一个请求都包含一条被复制状态机执行的指令。领导者把这条指令作为一条新的日志条目附加到日志中去，然后并行的发起附加条目 RPCs 给其他的服务器，让他们复制这条日志条目。当这条日志条目被安全的复制（下面会介绍），领导者会应用这条日志条目到它的状态机中然后把执行的结果返回给客户端。如果跟随者崩溃或者运行缓慢，再或者网络丢包，领导者会不断的重复尝试附加日志条目 RPCs （尽管已经回复了客户端）直到所有的跟随者都最终存储了所有的日志条目。</p><p>日志以图 3.5 展示的方式组织。每一个日志条目存储一条状态机指令和从领导者收到这条指令时的任期号。日志中的任期号用来检查是否出现不一致的情况，同时也用来保证图 3.2 中的某些性质。每一条日志条目同时也都有一个整数索引值来表明它在日志中的位置。</p><blockquote><p><img src="/raft-thesis-translate/3_5.png" srcset="/img/loading.gif" alt><br>图 3.5：日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字），和一个状态机需要执行的指令。一个条目当可以安全的被应用到状态机中去的时候，就认为是可以提交了。</p></blockquote><p>领导者来决定什么时候把日志条目应用到状态机中是安全的；这种日志条目被称为已提交。Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。在领导者将创建的日志条目复制到大多数的服务器上的时候，日志条目就会被提交（例如在图 3.5 中的条目 7）。同时，领导者的日志中之前的所有日志条目也都会被提交，包括由其他领导者创建的条目。第 3.6 节会讨论某些当在领导者改变之后应用这条规则的隐晦内容，同时他也展示了这种提交的定义是安全的。领导者跟踪了最大的将会被提交的日志项的索引，并且索引值会被包含在未来的所有附加日志 RPCs （包括心跳包），这样其他的服务器才能最终知道领导者的提交位置。一旦跟随者知道一条日志条目已经被提交，那么他也会将这个日志条目应用到本地的状态机中（按照日志的顺序）。</p><p>我们设计了 Raft 的日志机制来维护一个不同服务器的日志之间的高层次的一致性。这么做不仅简化了系统的行为也使得更加可预计，同时他也是安全性保证的一个重要组件。Raft 维护着以下的特性，这些同时也组成了图 3.2 中的日志匹配特性：</p><ul><li>如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。</li><li>如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。</li></ul><p>第一个特性来自这样的一个事实，领导者最多在一个任期里在指定的一个日志索引位置创建一条日志条目，同时日志条目在日志中的位置也从来不会改变。第二个特性由附加日志 RPC 的一个简单的一致性检查所保证。在发送附加日志 RPC 的时候，领导者会把新的日志条目紧接着之前的条目的索引位置和任期号包含在里面。如果跟随者在它的日志中找不到包含相同索引位置和任期号的条目，那么他就会拒绝接收新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足日志匹配特性的，然后一致性检查保护了日志匹配特性当日志扩展的时候。因此，每当附加日志 RPC 返回成功时，领导者就知道跟随者的日志一定是和自己相同的了。</p><p>在正常的操作中，领导者和跟随者的日志保持一致性，所以附加日志 RPC 的一致性检查从来不会失败。然而，领导者崩溃的情况会使得日志处于不一致的状态（老的领导者可能还没有完全复制所有的日志条目）。这种不一致问题会在领导者和跟随者的一系列崩溃下加剧。图 3.6 展示了跟随者的日志可能和新的领导者不同的方式。跟随者可能会丢失一些在新的领导者中有的日志条目，他也可能拥有一些领导者没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。</p><blockquote><p><img src="/raft-thesis-translate/3_6.png" srcset="/img/loading.gif" alt><br>图 3.6：当一个领导者成功当选时，跟随者可能是任何情况（a-f）。每一个盒子表示是一个日志条目；里面的数字表示任期号。跟随者可能会缺少一些日志条目（a-b），可能会有一些未被提交的日志条目（c-d），或者两种情况都存在（e-f）。例如，场景 f 可能会这样发生，某服务器在任期 2 的时候是领导者，已附加了一些日志条目到自己的日志中，但在提交之前就崩溃了；很快这个机器就被重启了，在任期 3 重新被选为领导者，并且又增加了一些日志条目到自己的日志中；在任期 2 和任期 3 的日志被提交之前，这个服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。</p></blockquote><p>在 Raft 算法中，领导者处理不一致是通过强制跟随者直接复制自己的日志来解决了。这意味着在跟随者中的冲突的日志条目会被领导者的日志覆盖。第 3.6 节会阐述如何通过增加一些限制来使得这样的操作是安全的。</p><p>要使得跟随者的日志进入和自己一致的状态，领导者必须找到最后两者达成一致的地方，然后删除从那个点之后的所有日志条目，发送自己的日志给跟随者。所有的这些操作都在进行附加日志 RPCs 的一致性检查时完成。领导者针对每一个跟随者维护了一个 nextIndex，这表示下一个需要发送给跟随者的日志条目的索引地址。当一个领导者刚获得权力的时候，他初始化所有的 nextIndex 值为自己的最后一条日志的 index 加 1（图 3.6 中的 11）。如果一个跟随者的日志和领导者不一致，那么在下一次的附加日志 RPC 时的一致性检查就会失败。在被跟随者拒绝之后，领导者就会减小 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得领导者和跟随者的日志达成一致。当这种情况发生，附加日志 RPC 就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导者的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导者保持一致，并且在接下来的任期里一直继续保持。</p><p>在领导者发现它与跟随者的日志匹配位置之前，领导者可以发送不带任何条目（例如心跳）的附加日志 RPCs 以节省带宽。 然后，一旦 matchIndex 恰好比 nextIndex 小 1，则领导者应开始发送实际条目。</p><p>如果需要的话，算法可以通过减少被拒绝的附加日志 RPCs 的次数来优化。例如，当附加日志 RPC 的请求被拒绝的时候，跟随者可以包含冲突的条目的任期号和自己存储的那个任期的最早的索引地址。借助这些信息，领导者可以减小 nextIndex 越过所有那个任期冲突的所有日志条目；这样就变成每个任期需要一次附加条目 RPC 而不是每个条目一次。在实践中，我们十分怀疑这种优化是否是必要的，因为失败是很少发生的并且也不大可能会有这么多不一致的日志。</p><p>通过这种机制，领导者在获得权力的时候就不需要任何特殊的操作来恢复一致性。他只需要进行正常的操作，然后日志就能自动的在回复附加日志 RPC 的一致性检查失败的时候自动趋于一致。领导者从来不会覆盖或者删除自己的日志（图 3.2 的领导者只附加特性）。</p><p>日志复制机制展示出了第 2.1 节中形容的一致性特性：Raft 能够接受，复制并应用新的日志条目只要大部分的机器是工作的；在通常的情况下，新的日志条目可以在一次 RPC 中被复制给集群中的大多数机器；并且单个的缓慢的跟随者不会影响整体的性能。由于附加日志 RPCs 请求的大小是可管理的（领导者无需在单个附加日志 RPC 请求中发送多个条目来赶进度），所以日志复制算法也是容易实现的。一些其他的一致性算法的描述中需要通过网络发送整个日志，这对开发者增加了优化此点的负担。</p><h3 id="3-6-安全性"><a href="#3-6-安全性" class="headerlink" title="3.6 安全性"></a>3.6 安全性</h3><p>前面的章节里描述了 Raft 算法是如何选举和复制日志的。然而，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个跟随者可能会进入不可用状态同时领导者已经提交了若干的日志条目，然后这个跟随者可能会被选举为领导者并且覆盖这些日志条目；因此，不同的状态机可能会执行不同的指令序列。</p><p>这一节通过在领导选举的时候增加一些限制来完善 Raft 算法。这一限制保证了任何的领导者对于给定的任期号，都拥有了之前任期的所有被提交的日志条目（图 3.2 中的领导者完整特性）。增加这一选举时的限制，我们对于提交时的规则也更加清晰。最终，我们将展示对于领导者完整特性的简要证明，并且说明领导者完整性特性是如何引导复制状态机做出正确行为的。</p><h4 id="3-6-1-选举限制"><a href="#3-6-1-选举限制" class="headerlink" title="3.6.1 选举限制"></a>3.6.1 选举限制</h4><p>在任何基于领导者的一致性算法中，领导者都必须存储所有已经提交的日志条目。在某些一致性算法中，例如 Viewstamped Replication，某个节点即使是一开始并没有包含所有已经提交的日志条目，它也能被选为领导者。这些算法都包含一些额外的机制来识别丢失的日志条目并把他们传送给新的领导者，要么是在选举阶段要么在之后很快进行。不幸的是，这种方法会导致相当大的额外的机制和复杂性。Raft 使用了一种更加简单的方法，它可以保证所有之前的任期号中已经提交的日志条目在选举的时候都会出现在新的领导者中，不需要传送这些日志条目给领导者。这意味着日志条目的传送是单向的，只从领导者传给跟随者，并且领导者从不会覆盖自身本地日志中已经存在的条目。</p><p>Raft 使用投票的方式来阻止一个候选人赢得选举除非这个候选人包含了所有已经提交的日志条目。候选人为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的日志条目在这些服务器节点中肯定存在于至少一个节点上。如果候选人的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么他一定持有了所有已经提交的日志条目。请求投票 RPC 实现了这样的限制： RPC 中包含了候选人的日志信息，然后投票人会拒绝掉那些日志没有自己新的投票请求。</p><p>Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。</p><h4 id="3-6-2-提交之前任期内的日志条目"><a href="#3-6-2-提交之前任期内的日志条目" class="headerlink" title="3.6.2 提交之前任期内的日志条目"></a>3.6.2 提交之前任期内的日志条目</h4><p>如同 3.5 节介绍的那样，领导者知道一条当前任期内的日志记录是可以被提交的，只要它被存储到了大多数的服务器上。如果一个领导者在提交日志条目之前崩溃了，未来后续的领导者会继续尝试复制这条日志记录。然而，一个领导者不能断定一个之前任期里的日志条目被保存到大多数服务器上的时候就一定已经提交了。图 3.7 展示了一种情况，一条已经被存储到大多数节点上的老日志条目，也依然有可能会被未来的领导者覆盖掉。</p><blockquote><p><img src="/raft-thesis-translate/3_7.png" srcset="/img/loading.gif" alt><br>图 3.7：如图的时间序列展示了为什么领导者无法决定对老任期号的日志条目进行提交。在 (a) 中，S1 是领导者，部分的复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为 S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。</p></blockquote><p>为了消除图 3.7 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导者当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。在某些情况下，领导者可以安全的知道一个老的日志条目是否已经被提交（例如，该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用一种更加保守的方法。</p><p>当领导者复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号, 这在提交规则上产生了额外的复杂性。在其他的一致性算法中，如果一个新的领导者要重新复制之前的任期里的日志时，它必须使用当前新的任期号。Raft 使用的方法更加容易辨别出日志，因为它可以随着时间和日志的变化对日志维护着同一个任期编号。另外，和其他的算法相比，Raft 中的新领导者只需要发送更少日志条目（其他算法中必须在他们被提交之前发送更多的冗余日志条目来为他们重新编号）。但是，这在实践中可能并不十分重要，因为领导者更换很少</p><h4 id="3-6-3-安全性论证"><a href="#3-6-3-安全性论证" class="headerlink" title="3.6.3 安全性论证"></a>3.6.3 安全性论证</h4><p>在给定了完整的 Raft 算法之后，我们现在可以更加精确的讨论领导者完整性特性（这一讨论基于第 8 章的安全性证明）。我们假设领导者完全性特性是不存在的，然后我们推出矛盾来。假设任期 T 的领导者（领导者 T）在任期内提交了一条日志条目，但是这条日志条目没有被存储到未来某个任期的领导者的日志中。设大于 T 的最小任期 U 的领导者 U 没有这条日志条目：</p><ol><li>在领导者 U 选举的时候一定没有那条被提交的日志条目（领导者从不会删除或者覆盖任何条目）。</li><li>领导者 T 复制这条日志条目给集群中的大多数节点，同时，领导者 U 从集群中的大多数节点赢得了选票。因此，至少有一个节点（投票者、选民）同时接受了来自领导者 T 的日志条目，并且给领导者 U 投票了，如图 3.8。这个投票者是产生这个矛盾的关键。</li><li>这个投票者必须在给领导者 U 投票之前先接受了从领导者 T 发来的已经被提交的日志条目；否则他就会拒绝来自领导者 T 的附加日志请求（因为此时他的任期号会比 T 大）。</li><li>投票者在给领导者 U 投票时依然保存有这条日志条目，因为任何中间的领导者都包含该日志条目（根据上述的假设），领导者从不会删除条目，并且跟随者只有在和领导者冲突的时候才会删除条目。</li><li>投票者把自己选票投给领导者 U 时，领导者 U 的日志必须和投票者自己一样新。这就导致了两者矛盾之一。</li><li>首先，如果投票者和领导者 U 的最后一条日志的任期号相同，那么领导者 U 的日志至少和投票者一样长，所以领导者 U 的日志一定包含所有投票者的日志。这是另一处矛盾，因为投票者包含了那条已经被提交的日志条目，但是在上述的假设里，领导者 U 是不包含的。</li><li>除此之外，领导者 U 的最后一条日志的任期号就必须比投票人大了。此外，他也比 T 大，因为投票人的最后一条日志的任期号至少和 T 一样大（他包含了来自任期 T 的已提交的日志）。创建了领导者 U 最后一条日志的之前领导者一定已经包含了那条被提交的日志（根据上述假设，领导者 U 是第一个不包含该日志条目的领导者）。所以，根据日志匹配特性，领导者 U 一定也包含那条被提交的日志，这里产生矛盾。</li><li>这里完成了矛盾。因此，所有比 T 大的领导者一定包含了所有来自 T 的已经被提交的日志。</li><li>日志匹配原则保证了未来的领导者也同时会包含被间接提交的条目，例如图 3.7 (d) 中的索引 2。</li></ol><blockquote><p><img src="/raft-thesis-translate/3_8.png" srcset="/img/loading.gif" alt><br>图 3.8：如果 S1 （任期 T 的领导者）提交了一条新的日志在它的任期里，然后 S5 在之后的任期 U 里被选举为领导者，然后至少会有一个机器，如 S3，既拥有来自 S1 的日志，也给 S5 投票了。</p></blockquote><p>通过领导者完全特性，我们就能证明图 3.2 中的状态机安全特性，即如果服务器已经在某个给定的索引值应用了日志条目到自己的状态机里，那么其他的服务器不会应用一个不一样的日志到同一个索引值上。在一个服务器应用一条日志条目到他自己的状态机中时，他的日志必须和领导者的日志，在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性保证拥有更高任期号的领导者会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。</p><p>最后，Raft 要求服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。</p><h3 id="3-7-跟随者和候选人崩溃"><a href="#3-7-跟随者和候选人崩溃" class="headerlink" title="3.7 跟随者和候选人崩溃"></a>3.7 跟随者和候选人崩溃</h3><p>到目前为止，我们都只关注了领导者崩溃的情况。跟随者和候选人崩溃后的处理方式比领导者要简单的多，并且他们的处理方式是相同的。如果跟随者或者候选人崩溃了，那么后续发送给他们的 RPCs 都会失败。Raft 中处理这种失败就是简单的通过无限的重试；如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求。Raft 的 RPCs 都是幂等的，所以这样重试不会造成任何问题。例如一个跟随者如果收到附加日志请求但是他已经包含了这一日志，那么他就会直接忽略这个新的请求。</p><h3 id="3-8-持久化状态和服务重启"><a href="#3-8-持久化状态和服务重启" class="headerlink" title="3.8 持久化状态和服务重启"></a>3.8 持久化状态和服务重启</h3><p>Raft 服务器必须持久化足够的信息到稳定存储中来保证服务的安全重启。特别地，每一个服务需要持久化当前的任期和投票选择；这是必要的，以防止服务器在相同的任期内投票两次，或者将新领导者的日志条目替换为废弃领导者的日志条目。每一个服务也要在统计日志提交状态之前持久化该日志，这可以防止已经提交的条目在服务器重启时丢失或“未提交”。</p><p>其他状态变量在重启之后丢失是安全的，因为他们都可以重新创建。最有趣的示例是 commitIndex，可以在重新启动时将其安全地重新初始化为零。即使每个服务器都同时重新启动，commitIndex 也只会暂时滞后于其真实值。选举领导者并能够提交新条目后，其 commitIndex 将增加，并将快速将该 commitIndex 传播给其关注者。</p><p>状态机可以是易失性的，也可以是持久性的。重新启动后，必须通过重新应用日志条目（应用最新快照后；请参阅第 5 章）来恢复易失性状态机。持久状态机在重启后已经应用了大多数条目。为了避免重新应用它们，其最后应用的索引也必须是持久的。</p><p>如果服务器丢失其任何持久状态，则无法使用其先前的身份安全地重新加入集群。通常可以通过调用集群成员更改将服务器以新的身份添加回集群（请参见第 4 章）。但是，如果大多数集群失去其持久状态，则日志条目可能会丢失，并且集群成员更改的进度将无法进行；这就需要系统管理员介入了，不得不面对可能的数据丢失。</p><h3 id="3-9-时间和可用性"><a href="#3-9-时间和可用性" class="headerlink" title="3.9 时间和可用性"></a>3.9 时间和可用性</h3><p>Raft 的要求之一就是安全性不能依赖时间：整个系统不能因为某些事件运行的比预期快一点或者慢一点就产生了错误的结果。但是，可用性（系统可以及时的响应客户端）不可避免的要依赖于时间。例如，如果消息交换比服务器故障间隔时间长，候选人将没有足够长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。</p><p>领导人选举是 Raft 中对时间要求最为关键的方面。Raft 可以选举并维持一个稳定的领导人,只要系统满足下面的时间要求：</p><blockquote><p>广播时间（broadcastTime） &lt;&lt; 选举超时时间（electionTimeout） &lt;&lt; 平均故障间隔时间（MTBF）</p></blockquote><p>在这个不等式中，广播时间指的是从一个服务器并行的发送 RPCs 给集群中的其他服务器并接收响应的平均时间；选举超时时间就是在第 3.4 节中介绍的选举的超时时间限制；然后平均故障间隔时间就是对于一台服务器而言，两次故障之间的平均时间。广播时间必须比选举超时时间小一个量级，这样领导人才能够发送稳定的心跳消息来阻止跟随者开始进入选举状态；通过随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间应该要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定的运行。当领导人崩溃后，整个系统会大约相当于选举超时的时间里不可用；我们希望这种情况在整个系统的运行中很少出现。</p><p>广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPCs 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，取决于存储的技术。因此，选举超时时间可能需要在 10 毫秒到 500 毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的需求。</p><h3 id="3-10-领导权禅让扩展"><a href="#3-10-领导权禅让扩展" class="headerlink" title="3.10 领导权禅让扩展"></a>3.10 领导权禅让扩展</h3><p>本节描述了 Raft 的可选扩展，该扩展允许一台服务器将其领导权禅让给另一台服务器。领导权禅让在两种情况下可能有用：</p><ol><li>有时领导者必须下台。例如，它可能需要重新启动以进行维护，或者可能已从集群中删除（请参阅第 4 章）。当它退出时，集群将在选举超时之前处于空闲状态，直到另一台服务器超时并赢得选举为止。通过使领导者在其下台之前将其领导权禅让给另一台服务器，可以避免这种短暂的不可用性。</li><li>在某些情况下，一台或多台服务器可能比其他服务器更适合领导集群。例如，高负载的服务器将无法成为好的领导者，或者在广域网中，首选主数据中心中的服务器，以最大程度地减少客户端与领导者之间的延迟。其他共识算法可能能够在领导者选举期间适应这些偏好，但是 Raft 需要一台具有足够最新日志的服务器才能成为领导者，这可能不是最受推荐的服务器。相反，Raft 的领导者可以定期检查以查看其可用的跟随者之一是否更合适，如果是，则将其领导权禅让给该服务器。（要是像人类的领导人如此优雅多好啊）。</li></ol><p>为了在 Raft 中禅让领导权，先前的领导者会将其日志条目发送到目标服务器，然后目标服务器不等待选举超市就直接运行选举。因此，先前的领导者可以确保目标服务器在其任期开始时拥有所有已提交的条目，并且像在正常选举中一样，多数投票可以确保维持安全属性（例如“领导者完整性”属性）。以下步骤更详细地描述了该过程：</p><ol><li>当前领导者停止接受新的客户请求。</li><li>当前领导者完整更新目标服务器的日志以使其与自己的日志匹配，使用第 3.5 节中描述的复制机制。</li><li>当前领导者将 TimeoutNow 请求发送到目标服务器。此请求与目标服务器的选举计时器触发具有相同的效果：目标服务器开始新的选举（增加其任期并成为候选人）。</li></ol><p>一旦目标服务器收到了 TimeoutNow 请求，它很有可能先于其他任何服务器开始选举，并成为下一任期的领导者。它给前任领导的下一条消息将包括其新任期号，从而导致前任领导下台。至此，领导权禅让完成。</p><p>目标服务器也有可能发生故障。在这种情况下，集群必须恢复客户端操作。如果在选举超时后领导权禅让仍未完成，则当前领导将中止禅让并恢复接受客户请求。如果当前领导者弄错了并且目标服务器实际上是可用的，那么在最坏的情况下，此错误将导致一次额外的选举，此后客户端操作会恢复。</p><p>此方法通过在 Raft 的正常过渡范围内进行操作来保证安全性。例如 Raft 已经保证了时钟不同步情况下的安全性。当目标服务器收到 TimeoutNow 请求时，它等效于目标服务器的时跳跃了一步，这是安全的。但是，我们目前尚未实施或评估这种领导权禅让方法。</p><h3 id="3-11-结论"><a href="#3-11-结论" class="headerlink" title="3.11 结论"></a>3.11 结论</h3><p>本章讨论了基于一致性系统的所有核心问题。Raft 没有通过像在单决策 Paxos 那样在一个值上达成一致，而是通过增长的操作日志来实现一致性，这是构建复制状态机所必需的。一旦达成协议它就会分发信息，以便其他服务器了解已提交的日志条目。Raft 通过切实有效的方法实现了一致性：选举一个独自做决定的领导者并且通过领导者传送必要的日志信息。我们已经在复制状态机 LogCabin 中实现了 Raft 的想法，一个复制状态机（在第 10 章中进行了介绍）。</p><p>Raft仅使用少量机制来解决完全共识问题。例如，它仅使用两个 RPCs（RequestVote 和 AppendEntries）。也许令人惊讶的是，创建一个紧凑的算法/实现并不是 Raft 的明确目标。相反，这是我们为易于理解而设计的结果，每一个机制必须充分激发积极性和具有可理解性。我们发现冗余曲折的机制很难激发积极性，所以在设计中自然的会被清除。</p><p>除非我们确信某个特定问题会对 Raft 开发造成较大影响，否则我们不会在 Raft 中提及。结果就是 Raft 的某些部分可能看起来很幼稚。例如，Raft 中的服务器通过等待选举超时来检测投票是否被瓜分。原则上，他们通常可以通过计算授予任何候选人的选票来更快地发现甚至解决瓜分选票。我们选择不在 Raft 中对此做优化，因为它会增加复杂性，但可能不会在实践中带来好处：在配置合理的部署中，很少有选票瓜分的情况。Raft 的其他部分可能显得过于保守。例如，领导者仅可以直接从当前任期提交条目，即使在某些特殊情况下它可以安全地提交先前任期的条目。应用一个更复杂的提交规则会有损易理解性并且可能没有显著的性能优化。在与其他人讨论 Raft 时，我们发现许多人不禁会想到并提出这样的优化，但是当目标是可理解性时，就应该放弃过早的优化。</p><p>不可避免地，本章可能遗漏了一些在实践中很有用的功能或优化。随着开发者在 Raft 上获得更多的经验，他们将了解何时以及为什么某些附加功能可能有用，并且他们可能需要在某些实际部署中实施这些功能。在本章中，我们概述了一些可选扩展，我们目前认为这些扩展是不必要的，但在需要时可以帮助指导开发者。通过关注可理解性，我们希望为开发者根据他们的经验调整 Raft 提供坚实的基础。由于 Raft 在我们的测试环境中工作，我们希望这些是直截了当的扩展而非基础的改变。</p><h2 id="4-集群成员更改"><a href="#4-集群成员更改" class="headerlink" title="4 集群成员更改"></a>4 集群成员更改</h2><p>到目前为止，我们一直假设集群配置（参与共识算法的服务器集合）是固定的。实际上，在生产环境中偶尔需要更改配置，例如在服务器出现故障替换服务器时或者更改副本数时。这时可以使用以下两种方法手动完成：</p><ul><li>可以通过使整个集群下线，更新配置文件，然后重启集群来更改配置。但是这将导致整个集群在配置更改期间不可用。</li><li>或者，新的服务器可以通过获取某个集群成员的网络地址来替换它。<br>然而，管理员必须保证被替换的服务器永远不会恢复，否则系统将失去其安全属性（例如，将有一个额外的投票）。</li></ul><p>这两种成员更改的方法都有显著的缺点，而且如果有任何手动步骤都会有操作错误的风险。</p><p>为了避免这些问题，我们决定将配置更改自动化，并将它们合并 Raft 共识算法中。Raft 允许集群在配置更改期间继续正常运行，并且只需对基本共识算法进行少量扩展即可实现成员更改。图 4.1 总结了用于集群成员更改的 RPC，本章其余部分将描述其元素。</p><blockquote><p><img src="/raft-thesis-translate/4_1.png" srcset="/img/loading.gif" alt><br>图 4.1：用于更改集群成员的 RPC。AddServer RPC 用于向当前配置添加新服务器，RemoveServer RPC 用于从当前配置中删除服务器。诸如 4.1 之类的节号指出了讨论特殊功能的地方。第 4.4 节讨论了在完整系统中使用这些 RPC 的方法。</p></blockquote><h3 id="4-1-安全"><a href="#4-1-安全" class="headerlink" title="4.1 安全"></a>4.1 安全</h3><p>保障安全是配置更改的首要挑战。为了确保该机制的安全，在过渡期间不能出现两位领导者同时当选的情况。如果单个配置更改添加或删除了许多服务器，则将集群从旧配置直接切换到新配置可能是不安全的；一次自动切换所有服务器是不可能的，因此集群可能在转换期间分裂成两个独立的主体（见图 4.2）。</p><blockquote><p><img src="/raft-thesis-translate/4_2.png" srcset="/img/loading.gif" alt><br>图 4.2：直接从一个配置切换到另一个配置是不安全的，因为不同的服务器将在不同的时间进行切换。在本例中，集群从 3 个服务器增长到 5 个服务器。不幸的是，在某个时间点，两个不同的领导者可以在同一个任期内被选举出来，一个拥有旧配置（C<sub>old</sub>）的多数，另一个拥有新配置（C<sub>new</sub>）的多数。</p></blockquote><p>大多数成员更改算法都引入了其他机制来处理这种问题。这是我们最初为 Raft 所做的，但后来我们发现了一个更简单的方法，即禁止会导致多数成员不相交的成员更改。因此，Raft 限制了允许的更改类型：一次只能从集群中添加或删除一个服务器。成员更改中更复杂的更改是通过一系列单服务器更改实现的。本章的大部分内容描述了单服务器方法，它比我们原来的方法更容易理解。为了完整起见，第 4.3 节描述了原始的方法，它增加了处理任意配置更改的复杂性。在发现更简单的单服务器更改方法之前，我们在 LogCabin 中实现了更复杂的方法；在撰写本文时，它仍然使用更复杂的方法。</p><p>当向集群中添加或删除单个服务器单个服务器时，旧集群的任何多数与新集群的任何多数重叠，参见图 4.3。这种重叠阻止了集群分裂成两个独立的多数派；在第 3.6.3 节的安全论证中，它保证了“投票人”的存在。因此，当只添加或删除一个服务器时，可以安全地直接切换到新配置。Raft 利用此属性，几乎不使用其他机制即可安全地更改集群成员。</p><blockquote><p><img src="/raft-thesis-translate/4_3.png" srcset="/img/loading.gif" alt><br>图 4.3：从偶数和奇数大小的集群中添加和删除单个服务器。在每个图中，蓝色矩形显示旧集群的大部分，红色矩形显示新集群的大部分。在每个单服务器成员更改中，旧集群的任何多数与新集群的任何多数之间都会有重叠，这用来保证安全性。例如在（b）中，旧集群的大部分必须包括其余 3 个服务器中的 2 个，而新集群的大部分必须包括新集群中的 3 个服务器，其中至少有 2 个来自旧集群。</p></blockquote><p>集群配置使用复制日志中的特殊条目进行存储和通信。这利用了 Raft 的现有机制来复制和持久化配置信息。通过对配置更改和客户机请求进行排序（允许两者在管道或批处理被同时复制），它还允许集群在进行配置更改时继续为客户机请求提供服务。</p><p>当领导者收到从当前配置（C<sub>old</sub>）中添加或删除服务器的请求时，它将新配置（C<sub>new</sub>）作为一个条目添加到其日志中，并使用常规的 Raft 机制复制该条目。新配置一旦添加到服务器的日志中，就会在这个服务器上生效：C<sub>new</sub> 条目被复制到 C<sub>new</sub> 指定的服务器上，而大部分服务器的新配置生效被用于确定 C<sub>new</sub> 条目的提交。这意味着服务器不会等待配置条目被提交，并且每个服务器总是使用在其日志中找到的最新配置。</p><p>一旦提交了 C<sub>new</sub> 条目，配置更改就完成了。此时，领导者知道大多数的 C<sub>new</sub> 指定的服务器已经采用了 C<sub>new</sub>。它还知道，没有收到 C<sub>new</sub> 条目的任意服务器都不能再构成集群的大多数，没有收到 C<sub>new</sub> 的服务器也不能再当选为领导者。C<sub>new</sub> 的提交让三件事得以继续:</p><ol><li>领导可以确认配置更改的成功完成。</li><li>如果配置更改删除了服务器，则可以关闭该服务器。</li><li>可以启动进一步的配置更改。在此之前，重叠的配置更改可能会降级为不安全的情况，如图 4.2 所示。</li></ol><p>如上所述，服务器总是在其日志中使用最新的配置，而不管是否提交了配置条目。这使得领导者可以很容易地避免重叠的配置更改（上面的第三项），方法是直到之前的更改的条目提交之后才开始新的更改。只有当旧集群的大多数成员都在 C<sub>new</sub> 的规则下运行时，才可以安全地开始另一次成员更改。如果服务器只在了解到 C<sub>new</sub> 已提交时才采用 C<sub>new</sub>，那么 Raft 的领导者将很难知道何时旧集群的大部分已经采用它。它们需要跟踪哪些服务器知道配置更改条目的提交，服务器也需要将它们的提交索引保存到磁盘；Raft 其实不需要这些机制。相反，Raft 的每台服务器只要发现该条目存在于其日志中，就采用 C<sub>new</sub>，并且知道一旦提交了 C<sub>new</sub> 条目，就可以安全地允许进一步的配置更改。不幸的是，这个决定意味着配置更改的日志条目可以被删除（如果领导者发生变化）；在这种情况下，服务器必须准备好返回到其日志中的先前配置。</p><p>在 Raft 中，用于达成一致意见的是调用者的配置，包括投票和日志复制:</p><ul><li>即使领导者并不在服务器的最新配置中，服务器依然接受它的 AppendEntries 请求。否则将永远不能将新服务器添加到集群中（它永远不会接受添加服务器的配置条目之前的任何日志条目）。</li><li>服务器还允许投票给不属于服务器当前最新配置的候选人（如果候选人有足够的最新日志和当前任期）。为了保持集群可用，可能偶尔需要进行投票表决。例如，考虑将第四个服务器添加到三个服务器的集群中。如果一个服务器出现故障，就需要新服务器的投票来形成多数票并选出一个领导者。</li></ul><p>因此，服务器可以直接处理传入的 RPC 请求，而无需查询其当前配置。</p><h3 id="4-2-可用性"><a href="#4-2-可用性" class="headerlink" title="4.2 可用性"></a>4.2 可用性</h3><p>集群成员更改在保持集群可用性方面引入了多个问题。第 4.2.1 节讨论了在将新服务器添加到集群之前使其追赶，以防止它拖延对新日志条目的提交。第 4.2.2 节介绍了如何从集群中删除现有的领导者。第 4.2.3 节介绍了如何防止已删除的服务器干扰新集群的领导者。最后，第 4.2.4 节以一个论点结束，说明为什么最终的成员更改算法足以在任何成员更改期间保留可用性。</p><h4 id="4-2-1-让新服务器追赶"><a href="#4-2-1-让新服务器追赶" class="headerlink" title="4.2.1 让新服务器追赶"></a>4.2.1 让新服务器追赶</h4><p>当服务器被添加到集群中时，它通常不会存储任何日志条目。如果以这种状态将它添加到集群中，那么它的日志可能需要相当长的时间才能赶上领导者的日志，在此期间，集群更容易出现不可用的情况。例如，一个包含三个服务器的集群通常可以容忍一个故障而不损失可用性。但是，如果向同一集群添加了第四个服务器，其中有一个日志为空，并且原来的三个服务器之一出现故障，则集群将暂时无法提交新条目（参见图 4.4（a））。如果一个集群中连续不断地添加许多新服务器，就会出现另一个可用性问题，此时需要新服务器来组成集群的大部分（参见图 4.4（b））。在这两种情况下，直到新服务器的日志赶上领导者的日志之前，集群都将不可用。</p><blockquote><p><img src="/raft-thesis-translate/4_4.png" srcset="/img/loading.gif" alt><br>图 4.4：添加空日志的服务器如何使可用性面临风险的示例。图中显示了服务器在两个不同集群中的日志。每个集群都有三个服务器，S1-S3。在（a）中，添加了 S4，然后 S3 失败。集群应该能够在一个节点失败后正常运行，但事实上它失去了可用性：它需要四个服务器中的三个来提交一个新条目，但是 S3 失败了，S4 的日志太晚了，无法追加新条目。在（b）中，快速连续地添加了 S4–S6。提交添加 S6（第三个新服务器）的配置条目需要 4 个服务器的日志来存储该条目，但是 S4-S6 的日志远远不够。在新服务器的日志赶上之前，这两个集群都是不可用的。</p></blockquote><p>为了避免可用性缺口，Raft 在配置更改之前引入了一个附加阶段，其中一个新服务器作为无投票权成员加入集群。领导者复制日志条目到它，但出于投票或提交目的，尚未计入多数。一旦新服务器赶上了集群的其余部分，就可以按照上面的描述进行重新配置。（支持无投票权服务器的机制在其他上下文中也很有用；例如，它可以用于将状态复制到大量服务器，这些服务器可以保证最终一致性提供只读请求。）</p><p>领导者需要确定什么时候一个新的服务器已经赶上进度从而可以继续进行配置更改。这需要注意保持可用性：如果服务器添加得太快，集群的可用性可能面临风险，如上所述。我们的目标是将任何暂时的不可用保持在一个选举超时以下，因为客户必须已经能够容忍这种程度的偶发性不可用（在领导者失败的情况下）。此外，如果可能的话，我们希望通过使新服务器的日志更接近领导者的日志来进一步减少不可用性。</p><p>如果新服务器不可用或速度太慢以至于永远无法赶上，则领导者还应中止更改。这项检查很重要：因为没有将这些检查包括在内，Lamport 的古老 Paxos 政府破裂了。他们不小心将成员更改为溺水的水手，无法再取得进步。尝试添加不可用或速度较慢的服务器通常是一个错误。实际上，我们的第一个配置更改请求就包含了网络端口号中的一个输入错误；系统正确地中止了更改并返回了一个错误。</p><p>我们建议使用以下算法来确定何时新服务器能够赶上并能被添加到集群中。将条目复制到新服务器的过程分为几轮，如图 4.5 所示。每一轮开始时都将当前领导者日志中的所有日志条目复制到新服务器的日志中。在为其当前轮复制条目时，新条目可能会到达领导者；它将在下一轮中复制它们。随着进程的进行，轮持续时间在时间上缩短。该算法等待固定的轮数（如 10 轮）。如果最后一轮的持续时间少于一个选举超时，则领导者将新服务器添加到集群中，并假设没有足够的未复制条目来创建显著的可用性缺口。否则，领导者将因错误中止配置更改。调用者可能总会再试一次（下一次成功的可能性更大，因为新服务器的日志已经追赶上一部分）。</p><blockquote><p><img src="/raft-thesis-translate/4_5.png" srcset="/img/loading.gif" alt><br>图 4.5：为了让新服务器追赶，领导者将条目复制到新服务器的过程分为几轮。每一轮结束后，新服务器就会拥有这一轮开始时领导者日志中记录的所有条目。然而，到那时，领导者可能已经收到了新的条目；这些将在下一轮中重复。</p></blockquote><p>作为让新服务器追赶的第一步，领导者必须发现新服务器的日志是空的。对于新服务器，在 AppendEntries 中的一致性检查将不断失败，直到领导者的 nextIndex 最终下降到 1。这种遍历可能是影响在集群中添加新服务器的性能的主要因素（在此阶段之后，可以通过使用批处理将日志条目以更少的 RPC 传输给跟随者）。各种方法可以使 nextIndex 更快地收敛到它的正确值，包括第 3 章中描述的方法。然而，解决添加新服务器这一特殊问题的最简单方法是，使跟随者在 AppendEntries 响应中返回其日志的长度；从而使领导者可以相应地捕捉跟随者的 nextIndex。</p><h4 id="4-2-2-删除当前领导者"><a href="#4-2-2-删除当前领导者" class="headerlink" title="4.2.2 删除当前领导者"></a>4.2.2 删除当前领导者</h4><p>如果要求现有领导者将自己从集群中删除，则它必须在某个时候下台。一种简单的方法是使用第 3 章中介绍的领导权禅让扩展：被要求删除自身的领导者会将其领导权转移到另一台服务器，该服务器随后将正常执行成员更改。</p><p>我们最初为 Raft 开发了一种不同的方法，在该方法中，现有领导者进行成员更改以删除自己，然后下台。这使 Raft 处于有些尴尬的操作模式，即领导者临时管理一个它不是其成员的配置。最初，我们需要这种方法来进行任意配置更改（请参阅第 4.3 节），其中旧的配置和新的配置可能没有任何可以转移领导权的服务器，同样的方法也适用于那些没有实现领导权禅让的系统。</p><p>在这种方法中，一旦提交 C<sub>new</sub> 条目，从配置中删除的领导者将会下台。如果领导者在此之前下台，它可能仍会超时并再次成为领导者，从而延迟了进度。在极端情况下，从两台服务器的集群中删除领导者时，服务器甚至可能必须再次成为领导者，集群才能取得进展；参见图4.6。因此，领导者要等到 C<sub>new</sub> 提交再下台。这是新配置肯定可以在没有被废黜的领导者参与的情况下运行的第一点：C<sub>new</sub> 的成员总是有可能从他们当中选出新的领导者。被删除的领导者卸任后，C<sub>new</sub> 中的服务器将超时并赢得选举。这种较小的可用性缺口一般是可以容忍的，因为在领导者失败时也会出现类似的可用性缺口。</p><blockquote><p><img src="/raft-thesis-translate/4_6.png" srcset="/img/loading.gif" alt><br>图 4.6：在提交 C<sub>new</sub> 条目之前，已删除的服务器可能需要引导集群取得进展。 该图显示了从两个服务器集群中删除 S1。 S1 目前是负责人，S1 还不应该下台；它仍然需要作为领导者。 在 S2 收到 S1 的 C<sub>new</sub> 条目之前，它无法成为领导者（因为 S2 仍然需要 S1 的投票才能构成 C<sub>old</sub> 的多数票，并且 S1 不会向 S2 投票，因为 S2 的日志不是最新的）。</p></blockquote><p>这种方法对决策产生了两种影响，这些影响不是特别有害，但可能令人惊讶。首先，领导者可以在一段时间内（提交 C<sub>new</sub> 时）管理一个不包含自身的集群。它复制日志条目，但不占多数。其次，不属于其自身最新配置的服务器仍应开始新的选举，因为在提交 C<sub>new</sub> 条目之前可能仍需要它（如图 4.6 所示）。除非它是其最新配置的一部分，否则它不会在选举中计入自己的选票。</p><h4 id="4-2-3-破坏性服务器"><a href="#4-2-3-破坏性服务器" class="headerlink" title="4.2.3 破坏性服务器"></a>4.2.3 破坏性服务器</h4><p>如果没有其他机制，不在 C<sub>new</sub> 中的服务器可能会破坏集群。一旦集群领导者创建了 C<sub>new</sub> 条目，不在 C<sub>new</sub> 中的服务器将不再接收心跳信号，因此它将超时并开始新的选举。此外，它不会收到 C<sub>new</sub> 条目，也不会了解该条目已经提交，因此它不会知道它已从集群中删除。服务器将使用新的任期编号发送 RequestVote RPC，这将导致当前的领导者恢复为跟随者状态。虽然终将选举出 C<sub>new</sub> 的一位新领导者，但破坏性服务器将再次超时，并且该过程将重复进行，从而导致可用性降低。如果从集群中删除了多个服务器，情况还可能进一步恶化。</p><p>我们消除干扰的第一个想法是，如果一个服务器要开始选举，它首先要检查它是否浪费了每个人的时间——它有机会赢得选举。这为选举引入了一个新阶段，称为预投票阶段。候选人首先会询问其他服务器其日志是否足够新以获得他们的投票。只有当候选人相信自己能从集群中的大多数人那里获得选票时，他才会增长任期编号并开始正常的选举。</p><p>不幸的是，预投票阶段并没有解决干扰服务器的问题：在某些情况下，干扰服务器的日志已经足够新，但是开始选举仍然会造成干扰。可能令人惊讶的是，这些可能在配置更改完成之前就发生了。例如，图 4.7 显示了从集群中删除的服务器。一旦领导者创建了 C<sub>new</sub> 日志条目，被删除的服务器可能会造成混乱。在这种情况下，预投票检查没有帮助，因为被删除的服务器的日志比大多数集群的日志更新。（虽然预投票阶段并没有解决破坏性服务器的问题，但总体上来说，它确实是提高领导者选举稳健性的一个有用的想法；参见第 9 章）。</p><blockquote><p><img src="/raft-thesis-translate/4_7.png" srcset="/img/loading.gif" alt><br>图 4.7：一个服务器如何在提交 C<sub>new</sub> 日志条目之前造成破坏，并且预投票阶段没有帮助的示例。图中显示了从一个包含四个服务器的集群中删除 S1 的过程。S4 是新集群的领导者，它在日志中创建了 C<sub>new</sub> 条目，但还没有复制该条目。旧集群中的服务器不再接收来自 S4 的心跳。甚至在提交 C<sub>new</sub> 之前，S1 就可以超时，增加它的任期，并将这个较大的任期号发送给新的集群，迫使 S4 下台。预投票算法没有帮助，因为 S1 的日志与大多数集群中的服务器一样是最新的。</p></blockquote><p>基于这种情况，我们现在认为，仅仅基于比较日志（如预投票检查）的解决方案不足以判断选举是否具有破坏性。我们不能要求服务器在开始选举之前检查 C<sub>new</sub> 中每个服务器的日志，因为 Raft 必须始终能够容忍错误。我们也不希望假设领导者能够足够快地可靠地复制条目，从而快速地通过图 4.7 所示的场景；这在实践中可能是可行的，但这取决于更强的假设，即我们更希望避免探索日志分歧的性能和复制日志条目的性能。</p><p>Raft 的解决方案是使用心跳来确定何时存在有效的领导者。在 Raft 中，如果领导者能够保持跟随者的心跳状态，则它被认为是活跃的（否则，另一个服务器将开始选举）。因此，服务器不应该能够干扰正在发送心跳的领导者。我们对 RequestVote RPC 进行了修改，以实现此目的：如果服务器在从当前领导者那里听到的最小选举超时内收到 RequestVote 请求，它不会更新其任期或授予其投票。它可以放弃请求，拒绝投票或延迟请求。结果基本相同。这不会影响正常的选举，在正常的选举中，每个服务器在开始选举之前至少要等待最小选举超时的时间。但是，它有助于避免 C<sub>new</sub> 以外的服务器造成的干扰：尽管领导者能够对其集群发出心跳，但不会因更大任期的节点而被废黜。</p><p>此更改与第 3 章中所述的领导权禅让机制相冲突，在第 3 章中，服务器合法地开始选举而无需等待选举超时。在这种情况下，即使其他服务器认为当前的集群领导者存在，也应该处理 RequestVote 消息。这些 RequestVote 请求可以包含一个特殊标志来指示此行为（“我有权破坏领导者，是它告诉我的！”）。</p><h4 id="4-2-4-可用性参数"><a href="#4-2-4-可用性参数" class="headerlink" title="4.2.4 可用性参数"></a>4.2.4 可用性参数</h4><p>本节认为，上述解决方案足以在成员更改期间保持可用性。由于 Raft 的成员更改是基于领导者的，因此我们证明了该算法将能够在成员更改期间维护和替换领导者，而且领导者将同时为客户请求提供服务并完成配置更改。我们假设大多数旧配置是可用的（至少在提交 C<sub>new</sub> 之前），并且大部分新配置可用。</p><ol><li><p>可以在配置更改的所有步骤中选举一位领导者。</p><ul><li>如果新集群中具有最新日志的可用服务器具有 C<sub>new</sub> 条目，它可以从大多数 C<sub>new</sub> 那里收集选票并成为领导者。</li><li>否则，C<sub>new</sub> 条目必然尚未提交。 在旧集群和新集群中，具有最新日志的可用服务器可以收集大多数  C<sub>old</sub> 和大多数 C<sub>new</sub> 的投票，因此，无论使用哪种配置，它都可以成为领导者。</li></ul></li><li><p>领导一经选举便得到维持，假设他的心跳达到了正常状态，<br>除非它因不在 C<sub>new</sub> 中但已提交 C<sub>new</sub> 而有意退出。</p><ul><li>如果领导者可以可靠地将心跳发送到其自己的跟随者，则它或其跟随者都不会接受更高的任期：他们不会超时开始任何新的选举，并且他们将忽略来自其他服务器的更高任期的任何 RequestVote 消息。因此，领导者不会被迫下台。</li><li>如果不在 C<sub>new</sub> 中的服务器提交 C<sub>new</sub> 条目并退出，则 Raft 将选出新的领导者。这个新领导者很可能将成为 C<sub>new</sub> 的一部分，从而完成配置更改。 但是，下台的服务器可能会再次成为领导者，这存在一些（较小）风险。 如果它再次当选，它将确认 C<sub>new</sub> 条目的提交并很快下台，并且 C<sub>new</sub> 中的服务器下次可能再次成功。</li></ul></li><li><p>在整个配置更改期间，领导者将为客户端请求提供服务。</p><ul><li>领导者可以在整个更改过程中继续将客户请求添加到他们的日志中。</li><li>由于在将新服务器添加到集群之前对其进行了跟踪，因此领导者可以提前提交其提交索引并及时回复客户端。</li></ul></li><li><p>领导者将通过提交 C<sub>new</sub> 来推进并完成配置更改，并在必要时退出以允许 C<sub>new</sub> 中的服务器成为领导者。</p></li></ol><p>因此，在上述假设下，本节中描述的机制足以在任何成员更改期间保持可用性。</p><h3 id="4-3-使用联合共识进行任意配置更改"><a href="#4-3-使用联合共识进行任意配置更改" class="headerlink" title="4.3 使用联合共识进行任意配置更改"></a>4.3 使用联合共识进行任意配置更改</h3><p>本节介绍了一种更复杂的集群成员更改方法，该方法可以处理对配置的任意更改。例如，可以一次将两个服务器添加到集群中，也可以一次替换五个服务器集群中所有的服务器。这是我们提出的第一种处理成员更改的方法，这里只是为了完整性而进行描述它。既然我们知道了更简单的单服务器方法，我们建议改为使用单服务器方法，因为处理任意更改需要额外的复杂性。任意更改通常是文献中假定的成员更改方式，但是我们认为在实际系统中并不需要这种灵活性，在实际系统中，一系列单服务器更改可以将集群成员更改为任何所需的配置。</p><p>为了确保跨任意配置更改的安全性，集群首先切换到过渡配置，我们称之为联合共识；一旦联合共识被提交，系统便过渡到新配置。联合共识将新旧配置结合到了一起：</p><ul><li>日志条目被复制到所有包含新配置或旧配置的服务器。</li><li>来自任一配置的任何服务器都可以作为领导者。</li><li>协议（用于选举和条目提交）要求新老配置各占多数。 例如，当从 3 个服务器的集群更改为 9 个服务器的不同集群时，协议要求既需要旧配置的 3 个服务器中的 2 个，也需要新配置的 9 个服务器中的 5 个。</li></ul><p>联合共识允许各个服务器在不同的时间在不同的配置之间转换，且不会影响安全性。 此外，联合共识允许集群在整个配置更改期间继续为客户请求提供服务。</p><p>这种方法通过联合配置的中间日志条目扩展了单服务器成员更改算法。图 4.8 说明了该过程。当领导者收到将配置从 C<sub>old</sub> 更改为 C<sub>new</sub> 的请求时，它保存用于联合共识的配置（图中的 C<sub>old</sub>,<sub>new</sub>）作为日志条目，并使用常规的 Raft 机制复制该条目。与单服务器配置更改算法一样，每台服务器将配置存储在日志中后即开始使用新配置。这意味着领导者将使用 C<sub>old</sub>,<sub>new</sub> 规则来确定何时提交 C<sub>old</sub>,<sub>new</sub> 的日志条目。如果领导者崩溃，则可以根据获胜的候选人是否收到了 C<sub>old</sub>,<sub>new</sub>，在 C<sub>old</sub> 或 C<sub>old</sub>,<sub>new</sub> 下选择新的领导者。无论如何，C<sub>new</sub> 不能在此期间做出单方面决定。</p><p>一旦提交了 C<sub>old</sub>,<sub>new</sub>，C<sub>old</sub>,<sub>new</sub> 都无法在未经对方批准的情况下做出决策，并且领导者完整性属性确保只有具有 C<sub>old</sub>,<sub>new</sub> 日志条目的服务器才能被选为领导者。现在，领导者可以安全地创建 C<sub>new</sub> 日志条目并将其复制到集群。一旦服务器看到该配置，它将立即生效。当在 C<sub>new</sub> 规则下提交了 C<sub>new</sub> 日志条目时，旧的配置就变为不相关，不在新配置中的服务器此时可以关闭。如图 4.8 所示，C<sub>old</sub> 和 C<sub>new</sub> 都没有时间可以单方面做出决定。这样可以保证安全。</p><blockquote><p><img src="/raft-thesis-translate/4_8.png" srcset="/img/loading.gif" alt><br>图4.8：使用联合共识进行配置更改的时间表。 虚线表示已创建但尚未提交的配置条目，实线表示最新提交的配置条目。领导者首先在其日志中创建 C<sub>old</sub>,<sub>new</sub> 配置条目，并将其提交给 C<sub>old</sub>,<sub>new</sub>（大多数 C<sub>old</sub> 和 C<sub>new</sub>）。 然后，它创建 C<sub>new</sub> 条目并将其提交给大多数 C<sub>new</sub>。 在任何时候，C<sub>old</sub> 和 C<sub>new</sub> 都不能同时独立地做出决策。</p></blockquote><p>联合共识方法可以推广到允许在先前的更改仍在进行时开始新的配置更改。然而，这样做没有太大的实际好处。相反，当配置更改已在进行时（当其最新配置未提交或不是简单多数时），领导者将拒绝其他配置更改。以这种方式拒绝的更改可以简单地等待并稍后再试。</p><p>这种联合共识方法比单服务器更改更为复杂，因为它需要在中间配置之间进行转换。联合配置还要求更改所有投票和提交决定的方式；领导者必须检查服务器是否既构成旧集群的大部分，也构成新集群的大多数，而不是简单地对服务器进行计数。要实现这一点，需要发现并更改我们的 Raft 实现中的六个比较。</p><h3 id="4-4-系统集成"><a href="#4-4-系统集成" class="headerlink" title="4.4 系统集成"></a>4.4 系统集成</h3><p>Raft 的实现可以通过不同的方式实现本章描述的集群成员更改机制。例如，图 4.1 中的 AddServer 和 RemoveServer RPC 可以由管理员直接调用，也可以由使用一系列单服务器步骤以任意方式更改配置的脚本调用。</p><p>在响应服务器故障等事件时自动调用成员更改可能是可取的。然而，这最好根据一个合理的政策来做。例如，对于集群来说，自动删除失败的服务器可能是危险的，因为它可能会留下太少的副本来满足预期的持久性和容错需求。一种可行的方法是让系统管理员配置所需的集群大小，在此约束下，可用的服务器可以自动替换失败的服务器。</p><p>当进行需要多个单服务器步骤的集群成员更改时，最好能够在删除服务器之前添加服务器。例如，要替换三个服务器集群中的一个服务器，添加一个服务器，然后删除另一个服务器，则系统在整个过程中始终允许处理一个服务器故障。但是，如果在添加另一个服务器之前先删除一个服务器，那么系统将暂时无法容忍任何故障（因为两个服务器集群需要两个服务器都可用）。</p><p>成员更改激发了引导集群的另一种方法。如果没有动态成员更改，每个服务器只有一个列出配置的静态文件。有了动态成员更改，服务器不再需要静态配置文件，因为系统在 Raft 日志中管理配置；当然它也可能容易出错（例如，应使用哪种配置来初始化新服务器？）。实际上，我们建议在第一次创建集群时，使用配置条目作为其日志中的第一个条目来初始化一个服务器。此配置仅列出该服务器。它本身构成其配置的大部分，因此它可以认为此配置已提交。从那时起，其他服务器应使用空日志进行初始化；它们被添加到集群中，并通过成员更改机制了解当前配置。</p><p>成员更改也需要一种动态的方法让客户找到集群。这将在第 6 章中讨论。</p><h3 id="4-5-结论"><a href="#4-5-结论" class="headerlink" title="4.5 结论"></a>4.5 结论</h3><p>本章介绍了 Raft 用于自动处理集群成员更改的扩展。这是基于共识的完整系统的重要组成部分，因为容错要求可能会随时间的推移而变化，并且最终需要更换故障服务器。</p><p>由于新配置会影响“多数”的含义，因此共识算法必须从根本上保证在配置更改期间的安全性。本章介绍了一种简单的方法，可以一次添加或删除单个服务器。这些操作简单地保障了安全性，因为在更改期间至少有一台服务器与大多数服务器重叠。可以组合多个单服务器更改来更彻底地修改集群。Raft 允许集群在成员更改期间继续正常运行。</p><p>要在配置更改期间保持可用性，需要处理几个重要的问题。特别是，不属于新配置的服务器干扰有效集群领导者的问题非常微妙；在选择基于心跳的有效解决方案之前，我们在几个基于日志比较的低效解决方案中挣扎。</p><h2 id="5-日志压缩"><a href="#5-日志压缩" class="headerlink" title="5 日志压缩"></a>5 日志压缩</h2><p>随着越来越多的客户请求，Raft 的日志在正常运行期间会不断增长。随着它变的越来越大，它会占用更多的空间，同时也需要更多的时间来回放。如果没有压缩日志的方法，最终将导致可用性问题：即服务器存储空间不足，或者启动时间太长。因此，任何实际系统都需要某种形式的日志压缩。</p><p>日志压缩的一般思想是，日志中的许多信息会随着时间的流逝而过时并可以丢弃。例如，如果稍后的操作将 x 设置为 3，则将 x 设置为 2 的操作已过时。一旦提交了日志条目并将其应用于状态机，就不再需要用于到达当前状态的中间状态和操作，并且可以压缩它们。</p><p>与 Raft 核心算法和成员更改不同，不同的系统在日志压缩方面有不同的需求。由于多种原因，没有一种适合所有人的解决方案来进行日志压缩。首先，不同的系统可能会选择在不同程度上权衡简单性和性能。其次，状态机必须紧密地参与日志压缩，并且状态机的大小以及状态机是基于磁盘还是易失性存储器的差异都很大。</p><p>本章的目的是讨论各种日志压缩方法。在每种方法中，日志压缩的大部分责任都落在状态机上，状态机负责将状态写入磁盘并压缩状态。状态机可以通过不同的方式来实现这一目标，本章将对此进行介绍，并在图 5.1 中进行总结：</p><blockquote><p><img src="/raft-thesis-translate/5_1.png" srcset="/img/loading.gif" alt><br>图 5.1：该图显示了如何在 Raft 中使用各种日志压缩方法。图中日志结构合并树的详细信息基于 LevelDB，日志清理的详细信息基于 RAMCloud。 省略了删除管理规则。</p></blockquote><ul><li>从概念上讲，为基于内存的状态机创建快照最为简单。在创建快照时，整个当前系统状态被写入稳定存储上的快照，然后丢弃该点之前的所有日志。这种方法在 Chubby 和 ZooKeeper 中使用，并且我们在 LogCabin 中也实现了它。它是本章第 5.1 节中最深入介绍的方法。</li><li>对于基于磁盘的状态机，作为正常操作的一部分，系统状态的最新副本将保留在磁盘上。因此，一旦状态机反映了对磁盘的写入，就可以丢弃 Raft 日志，并且仅在将一致的磁盘映像发送到其他服务器时才使用快照（第 5.2 节）。</li><li>第 5.3 节介绍了增量式日志压缩方法，例如日志清理和日志结构合并树。这些方法有效地将数据写入磁盘，并且随着时间的推移它们平均地利用资源。</li><li>最后，第 5.4 节讨论了通过将快照直接存储在日志中来最小化所需机制的快照方法。尽管该方法更容易实现，但仅适用于非常小的状态机。</li></ul><p>LogCabin 当前仅实现基于内存的快照方法（它嵌入了基于内存的状态机）。</p><p>各种压缩方法都共享了几个核心概念。首先，不要将压缩决策集中在领导者上，而是每个服务器独立地压缩其日志的提交前缀。这避免了领导者向已经在日志中记录了数据的跟随者发送数据。它还有助于模块化：日志压缩的大部分复杂性都包含在状态机中，并且与 Raft 本身没有太多交互。这有助于将整个系统的复杂性降至最低：Raft 的复杂性递增而不是倍增了日志压缩的复杂性。在第 5.4 节中进一步讨论了将压缩责任集中在领导者上的其他方法（对于非常小的状态机，基于领导者的方法可能更好）。</p><p>其次，状态机和 Raft 之间的基本交互涉及到将一个日志前缀的责任从 Raft 转移到状态机。在应用条目之后，状态机将这些条目以一种可以恢复当前系统状态的方式反映到磁盘上。完成后，它告诉 Raft 放弃相应的日志前缀前的所有日志。在 Raft 放弃对日志前缀前所有日志的责任之前，它必须保存一些描述日志前缀的自身状态。具体来说，Raft 保留了它丢弃的最后一个条目的索引和任期；这会将其余的日志锚定在状态机的状态之后，并允许 AppendEntries 一致性检查继续进行（它需要日志中第一个条目之前条目的索引和任期）。为了支持集群成员的更改，Raft 还保留了丢弃日志前缀前的最新配置。</p><p>第三，一旦 Raft 丢弃了日志前缀前的日志，状态机将承担两项新的责任。如果服务器重新启动，则状态机需要先从磁盘加载与被丢弃的日志条目应用相对应的状态，然后才能应用 Raft 日志中的任何条目。此外，状态机可能需要拍摄一个一致的状态映像，以便可以将其发送给缓慢的跟随者（跟随者的日志远远落后于领导者的日志）。将压缩延迟到日志条目“完全复制”到集群中的每个成员之前是不可行的，因为必须保证少数缓慢的跟随者不阻止集群完全可用，并且随时可以将新服务器添加到集群中。因此，缓慢的跟随者或新服务器偶尔需要通过网络接收其初始状态。当 AppendEntries 中所需的下一个条目已在领导者的当前日志中删除时，Raft 会检测到。在这种情况下，状态机必须提供一个一致的状态映像，然后由领导者发送给跟随者。</p><h3 id="5-1-基于内存的状态机快照"><a href="#5-1-基于内存的状态机快照" class="headerlink" title="5.1 基于内存的状态机快照"></a>5.1 基于内存的状态机快照</h3><p>第一种快照方法适用于状态机的数据结构保存在内存中的情况。对于数据集为 GB 或数十 GB 的状态机，这是一个合理的选择。它使操作能够快速完成，因为它们不必从磁盘中获取数据。它也很容易编程实现，因为可以使用丰富的数据结构，并且每个操作都可以运行完成（不阻塞 I/O）。</p><p>图 5.2 显示了当状态机保持在内存中时，在 Raft 中进行快照的基本思想。每个服务器都独立拍摄快照，仅覆盖其日志中的已提交条目。快照中的大部分工作涉及序列化状态机的当前状态，这特定于特定的状态机实现。例如，LogCabin 的状态机使用树作为其主要数据结构；它使用顺序的深度优先遍历序列化此树（以便在应用快照时，在其子节点之前创建父节点）。状态机还必须序列化其保留的信息，来为客户端提供线性化能力（请参阅第 6 章）。</p><blockquote><p><img src="/raft-thesis-translate/5_2.png" srcset="/img/loading.gif" alt><br>图 5.2：服务器用一个新的快照替换日志中提交的条目（索引 1 到 5），该快照只存储当前状态（本例中是变量 x 和 y）。在丢弃条目 1 到 5 之前，Raft 保存快照最后包含的索引（5）和任期（3），以便将快照放置到条目 6 之前的日志中。</p></blockquote><p>一旦状态机完成了快照的写入，就可以将日志截断。Raft 首先存储重新启动所需的状态：快照中包含的最后一个条目的索引和任期以及该索引之前的最新配置。然后，它将丢弃日志索引在该索引前的日志。由于以前的快照不再有用，因此也可以将其丢弃。</p><p>如上所述，领导者有时可能需要将其状态发送给缓慢的跟随者和正在加入集群的新服务器。在快照中，此状态只是最新的快照，领导者使用名为 InstallSnapshot 的新 RPC 进行传输，如图 5.3 所示。当跟随者使用此 RPC 接收快照时，它必须决定如何处理其现有的日志条目。通常，快照将包含跟随者日志中尚未包含的新信息。在这种情况下，跟随者将丢弃其整个日志；它全部被快照取代，并且可能具有与快照冲突的未提交条目。相反，如果跟随者接收到描述其某个日志前缀的快照（由于重新传输或错误操作），则快照所覆盖的日志条目将被删除，但快照之后的条目仍然有效，必须保留。</p><blockquote><p><img src="/raft-thesis-translate/5_3.png" srcset="/img/loading.gif" alt><br>图 5.3：领导者调用 InstallSnapshot RPC 将快照发送给缓慢的跟随者。当 AppendEntries 中所需的下一个条目已在领导者的当前日志中删除时，他才会发送快照。他将快照分成多个块进行传输。除其他好处外，这还给跟随者一种领导者还活跃的迹象，因此可以重置其选举计时器。每个块均按顺序发送，从而简化了将文件写入磁盘的过程。RPC 包括 Raft 在重新启动时加载快照所需的状态：快照所覆盖的最后一个条目的索引和任期，以及此时的最新配置。</p></blockquote><p>本节的其余部分讨论了基于内存的状态机快照的次要问题：</p><ul><li>第 5.1.1 节讨论了如何在正常操作的同时拍摄快照，以最大程度地减少拍摄快照对客户端的影响。</li><li>第 5.1.2 节讨论何时应该拍摄快照，从而平衡空间的使用和拍摄快照的开销。</li><li>第 5.1.3 节讨论实现快照时出现的问题。</li></ul><h4 id="5-1-1-并发拍摄快照"><a href="#5-1-1-并发拍摄快照" class="headerlink" title="5.1.1 并发拍摄快照"></a>5.1.1 并发拍摄快照</h4><p>创建快照可能需要很长时间，无论是序列化状态还是将其写入磁盘。例如，在当今的服务器上复制 10GB 内存大约需要 1 秒钟的时间，而对其进行序列化通常将花费更长的时间：即使是固态磁盘也只能在 1 秒钟内写入约 500MB，因此，序列化和写入快照必须与正常操作同时进行，以避免出现可用性缺口。</p><p>幸运的是，写时复制技术允许应用热更新，而不会影响正在写入的快照。有两种解决方法：</p><ul><li>状态机可以用不可变的（功能性）数据结构来支持这一点。因为状态机命令不会修改适当的状态，所以快照任务可以保留对某个先前状态的引用，并将其一起写入快照中。</li><li>或者，可以使用操作系统的写时复制支持（在编程环境允许的情况下）。例如，在 Linux 上，内存状态机可以使用 fork 来复制服务器整个地址空间。然后，子进程可以写出状态机的状态并退出，而父进程则继续为请求提供服务。LogCabin 的实现当前使用了此方法。</li></ul><p>服务器需要额外的内存来并创建快照，应该提前计划和管理这些内存。对于状态机来说，有一个快照文件的流接口是非常重要的，这样快照在拍摄时就不必完全暂存在内存中。尽管如此，写时复制需要的额外内存与快照过程中一直在改变的状态机成比例。此外，由于错误共享，依靠操作系统进行写时复制通常会使用更多的内存（例如，如果两个不相关的数据项恰好位于同一页内存中，即使只有第一项发生了更改，第二项也会重复）。两个不相关的数据项恰好位于内存的同一页上，即使只有第一项发生了更改，第二项也会重复）。在拍摄快照期间内存容量耗尽的不幸事件中，服务器应停止接受新的日志条目，直到完成快照为止。这将暂时牺牲服务器的可用性（集群可能仍保持可用状态），但至少允许服务器恢复。最好不要中止快照并稍后重试，因为下一次尝试也可能会遇到相同的问题。（LogCabin 实现了流接口，但目前还不能优雅地处理内存耗尽问题。）</p><h4 id="5-1-2-何时拍摄快照"><a href="#5-1-2-何时拍摄快照" class="headerlink" title="5.1.2 何时拍摄快照"></a>5.1.2 何时拍摄快照</h4><p>服务器必须决定何时进行快照。如果服务器快照太频繁，则会浪费磁盘带宽和其他资源；如果快照太不频繁，则可能会耗尽其存储容量，并增加了重新启动期间重放日志所需的时间。</p><p>一种简单的策略是在日志达到固定大小（以字节为单位）时拍摄快照。如果将此大小设置为比快照的预期大小大得多，那么用于快照的磁盘带宽开销将很小。但是，对于小型状态机，这可能导致不必要的没有被压缩的大量日志。</p><p>更好的方法是将快照的大小与日志的大小进行比较。如果快照比日志小很多倍，则可能值得拍摄快照。但是，在拍摄快照之前计算快照的大小可能既困难又麻烦，给状态机带来了很大的负担，也许需要与实际拍摄快照几乎相同的工作来动态地计算快照的大小。压缩快照文件可以节省空间和带宽，但是很难预测压缩输出的大小。</p><p>幸运的是，使用上一个快照的大小而不是下一个快照的大小可以得到合理的行为。一旦日志的大小超过前面快照的大小乘以一个可配置的扩展因子，服务器就会拍摄快照。扩展因子在磁盘带宽与空间利用率之间进行权衡。例如，扩展因子 4 导致约 20% 的磁盘的带宽用于快照（对于快照的每 1 字节，将写入 4 字节的日志条目），并且存储一个快照（旧的快照，日志 4 倍，和新写入的快照）需要大约其 6 倍的磁盘容量。</p><p>快照仍然会导致 CPU 和磁盘带宽的大量使用，这可能会影响客户机的性能。这可以通过增加硬件来缓解；例如，可以使用第二个磁盘驱动器来提供额外的磁盘带宽。</p><p>其实也可能以客户端请求永远不会在正在快照的服务器上等待的方式来调度拍摄快照。在这种方法中，服务器将进行协调，以便在任何时候（如果可能）仅对集群中的少数服务器拍摄快照。由于 Raft 只需要大多数服务器来提交日志条目，因此少数拍摄快照的服务器通常不会对客户端产生不利影响。当领导者希望拍摄快照时，它将首先退出，从而允许另一台服务器来管理集群。如果此方法足够可靠，则还可以消除并发拍摄快照的需求；服务器拍摄快照时可能只是不可用（尽管它们将计入集群掩盖故障的能力）。这对于将来的工作来说是一个令人兴奋的机会，因为它有可能提高整体系统性能并减少机制。</p><h4 id="5-1-3-实现问题"><a href="#5-1-3-实现问题" class="headerlink" title="5.1.3 实现问题"></a>5.1.3 实现问题</h4><p>本节回顾了拍摄快照实现所需的主要组件，并讨论了实现它们的困难：</p><ul><li><p>保存和加载快照：保存快照涉及序列化状态机的状态并将该数据写到文件中，而加载则是相反的过程。尽管从各种类型的数据对象的原型表示中序列化它们有些繁琐，我们发现这还是相当简单。从状态机到磁盘上文件的流接口有助于避免将整个状态机状态缓冲到内存；压缩流并对其应用校验和也可能是有益的。LogCabin 首先将每个快照写入一个临时文件，然后在写入完成并刷新到磁盘后重命名该文件。这样可以确保没有服务器在启动时加载部分写入的快照。</p></li><li><p>传输快照：传输快照涉及实现 InstallSnapshot RPC 的领导者和跟随者。这非常简单，并且可以与从磁盘中保存和加载快照共享一些代码。传输的性能通常不是很重要（需要此快照的跟随者尚未参与条目的提交，所以可能不需要很快；另一方面，如果集群遭受其他故障，则可能需要跟随者尽快追赶上以恢复可​​用性）。</p></li><li><p>消除不安全的日志访问并丢弃日志条目：我们最初设计 LogCabin 时没有考虑日志压缩，因此代码假定如果日志中存在条目 i，则也将存在条目 1 至 i - 1。考虑日志压缩之后，这不再正确。例如，在确定 AppendEntries RPC 中上一个条目的任期时，该条目可能已被丢弃。在整个代码中去除这些假设需要仔细的推理和测试。如果编译器能够强制每个对日志的访问都处理索引越界的情况，那么在更强大的类型系统的帮助下，这将更容易实现。一旦我们确保所有日志访问的安全，丢弃日志前缀就很简单了。在此之前，我们只能单独测试保存、加载和传输快照，但是当日志条目可以安全地丢弃时，这些都可以在系统范围的测试中执行。</p></li><li><p>写时复制并发拍摄快照：并发拍摄快照可能需要重新运行状态机或利用操作系统的 fork 操作。 LogCabin 目前使用了 fork，它与线程和 C++ 析构函数的交互性很差。要使其正常工作会带来一些困难。但是，它只需要少量代码，并且完全不需要修改状态机的数据结构，因此我们认为这是正确的方法。</p></li><li><p>决定何时拍摄快照:我们建议在开发期间应用每个日志条目后进行快照，因为这有助于快速捕获 bug 。一旦实现完成后，就应该添加一个更有用的拍摄快照策略（例如，使用有关 Raft 日志大小和最后一个快照大小的统计信息）。</p></li></ul><p>我们发现快照的分段开发和测试是一个挑战。在可以丢弃日志条目之前，这些组件中的大多数必须已经就位，但只有在此之后，才会在系统范围的测试中执行许多新的代码路径。因此，实现者应该仔细考虑实现和测试这些组件的顺序。</p><h3 id="5-2-基于磁盘的状态机快照"><a href="#5-2-基于磁盘的状态机快照" class="headerlink" title="5.2 基于磁盘的状态机快照"></a>5.2 基于磁盘的状态机快照</h3><p>本节讨论了使用磁盘作为其主要记录位置的大型状态机（数十或数百 GB）的快照方法。这些状态机的行为有所不同，它们总是在磁盘上准备好状态的副本，以防崩溃。应用 Raft 日志中的每个条目都会更改磁盘上的状态，并有效地获得新的快照。因此，一旦应用了条目，就可以从 Raft 日志中将其丢弃。（状态机还可以缓冲内存中的写操作，以期提高磁盘效率；一旦将它们写入磁盘，相应的条目就可以从 Raft 日志中丢弃。）</p><p>基于磁盘的状态机的主要问题是改变磁盘上的状态会导致性能下降。如果没有写缓冲，那么每个命令应用时都需要一个或多个随机磁盘写，这会限制系统的总体写吞吐量（而写缓冲可能也没有多大帮助）。第 5.3 节讨论了日志压缩的增量方法，该方法可通过大量顺序写更有效地将数据写入磁盘。</p><p>基于磁盘的状态机必须能够提供一直的磁盘快照，以便将其传输给缓慢的跟随者。尽管他们总是在磁盘上有快照，但他们也在不断对其进行修改。因此，他们仍然需要写时复制技术，以便在足够长的时间内保持一致的快照，以便传输它。幸运的是，磁盘格式几乎总是划分为逻辑块，因此在状态机中实现写时复制应该很简单。基于磁盘的状态机也可以依赖于操作系统支持来获取快照。例如，Linux 上的 LVM（逻辑卷管理）可用于创建整个磁盘分区的快照，另外一些最新的文件系统允许快照单个目录。</p><p>复制磁盘映像的快照可能会花费很长时间，并且随着对磁盘修改的积累，保留快照所需的额外磁盘使用量也会增加。尽管我们尚未实现基于磁盘的快照，但我们推测基于磁盘的状态机可以通过以下算法传输其磁盘内容来避免大部分此类开销：</p><ol><li>对于每个磁盘块，跟踪其上次修改的时间。</li><li>在继续正常操作的同时，将整个磁盘内容逐块传输到跟随者。在此过程中，领导者上没有使用额外的磁盘空间。由于块是并发修改的，因此很可能导致跟随者磁盘上的磁盘映像不一致。当每个块从领导者转移时，注意它最后的修改时间。</li><li>拍摄磁盘内容的写时复制快照。一旦采取了这种措施，领导者将拥有其磁盘内容的一致性副本，但是由于客户端的持续操作，对磁盘的修改会使用额外的磁盘空间。</li><li>仅重新传输在步骤 2 中首次传输它们与在步骤 3 中拍摄快照之间所修改的磁盘块。</li></ol><p>我们希望一致性快照的大多数块在步骤 3 拍摄快照前已经被传输。如果是这种情况，步骤 4 中的传输将很快进行：在步骤 4 中用于保留领导者快照的额外磁盘容量将很少，而且在步骤 4 中用于再次传输修改磁盘块的额外网络带宽也很低。</p><h3 id="5-3-增量清理方法"><a href="#5-3-增量清理方法" class="headerlink" title="5.3 增量清理方法"></a>5.3 增量清理方法</h3><p>也可以采用增量方法进行压缩，例如日志清理和日志结构合并树（LSM 树）。 尽管它们比快照更复杂，但增量方法具有一些理想的特性：</p><ul><li>它们一次只处理一部分数据，因此它们会随时间平均分配压缩负载。</li><li>无论是非正常操作还是压缩期间，它们都可以高效地写入磁盘。在两种情况下，它们都使用了大量顺序写。增量方法还选择性地压缩了磁盘中可回收空间最大的部分，因此与基于内存的状态机快照（在每个快照上重写所有磁盘）相比，它们向磁盘写入的数据更少。</li><li>他们可以很轻松地传输一致的状态快照，因为它们不会修改磁盘区域。</li></ul><p>第 5.3.1 节和第 5.3.2 节首先介绍了日志清理和 LSM 树的基本知识。然后，第 5.3.3 节讨论了如何将它们应用于 Raft。</p><h4 id="5-3-1-日志清理基础"><a href="#5-3-1-日志清理基础" class="headerlink" title="5.3.1 日志清理基础"></a>5.3.1 日志清理基础</h4><p>日志清理是在日志结构文件系统的上下文中引入的，最近已提出用于内存存储系统（例如 RAMCloud）的方法。原则上，日志清理可用于任何类型的数据结构，尽管某些类型的数据结构比其他类型的数据结构更难有效地实现。</p><p>日志清理功能将日志保留为系统状态的记录位置。该布局针对顺序写进行了优化，并高效的实现了随机读。因此，需要索引结构来定位要读取的数据项。</p><p>在日志清理时，会将日志分为称为段的连续区域。日志清理器的每次都使用三步算法来压缩日志：</p><ol><li>首先选择要清除的段，这些段已累积了大量的过期条目。</li><li>然后，将活动条目（那些有助于当前系统状态的活动条目）从这些段复制到日志的头部。</li><li>最后，它释放了这些段的存储空间，从而使该空间可用于新段。</li></ol><p>为了最小化对正常运行的影响，这个过程并发执行。</p><p>将活动条目复制到日志开头的结果是，条目无法按顺序进行重放。这些条目可以包含其他信息（例如版本号），以便在应用日志时重新创建正确的顺序。</p><p>选择清理哪一部分的策略对性能有很大的影响；先前的研究提出了一种成本效益策略，该策略不仅考虑了活动条目所使用的空间数量，而且还考虑了这些条目可能保持活动的时间。</p><p>确定条目是否是活动的是状态机的职责。例如，在键值存储中，如果键存在并且当前已设置为给定值，则用于将键设置为特定值的日志条目是活动的。确定删除键的日志条目是否是活动的则更加微妙：只要在日志中存在设置该键的任何先前条目，该日志条目就是活动的。RAMCloud 根据需要保留删除命令（称为逻辑删除命令），但是另一种方法是定期写出当前状态下存在的键的摘要，然后所有未列出的键的日志条目都不是活动的。键值存储是一个相当简单的例子；其他状态机是可能确定活动性的，但不幸的是，其方法各不相同。</p><h4 id="5-3-2-日志结构合并树基础"><a href="#5-3-2-日志结构合并树基础" class="headerlink" title="5.3.2 日志结构合并树基础"></a>5.3.2 日志结构合并树基础</h4><p>日志结构合并树（LSM 树）最初由 O’Neil 描述，后来通过 BigTable 在分布式系统中流行。 它们用在诸如 Apache Cassandra 和 HyperDex 之类的系统中，并且可以作为诸如 LevelDB 及其分支（例如 RocksDB 和 HyperLevelDB ）之类的系统使用。</p><p>LSM 树是存储有序键值对的树状数据结构。在较高的层次上看，他们使用磁盘的方式类似于日志清理方法：它们会进行大量顺序写，并且不会就地修改磁盘上的数据。 但是，LSM 树不维护日志中的所有状态，而是重新组织状态以实现更好的随机访问。</p><p>典型的 LSM 树将最近写入的键保留在磁盘上的小日志中。当日志达到固定大小时，将按键对日志进行排序，并按排序顺序将其写入称为 Run 的文件。Run 永远不会被修改，但压缩过程会定期将多个 Run 合并在一起，从而产生新的 Run 并丢弃旧的 Run。合并让人想起归并排序；当一个键在多个输入 Run 中时，仅保留最新版本，因此生成的 Run 更加紧凑。图 5.1 总结了 LevelDB 中使用的压缩策略。它按年龄区分运行以提高效率（类似于日志清理）。</p><p>在正常运行期间，状态机可以直接对此数据进行操作。要读取键，它首先检查该键是否最近在其日志中被修改，然后检查每个 Run。为了避免在每次查找 Run 是否有键时实行遍历，某些系统会为每个 Run 创建一个布隆过滤器（紧凑的数据结构，在某些情况下可以肯定地说某个键在 Run 中不存在，尽管有时即使键不存在也可能需要搜索 Run）。</p><h4 id="5-3-3-Raft-中的日志清理和日志结构化合并树"><a href="#5-3-3-Raft-中的日志清理和日志结构化合并树" class="headerlink" title="5.3.3 Raft 中的日志清理和日志结构化合并树"></a>5.3.3 Raft 中的日志清理和日志结构化合并树</h4><p>我们还没有尝试在 Raft 中实现日志清理或 LSM 树，但是我们推测两者都可以工作的很好。将 LSM 树应用于 Raft 似乎相当简单。因为 Raft 日志已经将最近的条目持久化到磁盘上，所以 LSM 树可以将最近的数据以更方便的树格式保存在内存中。这对于服务查找将是快速的，并且当 Raft 日志达到固定大小时，树已经排好序，可以作为新的 Run 写入磁盘。将状态从领导者转移到缓慢的跟随者需要将所有 Run 发送到跟随者（但不包括内存树）；幸运的是，Run 是不可变的，因此不必担心在传输期间会修改 Run。</p><p>对 Raft 进行日志清理则不是很明确。我们首先考虑了一种方法，其中将 Raft 分为几段并进行清理（见图 5.4（a））。不幸的是，清理会在释放段的日志中放置很多空洞，这需要修改日志复制的方法。我们认为这种方法是可行的，但是它给 Raft 及其与状态机的交互增加了极大的复杂性。此外，由于只有领导者可以追加到 Raft 日志中，因此清理工作必须基于领导者，这将浪费领导者的网络带宽（这将在 5.4 节中进一步讨论）。</p><blockquote><p><img src="/raft-thesis-translate/5_4.png" srcset="/img/loading.gif" alt><br>图 5.4：在 Raft 中进行日志清理的两种可能方法。</p></blockquote><p>更好的方法是类似于 LSM 树来处理日志清理：Raft 将最近的更改保留为连续的日志，而状态机将其自身的状态保留为日志，但是这些日志在逻辑上是不同的（请参见图 5.4（b））。当 Raft 日志增长到固定大小时，它的新条目将被写入状态机日志中的一个新段，并且 Raft 日志的相应前缀前的日志将被丢弃。状态机中的段将在每台服务器上独立清理，并且 Raft 日志将完全不受此影响。与直接清除 Raft 日志相比，我们更喜欢这种方法，因为日志清除的复杂性完全封装在状态机中（状态机和 Raft 之间的接口仍然很简单），并且服务器可以独立进行清理。</p><p>如上所述，这种方法将要求状态机将所有 Raft 的日志条目写入其自己的日志中（尽管它可以大批量地这样做）。可以通过直接从 Raft 日志中移动包含日志条目的文件，并将该文件合并到状态机的数据结构中，从而优化掉这个额外的副本。这对于性能至关重要的系统可能是有用的优化，但是不幸的是，它将更紧密地耦合状态机模块和 Raft 模块，因为状态机将需要了解 Raft 日志的磁盘表示。</p><h3 id="5-4-备选方案：基于领导者的方法"><a href="#5-4-备选方案：基于领导者的方法" class="headerlink" title="5.4 备选方案：基于领导者的方法"></a>5.4 备选方案：基于领导者的方法</h3><p>本章介绍的日志压缩方法有别于 Raft 强有力的领导者原则，因为服务器可以在领导者不知情的情况下压缩日志。但是，我们认为这种背离是合理的。尽管拥有领导者可以避免在达成共识时出现冲突的决策，但是在拍摄快照时已经达成共识，因此不会有决策冲突。数据仍然仅从领导者流向跟随者，但是跟随者现在可以独立地重组其数据。</p><p>我们还考虑了基于领导者的日志压缩方法，但是任何好处通常都被性能考虑所抵消。领导者压缩它的日志，然后将结果发送给跟随者，这是很浪费的，因为他们也可以独立地压缩自己的日志。将冗余状态发送给每个跟随者将浪费网络带宽并减慢压缩过程。每个跟随者已经掌握了压缩自身状态所需的信息，而领导者的向外网络带宽通常是 Raft 最宝贵的资源（瓶颈）。对于基于内存的快照，从服务器本地状态拍摄快照通常比通过网络发送和接收快照要节约资源得多。对于增量压缩方法，这更多地取决于硬件配置，但是我们也认为独立压缩会更节约资源。</p><h4 id="5-4-1-将快照存储在日志中"><a href="#5-4-1-将快照存储在日志中" class="headerlink" title="5.4.1 将快照存储在日志中"></a>5.4.1 将快照存储在日志中</h4><p>基于领导者的方法的一个可能好处是，如果所有系统状态都可以存储在日志中，那么就不需要新的机制来复制和保持状态。因此，我们考虑了一种基于领导者的快照方法，在这种方法中，领导者将创建快照并将快照存储为 Raft 日志中的条目，如图 5.5 所示。领导者随后将使用 AppendEntries RPC 将快照发送给其每个跟随者。为了减少对正常操作的任何干扰，每个快照将被分成许多条目，并与日志中的普通客户端命令交织在一起。</p><blockquote><p><img src="/raft-thesis-translate/5_5.png" srcset="/img/loading.gif" alt><br>图 5.5：基于领导者的方法将快照存储在日志中的块中，与客户端请求交错。快照过程在开始条目处开始，并在结束条目处完成。快照存储在开始和结束之间的多个日志条目中。因此，客户端请求可以与快照并行进行，每个条目的大小受到限制，并且条目附加到日志的速率受到限制：只有当领导者获悉前一个快照块已经提交时，下一个快照块才会继续附加日志。每个服务器得知结束条目已提交后，就可以丢弃其日志中的条目直到对应的开始条目为止。重放日志需要一个两步算法：首先应用最后一个完整的快照，然后再应用快照开始条目之后的客户端请求。</p></blockquote><p>与将快照存储在日志之外相比，这将实现更好的机制经济性，因为服务器不需要单独的机制来传输或持久化快照（它们将像其他日志条目一样被复制并持久化）。但是这些跟随者可能很容易生成自己的快照，所以这样会浪费跟随者的网络带宽，除此以外这还有一个严重的问题，如果领导者在创建快照的过程中失败，则它将部分快照保留在服务器的日志中。原则上，这种情况可能会反复发生，并且会耗尽服务器的存储容量，并会因无数次失败的快照尝试而积累起来。因此，我们认为这种机制在实践中不可行。</p><h4 id="5-4-2-对于非常小的状态机使用基于领导者的方法"><a href="#5-4-2-对于非常小的状态机使用基于领导者的方法" class="headerlink" title="5.4.2 对于非常小的状态机使用基于领导者的方法"></a>5.4.2 对于非常小的状态机使用基于领导者的方法</h4><p>对于非常小的状态机，将快照存储在日志中不仅可行，而且还可以大大简化。如果快照足够小（最多约 1M 字节），则可以轻松地将其放入单个日志条目中，而不会中断正常操作时间太长。为了以这种方式压缩服务器的日志，领导者需要：</p><ol><li>停止接受新的客户请求。</li><li>等待提交其日志中的所有条目，并等待其状态机应用其日志中的所有条目。</li><li>（同步）拍摄快照。</li><li>将快照追加到其日志末尾的单个日志条目中。</li><li>恢复接受新的客户请求。</li></ol><p>一旦每个服务器都知道快照条目已提交，它就可以丢弃其日志中快照之前的每个条目。这种方法会在客户端请求被停止和快照条目被传输时造成一个小的可用性缺口，但是它对非常小的状态机的影响是有限的。</p><p>这种更简单的方法避免了在日志之外持久化快照，使用新的 RPC 传输快照以及并发快照的实现工作。然而，成功的系统往往比它们最初设计者所期望的使用得更多，这种方法对于大型的状态机并不适用。</p><h3 id="5-5-结论"><a href="#5-5-结论" class="headerlink" title="5.5 结论"></a>5.5 结论</h3><p>本章讨论了 Raft 中日志压缩的几种方法，图 5.1 对此进行了概述。不同的方法适用于不同的系统，具体取决于状态机的大小，所需的性能水平以及预算的复杂程度。Raft 支持具有多种共享同一概念框架的方法：</p><ul><li>每个服务器都独立压缩其日志的提交前缀。</li><li>状态机和 Raft 之间的基本交互涉及将日志前缀的责任从 Raft 转移到状态机。一旦状态机将命令应用到磁盘上，它就会指示 Raft 放弃日志的相应前缀前的日志。Raft 会保留它最后丢弃条目的索引和任期，以及该索引的最新配置。</li><li>一旦 Raft 丢弃了日志前缀前的日志，状态机将承担两项新的职责：在重新启动时加载状态，提供一致的映像传输给缓慢的跟随者。</li></ul><p>基于内存的状态机快照已在包括 Chubby 和 ZooKeeper 在内的多个生产系统中成功使用，并且我们已在 LogCabin 中实现了这种方法。尽管对大多数操作而言，在内存数据结构上进行操作很快速，但快照过程中的性能可能会受到很大的影响。并发拍摄快照有助于隐藏资源使用情况，将来，在集群中调度服务器在不同时间进行快照可能会使快照过程完全不影响客户端。</p><p>在适当位置改变其状态的基于磁盘的状态机在概念上很简单。他们仍然需要写时复制技术来将一致的磁盘映像传输到其他服务器，但这对于磁盘来说可能是一个很小的负担，因为磁盘自然会分成多个块。但是，正常操作期间的磁盘随机写速度通常会很慢，因此这种方法将限制系统的写入吞吐量。</p><p>最终，增量方法可能是最有效的压缩形式。通过一次对状态的小片段进行操作，它们可以限制资源使用的突然增加（并且还可以并发压缩）。它们还可以避免将相同的数据重复写入磁盘。稳定的数据应该进入不经常压缩的磁盘区域。尽管实现增量压缩可能很复杂，但是可以将这种复杂性转移到诸如 LevelDB 之类的库中。此外，通过将数据结构保留在内存中并在内存中缓存更多磁盘，具有增量压缩功能的客户端操作的性能可以接近基于内存的状态机。</p><h2 id="6-客户端交互"><a href="#6-客户端交互" class="headerlink" title="6 客户端交互"></a>6 客户端交互</h2><p>本章描述了客户端如何与基于 Raft 的复制状态机交互的几个问题：</p><ul><li>第 6.1 节描述了客户端如何寻找集群，即使集群的成员会随着时间变化。</li><li>第 6.2 节描述了如何将客户的请求路由到领导者进行处理。 </li><li>第 6.3 节描述了 Raft 如何提供线性化一致性。</li><li>第 6.4 节描述了 Raft 如何更有效地处理只读查询。</li></ul><p>图 6.1 显示了客户端用来与复制状态机进行交互的 RPCs。本章将讨论这些元素。这些问题适用于所有基于共识的系统，并且 Raft 的解决方案与其他系统类似。</p><blockquote><p><img src="/raft-thesis-translate/6_1.png" srcset="/img/loading.gif" alt><br>图 6.1：客户端调用 ClientRequest RPC 来修改复制状态；他们调用 ClientQuery RPC 来查询复制状态。新的客户端使用 RegisterClient RPC 接收其客户端标识符，这有助于确定何时线性化所需的会话信息已被丢弃。在该图中，非领导者的服务器将客户端重定向到领导者，并且在不依赖时钟实现线性化的情况下为只读请求提供服务（本文提供了替代方案）。 诸如 6.3 之类的节号指出了讨论特殊功能的地方。</p></blockquote><p>本章假定基于 Raft 的复制状态机作为网络服务直接暴露给客户端。也可以将 Raft 直接集成到客户端应用程序中。在这种情况下，客户端交互中的某些问题可能会被推高一个层级到嵌入在应用程序里的网络客户端。例如，嵌入应用程序的网络客户端时会有和 Raft 作为网络服务在发现集群时遇到类似的问题。</p><h3 id="6-1-寻找集群"><a href="#6-1-寻找集群" class="headerlink" title="6.1 寻找集群"></a>6.1 寻找集群</h3><p>当 Raft 作为一个网络服务公开时，客户端必须找到集群才能与复制状态机进行交互。对于具有固定成员资格的集群，这很简单；比如可以把网络地址静态地存储在客户端配置文件里。但是，如何发现一个随着时间动态改变成员的集群是一个大的挑战。一般有两种方法：</p><ol><li>客户端可以使用网络广播或多播来查找所有集群成员。但是，这仅在支持这些功能的特定环境中有效。</li><li>客户端可以通过外部目录服务（例如 DNS）发现集群成员，该服务可在众所周知的位置访问。外部系统不必与集群成员的服务器列表保持一致，但是必须包含：客户端应始终能够找到所有集群成员，但是包括一些当前不是集群成员的其他服务器是无害的。因此，在集群成员更改期间，应在成员更改之前更新服务器的外部目录，以包括即将添加到集群中的所有服务器，然后在成员更改完成后再次更新，以删除不再属于该集群的任何服务器。<br>LogCabin 客户端当前使用 DNS 查找集群。LogCabin 目前不会在成员更改之前和之后自动更新 DNS 记录（这留给了管理脚本）。</li></ol><h3 id="6-2-将请求路由到领导者"><a href="#6-2-将请求路由到领导者" class="headerlink" title="6.2 将请求路由到领导者"></a>6.2 将请求路由到领导者</h3><p>Raft 中的客户端请求是通过领导者处理的，因此客户需要一种找到领导者的方法。客户端首次启动时，它将随机连接一个服务器。如果客户第一次选择的不是领导者，则该服务器将拒绝该请求。在这种情况下，一种非常简单的方法是让客户端与另一台随机选择的服务器再次尝试，直到找到领导者为止。如果客户端纯随机地选择服务器，那么这种简单的方法有望在 $\frac{n+1}{2}$ 次尝试后找到有 n 个服务器的集群领导者，这对于小型集群而言可能是足够快的。</p><p>通过简单的优化，也可以更快地将请求路由到领导者。服务器通常知道当前集群领导者的地址，因为 AppendEntries 请求包括领导者的身份。当不是领导者的服务器收到来自客户端的请求时，它可以执行以下两项操作之一：</p><ol><li>第一个选项，是我们建议的而且也是 LogCabin 中实现的，即非领导者服务器拒绝请求并将领导者的地址（如果知道）返回给客户端。这允许客户端直接重新连接到领导者，因此以后的请求可以全速处理。它也几乎不需要额外的代码来实现，因为客户端已经需要有当领导者故障时重连到另一个不同服务器的能力了。</li><li>或者，服务器可以将客户的请求代理给领导者。在某些情况下，这可能更简单。例如，如果客户端连接到任何服务器以进行读取请求（请参阅第 6.4 节），那么代理该客户端的写请求将使客户端不必管理与只用于写的领导者的不同连接。</li></ol><p>Raft 还必须防止过期领导者的信息无限期地延迟客户的请求。领导者信息可能在整个系统中，在领导者，跟随者和客户端中都会过期：</p><ul><li>领导者：服务器可能处于领导者状态，但是如果它不是当前领导者，则可能不必要地延迟了客户端请求。例如，假设一个领导者已和集群的其余部分进行了分区，但是它仍然可以与特定的客户端进行通信。如果没有其他机制，它可能会永远延迟来自该客户端的请求，无法将日志条目复制到任何其他服务器。期间可能还有另一个新任期的领导者能够与大多数集群成员通信，并且能够提交客户的请求。因此，如果没有在其大部分集群中成功进行一轮心跳，选举超时就结束了，Raft 领导者就会下台。这样，客户端可以通过另一台服务器重试其请求。</li><li>跟随者：跟随者保持跟踪领导者的身份，以便他们可以重定向或代理客户端的请求。他们在开始新的选举或更改任期时必须放弃此信息。否则，它们可能不必要地延迟客户端（例如，两台服务器可能会彼此重定向，从而使客户端陷入无限循环）。</li><li>客户端：如果客户端失去与领导者（或任何特定服务器）的连接，则应仅简单随机重试一个服务器。如果该服务器发生故障，则坚持联系最后一位已知的领导者将导致不必要的延迟。</li></ul><h3 id="6-3-实现线性化语义"><a href="#6-3-实现线性化语义" class="headerlink" title="6.3 实现线性化语义"></a>6.3 实现线性化语义</h3><p>到目前为止，Raft为客户端提供了 at-least-once 的语义。复制状态机可能会多次应用命令。例如，假设客户端向领导者提交命令，并且领导者将该命令附加到其日志中并提交日志条目，但是随后在响应客户端之前崩溃。由于客户端未收到确认，因此它将命令重新提交给新领导者，新领导者又将该命令作为新条目追加到其日志中并且也提交此新条目。尽管客户端希望该命令执行一次，但实际上它却执行了两次。如果网络导致客户端请求重复即使在没有客户端牵涉的情况下也会导致请求被应用多次。</p><p>这个问题不是 Raft 独有的。它发生在大多数有状态的分布式系统中。但是，这些 at-least-once 的语义特别不适用于基于共识的系统，在该系统中，客户通常需要更强的保证。来自重复命令的问题可能以微妙的方式表现出来，客户端很难从中恢复。这些问题会导致错误的结果，错误的状态或同时导致这两种情况。图 6.2 显示了一个错误结果的示例：状态机正在提供锁，并且客户端发现它无法获取锁，因为它的原始请求（未收到确认）已经获取了该锁。不正确状态的一个例子是增量操作，客户端将一个值打算增加1，但是却增加了2或更多。网络层级的重新排序和客户端并发可以导致更令人惊讶的结果。</p><blockquote><p><img src="/raft-thesis-translate/6_2.png" srcset="/img/loading.gif" alt><br>图 6.2：重复命令可能导致错误结果的示例。客户端向复制状态机提交命令以获取锁。客户端的第一个命令获得了锁，但客户端并未收到确认。当客户端重试该请求时，它将发现该锁已被获取。</p></blockquote><p>我们在 Raft 中的目标是实现线性化语义[34]，它避免了此类问题。在线性化语义中，每个操作似乎在调用和响应之间的某个时刻恰好执行一次。这是一种很强的一致性形式，客户可以轻松地进行推理，并且不允许多次处理命令。</p><p>为了在 Raft 中实现线性化语义，服务器必须过滤掉重复的请求。基本思想是服务器保存客户端操作的结果，并使用它们跳过重复的相同请求。为了实现这一点，每个客户端都被赋予唯一的标识符，并且客户端为每个命令分配唯一的序列号。每个服务器的状态机为每个客户端维护一个会话。该会话跟踪为客户端处理的最新序列号以及相关的响应。如果服务器接收到已经执行了序列号的命令，它将直接响应而无需重新执行该请求。</p><p>鉴于这种对重复请求的过滤，Raft 提供了线性化语义。Raft 日志提供在每个服务器上应用命令的串行顺序。命令会根据它们在 Raft 日志中的第一次出现而立即，精确的生效，因为如上所述，状态机会过滤掉所有后续出现。</p><p>该方法还普遍适用于允许来自单个客户端的并发请求。客户端的会话不只跟踪客户端最近的序号和响应，而是包含了一组序号和响应。客户端每次请求都将包括其尚未收到响应的最低序列号，然后状态机将丢弃所有比之较低的序列号响应。</p><p>不幸的是，由于空间有限，会话无法永远保持。服务器必须最终决定终止客户端的会话，但这会带来两个问题：服务器如何就何时终止客户端的会话达成共识，以及如何处理不幸过早过期的活跃客户端？</p><p>服务器必须就何时终止客户会话达成一致；否则，服务器的状态机可能会彼此分离。例如，假设一台服务器使特定客户端的会话到期，然后重新应用该客户端的许多重复命令；同时，其他服务器使会话保持活动状态，并且不应用重复项。复制的状态机将变得不一致。为避免此类问题，会话到期必须具有确定性，就像正常状态机操作必须一样。一种选择是设置会话数的上限，并使用 LRU（最近最少使用）策略删除条目。另一种选择是基于一个意见一致的时间源。在 LogCabin 中，领导者会对 Raft 日志的每个命令增加一个当前时间的属性。作为提交日志的一部分，服务器在这个时间上达成一致；然后，状态机使用确定时间来过期不活跃的会话。活跃客户端会在不活跃期间发出 keep-alive 的请求，该请求还会增加领导者的时间戳并提交到 Raft 日志中，以保持会话状态。</p><p>第二个问题是如何处理会话期满后继续运行的客户端。我们希望这是一种异常情况；但是总会有一些风险，因为通常没有办法知道客户端何时退出。一种选择是在没有会话记录时为客户端分配一个新会话，但这会冒重复执行在客户端上一个会话过期之前执行的命令的风险。为了提供更严格的保证，服务器需要将新客户端与会话已过期的客户端区分开。客户端首次启动时，可以使用 RegisterClient RPC 在集群中注册自己。这将分配新客户端的会话，并向客户端返回其标识符，客户端随后的所有命令都要带上这个标识符。如果状态机遇到一个没有会话记录的命令，则它不处理该命令而是向客户端返回错误。在这种情况下，LogCabin 当前会导致客户端崩溃（大多数客户端可能无法正确优雅地处理会话过期错误，但系统通常能够处理客户端崩溃）。</p><h3 id="6-4-高效处理只读查询"><a href="#6-4-高效处理只读查询" class="headerlink" title="6.4 高效处理只读查询"></a>6.4 高效处理只读查询</h3><p>只读的客户端命令仅查询而不会改变复制的状态机。因此，一个很自然的想法就是这些查询是否可以绕过 Raft 日志，Raft 日志的目的是将更改以相同顺序复制到服务器的状态机上。绕过日志具有明显的性能优势：只读查询在许多应用程序中很常见，并且将条目追加到日志所需的同步磁盘写操作非常耗时。</p><p>然而，如果没有其他预防措施，则绕开日志可能会导致只读查询的结果过时。例如，领导者可能与集群的其余部分分开了，集群的其余部分可能已经选出了新的领导者并将新条目提交到了 Raft 日志中。如果分区的领导者在不咨询其他服务器的情况下响应了只读查询，则将返回过时的结果，这就不是线性化了。线性化语义要求读取的结果反映在读取启动后的某个时刻系统的状态。每次读取必须至少返回最新提交的写入结果。（允许陈旧读取的系统只能提供串行化，这是较弱的一致性形式。）在两个第三方 Raft 实现中已经发现了由于陈旧读取导致的问题，因此应特别注意此问题。</p><p>幸运的是，只读请求是可以绕过 Raft 日志并且提供线性化语义的。为此，领导者采取以下步骤：</p><ol><li>如果领导者尚未将其当前任期的条目标记为已提交，则它将等待直到它已经完成为止。领导者完整性属性可以保证领导者拥有所有已提交的条目，但是在任期开始之初，它可能不知道这些是谁。为了找出答案，它需要从其任期中提交一个条目。Raft 通过让每位领导者在任期开始时在日志中输入一个空白的禁止操作条目来解决此问题。提交此空操作条目后，领导者的 commitIndex 将在其任期内至少与其他任何服务器一样大。</li><li>领导者将其当前 commitIndex 保存在局部变量 readIndex 中。这将用作查询所针对的状态版本的下限。</li><li>领导者需要确保自己没有被不知道的新领导者所取代。它发出新一轮的心跳，并等待大多数集群成员的确认。收到这些确认后，一旦收到这些确认，领导者知道在发送心跳的那一刻不可能有一个比起任期大的领导者。因此，当时的 readIndex 是集群中任何服务器看到的最大提交索引。</li><li>领导者等待其状态机至少应用到 readIndex；这足够满足线性化要求。</li><li>最后，领导者针对其状态机发出查询，并向客户端回复结果。</li></ol><p>这种方法比将提交一个只读查询的条目到日志中更有效，因为它避免了同步磁盘写入。为了进一步提高只读查询的性能，领导者可以分摊确认其领导权的成本：可以通过一轮心跳确认累积的任意数量只读查询。</p><p>跟随者还可以帮助分担只读查询的处理。这样可以提高系统的读取吞吐量，也可以转移领导者的负载，从而使领导者可以处理更多的读写请求。但是，如果不采取其他预防措施，这些读取操作也将具有返回过期数据的风险。例如，一个分区的跟随者可能很长一段时间没有收到来自领导者的任何新日志条目，或者即使跟随者从领导者那里得到了心跳，该领导者本身在不知情的情况下也可能被罢免。为了安全地进行读取，跟随者可以发起一个请求到领导者询问当前的 readIndex（领导者将执行上述步骤 1-3）；然后，跟随者可以在其自己的状态机上针对任何数量的累积只读查询执行步骤 4 和 5。</p><p>LogCabin 在领导者上实现了上述算法，并且在高负载的情况下通过积攒多个只读查询来分摊开销。 LogCabin 中的跟随者目前不提供只读请求。</p><h4 id="6-4-1-使用时钟减少只读查询的消息"><a href="#6-4-1-使用时钟减少只读查询的消息" class="headerlink" title="6.4.1 使用时钟减少只读查询的消息"></a>6.4.1 使用时钟减少只读查询的消息</h4><p>到目前为止，提出的只读查询方法已在异步模型（时钟，处理器和消息都可以任意速度运行）中提供了线性化语义。这个安全级别需要通信才能实现：对于每一批只读查询，它需要一轮心跳到一半集群，这会增加查询的延迟。本节的其余部分探讨了一种替代方法，其中只读查询将通过依靠时钟来避免发送消息。LogCabin 当前没有实现此替代方案，并且我们不建议使用，除非有性能需求。</p><p>为了对只读查询使用时钟代替信息交互，正常的心跳机制需要提供一种租约形式。一旦集群的大多数成员都认可了领导者的心跳，领导者将假定在选举超时期间没有其他服务器将成为领导者，并且可以相应地延长其租约（见图 6.3）。然后，领导者将在此期间将直接回复只读查询而无需任何其他通信。（第 3 章介绍的领导权禅让机制允许尽早的更换领导者；领导者在禅让领导权之前需要终止其租约。）</p><blockquote><p><img src="/raft-thesis-translate/6_3.png" srcset="/img/loading.gif" alt><br>图 6.3：要使用时钟而不是消息进行只读查询，领导者将使用正常的心跳机制来维持租约。一旦领导者的心跳得到大多数集群成员的认可，它将延长租约到 $start + \frac{election\ timeout}{clock\ drift\ bound}$，因为时钟漂移限制了跟随者在此之前不应该超时。当领导者保留其租约时，它将为只读查询提供服务而无需进行通信。</p></blockquote><p>租约方法假设跨服务器的时钟漂移是有界的（在给定的时间段内，没有任何服务器的时钟增加超过这个限制的时间）。发现和维护这个界限可能会带来操作上的巨大挑战（例如，由于调度和垃圾收集中断，虚拟机迁移或用于时间同步的时钟速率调整）。如果违反了这些假设，则系统可能会返回任意过期的信息。</p><p>幸运的是，一个简单的扩展可以改善提供给客户端的保证，因此即使在异步假设下（即使时钟会出错），每个客户端也将看到复制的状态机单调进度（顺序一致性）。例如，客户端将看不到日志索引 n 的状态，然后切换到其他服务器仅看到了日志索引 n - 1 的状态。为实现此保证，服务器将包括与状态机状态相对应的索引到回复客户端的响应中。客户端将跟踪他们看到的结果相对应的最新索引，并在每次请求时将这些信息提供给服务器。如果服务器收到的客户端请求的索引大于服务器上次应用的日志索引的索引，则服务器不会为该请求提供服务。</p><h3 id="6-5-结论"><a href="#6-5-结论" class="headerlink" title="6.5 结论"></a>6.5 结论</h3><p>本章讨论了客户端如何与 Raft 交互的几个问题。就正确性而言，提供可线性化语义和优化只读查询的问题尤其微妙。当前的共识性文献都是讲集群服务器间的沟通，没有涉及这些重要的问题。我们认为这是一个错误。完整的系统必须与客户端正确交互，否则核心共识算法提供的一致性级别将被浪费。正如我们已经在基于 Raft 的真实系统中看到的那样，客户端交互可能是错误的主要来源，但是我们希望更好地了解这些问题以帮助防止将来出现问题。</p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>一致性协议</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 协议介绍</title>
    <link href="/raft/"/>
    <url>/raft/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="共识算法"><a href="#共识算法" class="headerlink" title="共识算法"></a>共识算法</h3><p>共识算法允许一组节点像一个整体一样一起工作，即使其中一些节点出现故障也能够继续工作下去。更详细点，可以认为每一个节点上都运行着一个状态机和一组日志。在客户端提交命令后，状态机负责执行命令并返回结果，同时还可能改变自己的状态；日志则记录了所有可能会影响到状态机的命令。任何一个处于初始状态的状态机都可以通过重新按顺序执行这组日志，来让自己恢复到最终状态。</p><p>共识算法常被用来确保每一个节点上的状态机一定都会按相同的顺序执行相同的命令， 并且最终会处于相同的状态。换句话说，可以理解为共识算法就是用来确保每个节点上的日志顺序都是一致的。（不过需要注意的是，只确保“提交给状态机的日志”顺序是一致的，而有些日志项可能只是暂时添加，尚未决定要提交给状态机）。正因为如此，共识算法在构建可容错的大规模分布式系统中扮演着重要的角色。</p><p><img src="/raft/rsm.png" srcset="/img/loading.gif" alt></p><p>上图就是每个节点的状态机，日志模块，共识模块与客户端交互的过程。</p><p>当然，实际使用系统中的共识算法一般满足以下特性：</p><ul><li>在非拜占庭条件（无恶意欺骗）下保证共识的一致性；</li><li>在多数节点存活时，保持可用性；</li><li>不依赖于时间，错误的时钟和高延迟只会导致可用性问题，而不会导致一致性问题；</li><li>在多数节点一致后就返回结果，而不会受到个别慢节点的影响。<br>（注：“多数”永远指的是配置文件中所有节点的多数，而不是存活节点的多数）<br>（注：非拜占庭条件，指的就是每一个节点都是诚实可信的，每一次信息的传递都是真实的且符合协议要求的，当节点无法满足协议所要求的条件时，就停止服务，节点仅会因为网络延迟或崩溃出现不一致，而不会有节点传递错误的数据或故意捏造假数据。）</li></ul><h3 id="Raft-的由来与宗旨"><a href="#Raft-的由来与宗旨" class="headerlink" title="Raft 的由来与宗旨"></a>Raft 的由来与宗旨</h3><p>众所周知，Paxos 是一个非常划时代的共识算法。在 Raft 出现之前的 10 年里，Paxos 几乎统治着共识算法这一领域：因为绝大多数共识算法的实现都是基于 Paxos 或者受其影响，同时 Paxos 也成为了教学领域里讲解共识问题时的示例。</p><p>但是不幸的是，尽管有很多工作都在尝试降低 Paxos 的复杂性，但是它依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。比如 <code>Google Chubby</code> 的论文就提到，因为 Paxos 的描述和现实差距太大，所以最终人们总会实现一套未经证实的类 Paxos 协议。</p><p>基于以上背景，<code>Diego Ongaro</code> 在就读博士期间，深入研究 Paxos 协议后提出了 Raft 协议，旨在提供更为易于理解的共识算法。Raft 的宗旨在于可实践性和可理解性，并且相比 Paxos 几乎没有牺牲多少性能。</p><blockquote><p>趣闻：<a href="https://groups.google.com/forum/#!topic/raft-dev/95rZqptGpmU" target="_blank" rel="noopener">Raft 名字的来源</a>。简而言之，其名字即来自于 <code>R{eliable|plicated|dundant} And Fault-Tolerant</code>， 也来自于这是一艘可以帮助你逃离 Paxos 小岛的救生筏（Raft）。</p></blockquote><h3 id="工业界的实现"><a href="#工业界的实现" class="headerlink" title="工业界的实现"></a>工业界的实现</h3><ul><li><code>Tidb</code></li><li><code>Consul</code></li><li><code>etcd</code></li><li>…</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>这一部分会简单介绍 Raft 的一些基本概念。若暂时没看懂并没有关系，后面会一一介绍清楚，带着问题耐心读完此博客即可。</p><h3 id="Raft-的子问题"><a href="#Raft-的子问题" class="headerlink" title="Raft 的子问题"></a>Raft 的子问题</h3><p>Raft 将共识算法这个难解决的问题分解成了多个易解决，相对独立的子问题，这些问题都会在接下来的章节中进行介绍。</p><ul><li><code>选主</code>：选出集群的 leader 来统筹全局。</li><li><code>日志同步</code>：leader 负责从客户端接收请求，并且在集群中扩散同步。</li><li><code>安全</code>：各节点间状态机的一致性保证。</li></ul><p>在完整的论文和 etcd 实现中，其实还有一些问题：</p><ul><li><code>配置变更</code>：集群动态增删节点。</li><li><code>禅让</code>：能够将 leader 迅速转给另一个 follower。</li><li><code>Pre-Vote</code>：在竞选开始时先进行一轮申请，若被允许再转变为 candidate，这样有助于防止某些异常节点扰乱整个集群的正常工作。</li><li>…</li></ul><h3 id="Raft-的节点类型"><a href="#Raft-的节点类型" class="headerlink" title="Raft 的节点类型"></a>Raft 的节点类型</h3><p>Raft 将所有节点分为三个身份：</p><ul><li><code>leader</code>：集群内最多只会有一个 leader，负责发起心跳，响应客户端，创建日志，同步日志。</li><li><code>candidate</code>：leader 选举过程中的临时角色，由 follower 转化而来，发起投票参与竞选。</li><li><code>follower</code>：接受 leader 的心跳和日志同步数据，投票给 candidate。</li></ul><p><img src="/raft/state.png" srcset="/img/loading.gif" alt></p><p>上图可以看出 Raft 中节点状态之间变迁的条件。</p><p>在完整的论文和 etcd 实现中，其实又增加了几种中间状态：</p><ul><li><code>Learner</code>：新加入的节点，不具有选举权，需要从 leader 同步完数据后， 才能转变为 follower。严格来说，learner 并不算集群成员。</li><li><code>Pre-Candidate</code>：刚刚发起竞选，还在等待 <code>Pre-Vote</code> 结果的临时状态， 取决于 <code>Pre-Vote</code> 的结果，可能进化为 candidate，可能退化为 follower。<br>（注：此两种节点状态的流程和作用后面章节会介绍，可先无视此两个状态）</li></ul><h3 id="Raft-的节点状态"><a href="#Raft-的节点状态" class="headerlink" title="Raft 的节点状态"></a>Raft 的节点状态</h3><p>每一个节点都应该有的持久化状态：</p><ul><li><code>currentterm</code>：当前任期。</li><li><code>votedFor</code>：在当前 term，给哪个节点投了票，值为 NULL 或 <code>candidate id</code>。</li><li><code>log[]</code>：已经 committed 的日志。</li></ul><p>每一个节点都应该有的可以非持久化的状态：</p><ul><li><code>commitindex</code>：已提交的最大 index。</li><li><code>lastApplied</code>：已被状态机应用的最大 index。<br>（注：这两个不需要持久化是因为状态机本身是非持久化的，而状态机的状态可以通过 log[] 来恢复）</li></ul><p>leader 的非持久化状态：</p><ul><li><code>nextindex[]</code>：为每一个 follower 保存的，应该发送的下一份 <code>entry index</code>；<br>初始化为 Last index + 1。</li><li><code>matchindex[]</code>：已确认的，已经同步到每一个 follower 的 <code>entry index</code><br>初始化为 0，单调递增。<br>（注：每次选举后，都应该立刻重新初始化）</li></ul><h3 id="Raft-的任期概念"><a href="#Raft-的任期概念" class="headerlink" title="Raft 的任期概念"></a>Raft 的任期概念</h3><p><img src="/raft/term.png" srcset="/img/loading.gif" alt></p><p>Raft 将时间划分成为任意不同长度的 term。term 用连续的数字进行表示。每一个 term 的开始都是一次选举，一个或多个 candidate 会试图成为 leader。如果一个  candidate 赢得了选举，它就会在该 term 的剩余时间担任 leader。在某些情况下，选票会被瓜分，有可能没有选出 leader，那么，将会开始另一个 term，并且立刻开始下一次选举。Raft 保证在给定的一个 term 最多只有一个 leader。</p><p>不同的服务器节点可能多次观察到 term 之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个 term 全程。term 在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如过期的 leader。</p><p>每个节点都会存储当前 term 号，这一编号在整个时间内单调增长。当服务器之间通信的时候会交换当前 term 号；如果一个服务器的当前 term 号比其他人小，那么他会更新自己的 term 到较大的 term 值。如果一个 candidate 或者 leader 发现自己的 term 过期了，那么他会立即退回 follower。如果一个节点接收到一个包含过期 term 号的请求，那么它会直接拒绝这个请求。</p><h3 id="Raft-的日志组成"><a href="#Raft-的日志组成" class="headerlink" title="Raft 的日志组成"></a>Raft 的日志组成</h3><ul><li><p><code>entry</code>：Raft 中，将每一个事件都称为一个 entry，每一个 entry 都有一个表明它在 log 中位置的 index（之所以从 1 开始是为了方便 <code>prevLogIndex</code> 从 0 开始）。只有 leader 可以创建 entry。entry 的内容为 <code>&lt;term, index, cmd&gt;</code>，其中 cmd 是可以应用到状态机的操作。被提交给状态机后，entry 被称为是 committed 的。</p></li><li><p><code>log</code>：由 entry 构成的数组，只有 leader 可以改变其他节点的 log。 entry 总是先被添加进 log（写操作都应该立刻持久化），然后才发起共识请求，通过后才会被 leader 提交给状态机。follower 只能从 leader 那获取到当前已经 commit 日志的最大索引号，然后应用到自己的状态机。</p></li></ul><h3 id="Raft-的保证"><a href="#Raft-的保证" class="headerlink" title="Raft 的保证"></a>Raft 的保证</h3><ul><li><code>Election Safety</code>：最多只会有一个 leader。</li><li><code>Leader Append-Only</code>：leader 的日志是只增的。</li><li><code>Log Matching</code>：如果两个节点的日志中有两个 entry 有相同的 index 和 term，那么它们就是相同的 entry。</li><li><code>Leader Completeness</code>：一旦一个操作被提交了，那么在之后的 term 中，该操作都会存在于日志中。</li><li><code>State Machine Safety</code>：一致性，一旦一个节点应用了某个 index 的操作到状态机，那么其他所有节点应用的该 index 的操作都是一致的。</li></ul><h2 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h2><p>Raft 使用心跳来维持 leader 身份。任何节点都以 follower 的身份启动。 leader 会定期的发送心跳给所有的 followers 以确保自己的身份。 每当 follower 收到心跳后，就刷新自己的 electionElapsed，重新计时。</p><p>（后文中，会将预设的选举超时称为 electionTimeout，而将当前经过的选举耗时称为 electionElapsed。）</p><p>一旦一个 follower 在指定的时间内没有收到任何 RPC（称为 electionTimeout），则会发起一次选举。 当 follower 试图发起选举后，其身份转变为 candidate，在增加自己的 term 后， 会向所有节点发起 RequestVoteRPC 请求，candidate 的状态会一直持续直到：</p><ul><li>赢得选举</li><li>其他节点赢得选举</li><li>一轮选举结束，无人胜出</li></ul><p>选举的方式非常简单，谁能获取到多数选票 <code>(N/2 + 1)</code>，谁就成为 leader。 在一个 candidate 节点等待投票响应的时候，它有可能会收到其他节点声明自己是 leader 的心跳， 此时有两种情况：</p><ul><li>该请求的 term 和自己一样或更大：说明对方已经成为 leader，自己立刻退为 follower；</li><li>该请求的 term 小于自己：拒绝请求。</li></ul><p>在 etcd 的实现中，如果 candidate 收到 term 大于自己的 RequestVote，也会退为 follower。 （准确的说是，收到一切 term 更大的，除了 RequestPreVote、PreVoteResp 外的所有消息，都退为 follower）。</p><p>follower 收到 candidate 的 RequestVote 后，会检查自己是否已经投过票。 不过如果来源于同一个 candidate，那么 follower 可以在同一 term 内多次投给同一个 candidate。</p><pre><code class="hljs GO"><span class="hljs-comment">// 检查自己是否已经投过票了，如果投票请求来自同一节点，可以重复投票。</span><span class="hljs-comment">// We can vote if this is a repeat of a vote we've already cast...</span>canVote := r.Vote == m.From ||    <span class="hljs-comment">// ...we haven't voted and we don't think there's a leader yet in this term...</span>    (r.Vote == None &amp;&amp; r.lead == None) ||    <span class="hljs-comment">// ...or this is a PreVote for a future term...</span>    (m.Type == pb.MsgPreVote &amp;&amp; m.Term &gt; r.Term)</code></pre><p>为了防止在同一时间有太多的 follower 转变为 candidate 导致无法选出绝对多数， Raft 采用了随机选举超时（<code>randomized election timeouts</code>）的机制， 每一个 candidate 在发起选举后，都会记录一个选举超时（在 <code>150-300ms</code> 间）， 一旦超时后仍然没有完成选举，则增加自己的 term，然后发起新一轮选举。 在这种情况下，应该能在较短的时间内确认出 leader。 （因为 term 较大的有更大的概率压倒其他节点）</p><p>etcd 中将随机选举超时设置为 <code>[electiontimeout, 2 * electiontimeout - 1]</code>。</p><p>如果一个 leader 在 electionTimeout 内无法完成一次多数节点的 heartbeat， 说明该 leader 很可能已经与集群失去联系了，那么该 leader 应该向所有的客户端请求返回 fail， 并且退回到 follower。</p><h2 id="日志同步"><a href="#日志同步" class="headerlink" title="日志同步"></a>日志同步</h2><p>leader 被选举后，则负责所有的客户端请求。每一个客户端请求都包含一个命令，该命令可以被作用到 RSM。</p><p>leader 收到客户端请求后，会生成一个 entry，包含 <index, term number, cmd>。 在将这个 entry 添加到自己的日志末尾后，向所有的节点广播该 entry。</index,></p><p>follower 如果同意接受该 entry，则在将 entry 添加到自己的日志后，返回同意。</p><p>如果 leader 收到了多数的成功答复，则将该 entry 应用到自己的 RSM， 之后可以称该 entry 是 committed 的。该 committed 信息会随着 AppendEntriesRPC 被传达到其他节点。</p><p><img src="/raft/log.png" srcset="/img/loading.gif" alt></p><p>Raft 保证下列两个性质：</p><p>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd；<br>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们前面的 entry 也一定相同。<br>通过“仅有 leader 可以生成 entry”来确保第一个性质， 第二个性质则通过一致性检查（consistency check）来保证，该检查包含几个步骤：</p><p>leader 在通过 AppendEntriesRPC 和 follower 通讯时，会带上上一块 entry 的信息， 而 follower 在收到后会对比自己的日志 ， 如果发现这个 entry 的信息（index、term）和自己日志内的不符合，则会拒绝该请求。<br>一旦 leader 发现有 follower 拒绝了请求，则会与该 follower 再进行一轮一致性检查， 找到双方最大的共识点，然后用 leader 的 entries 记录覆盖 follower 所有在最大共识点之后的数据。</p><p>寻找共识点时，leader 还是通过 AppendEntriesRPC 和 follower 进行一致性检查， 方法是发送再上一块的 entry， 如果 follower 依然拒绝，则 leader 再尝试发送更前面的一块，直到找到双方的共识点。 因为分歧发生的概率较低，而且一般很快能够得到纠正，所以这里的逐块确认一般不会造成性能问题。<br>每个 leader 都会为每一个 follower 保存一个 nextIndex 的变量， 标志了下一个需要发送给该 follower 的 entry 的 index。 在 leader 刚当选时，该值初始化为该 leader 的 log 的 index+1。 一旦 follower 拒绝了 entry，则 leader 会执行 nextIndex—，然后再次发送。</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><h3 id="选举限制"><a href="#选举限制" class="headerlink" title="选举限制"></a>选举限制</h3><p>因为 leader 的强势地位，所以 Raft 在投票阶段就确保选举出的 leader 一定包含了整个集群中目前已 committed 的所有日志。</p><p>当 candidate 发送 RequestVoteRPC 时，会带上最后一个 entry 的信息。 所有的节点收到该请求后，都会比对自己的日志，如果发现自己的日志更新一些，则会拒绝投票给该 candidate。 （Pre-Vote 同理，如果 follower 认为 Pre-Candidate 没有资格的话，会拒绝 PreVote）</p><p>判断日志新旧的方式：获取请求的 entry 后，比对自己日志中的最后一个 entry。 首先比对 term，如果自己的 term 更大，则拒绝请求。 如果 term 一样，则比对 index，如果自己的 index 更大（说明自己的日志更长），则拒绝请求。</p><pre><code class="hljs GO"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(l *raftLog)</span> <span class="hljs-title">isUpToDate</span><span class="hljs-params">(lasti, term <span class="hljs-keyword">uint64</span>)</span> <span class="hljs-title">bool</span></span> &#123;<span class="hljs-keyword">return</span> term &gt; l.lastTerm() || (term == l.lastTerm() &amp;&amp; lasti &gt;= l.lastIndex())&#125;</code></pre><p><img src="/raft/leader_restriction.png" srcset="/img/loading.gif" alt></p><p>在上图中，raft 为了避免出现一致性问题，要求 leader 绝不会提交过去的 term 的 entry （即使该 entry 已经被复制到了多数节点上）。leader 永远只提交当前 term 的 entry， 过去的 entry 只会随着当前的 entry 被一并提交。（上图中的 c，term2 只会跟随 term4 被提交。）</p><p>如果一个 candidate 能取得多数同意，说明它的日志已经是多数节点中最完备的， 那么也就可以认为该 candidate 已经包含了整个集群的所有 committed entries。</p><p>因此 leader 当选后，应该立刻发起 AppendEntriesRPC 提交一个 no-op entry。</p><h3 id="节点崩溃"><a href="#节点崩溃" class="headerlink" title="节点崩溃"></a>节点崩溃</h3><p>如果 leader 崩溃，集群中的所有节点在 electionTimeout 时间内没有收到 leader的心跳信息就会触发新一轮的选主。总而言之，最终集群总会选出唯一的 leader 。按论文中的说法，即使一次RPC高达 <code>30～40ms</code> 时，<code>99.9%</code> 的选举依然可以在 <code>3s</code> 内完成，但一般一个机房内一次 RPC 只需 1ms。当然，选主期间整个集群对外是不可用的。 </p><p>如果 follower 和 candidate 奔溃相对而言就简单很多， 因为 Raft 所有的 RPC 都是幂等的，所以 Raft 中所有的请求，只要超时，就会无限的重试。follower 和 candidate 崩溃恢复后，可以收到新的请求，然后按照上面谈论过的追加或拒绝 entry 的方式处理请求。</p><h3 id="时间与可用性"><a href="#时间与可用性" class="headerlink" title="时间与可用性"></a>时间与可用性</h3><p>Raft 原则上可以在绝大部分延迟情况下保证一致性， 不过为了保证选择和 leader 的正常工作，最好能满足下列时间条件：</p><pre><code class="hljs armasm"><span class="hljs-keyword">broadcastTime </span>&lt;&lt; electionTimeout &lt;&lt; MTBF</code></pre><ul><li><code>broadcastTime</code>：向其他节点并发发送消息的平均响应时间；</li><li><code>electionTimeout</code>：follower 判定 leader 已经故障的时间（heartbeat 的最长容忍间隔）；</li><li><code>MTBF(mean time between failures)</code>：单台机器的平均健康时间；</li></ul><p>一般来说，broadcastTime 一般为 <code>0.5～20ms</code>（需要磁盘持久化），MTBF 一般为一两个月， electionTimeout 可以设置为 <code>10～500ms</code>。</p><h2 id="配置更改"><a href="#配置更改" class="headerlink" title="配置更改"></a>配置更改</h2><h3 id="一次变更一台"><a href="#一次变更一台" class="headerlink" title="一次变更一台"></a>一次变更一台</h3><h4 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h4><p>因为在 Raft 算法中，集群中每一个节点都存有整个集群的信息，而集群的成员有可能会发生变更（节点增删、替换节点等）。 Raft 限制一次性只能增／删一个节点，在一次变更结束后，才能继续进行下一次变更。</p><p>如果一次性只变更一个节点，那么只需要简单的要求“在新／旧集群中，都必须取得多数（N/2+1）”， 那么这两个多数中必然会出现交集，这样就可以保证不会因为配置不一致而导致脑裂。</p><p><img src="/raft/singlechange.png" srcset="/img/loading.gif" alt></p><p>当 leader 收到集群变更的请求后，就会生成一个特殊的 entry 项用来保存配置， 在将配置项添加到 log 后，该配置立刻生效（也就是说任何节点在收到新配置后，就立刻启用新配置）。 然后 leader 将该 entry 扩散至多数节点，成功后则提交该 entry。 一旦一个新配置项被 committed，则视为该次变更已结束，可以继续处理下一次变更了。</p><p>为了保证可用性，需要新增一项规则，节点在响应 RPC 时，不考虑来源节点是否在自己的配置文件之中。 也就是说，即使收到了一个并不在自己配置文件之中的节点发来的 RPC， 也需要正常处理和响应，包括 AppendEntriesRPC 和 RequestVoteRPC。</p><h3 id="一次变更多台"><a href="#一次变更多台" class="headerlink" title="一次变更多台"></a>一次变更多台</h3><h4 id="方式-1"><a href="#方式-1" class="headerlink" title="方式"></a>方式</h4><p>这种变更方式可以一次性变更多个节点（arbitrary configuration）。</p><p>当集群成员在变更时，为了保证服务的可用性（不发生中断），以及避免因为节点变更导致的一致性问题， Raft 提出了两阶段变更，当接收到新的配置文件后，集群会首先进入 joint consensus 状态， 待新的配置文件提交成功后，再回到普通状态。</p><p>更具体的，joint consensus 指的是包含新／旧配置文件全部节点的中间状态：</p><ul><li>entries 会被复制到新／旧配置文件中的所有节点；</li><li>新／旧配置文件中的任何一个节点都有可能被选为 leader；</li><li>共识（选举或提交）需要同时在新／旧配置文件中分别获取到多数同意（<code>separate majorities</code>）</li></ul><p>（注：<code>separate majorities</code>的意思是需要新／旧集群中的多数都同意。比如如果是从 3 节点切换为全新的 9 节点， 那么要求旧节点中的 2 节点，和新节点中的 4 节点都同意，才被认为达成了一次共识。）</p><p>所以，在一次配置变更中，一共有三个状态：</p><ul><li><code>C_old</code>：使用旧的配置文件；</li><li><code>C_old,new</code>：同时使用新旧配置文件，也就是新／旧节点的并集；</li><li><code>C_new</code>：使用新的配置文件。</li></ul><p>配置文件使用特殊的 entries 进行存储，一个节点一旦获取到新的配置文件， 即使该配置 entry 并没有 committed，也会立刻使用该配置。 所以一次完整的配置变更可以表示为下图：</p><p><img src="/raft/jointchange.png" srcset="/img/loading.gif" alt></p><ol><li>C_old,new 被创建，集群进入 joint consensus，leader 开始传播该 entry；</li><li>C_old,new 被 committed，也就是说此时多数节点都拥有了 C_old,new，此后 C_old 已经不再可能被选为 leader；</li><li>leader 创建并传播 C_new；</li><li>C_new 被提交，此后不在 C_new 内的节点不允许被选为 leader，如有 leader 不在 C_new 则自行退位。</li></ol><h4 id="删除当前节点的有趣现象"><a href="#删除当前节点的有趣现象" class="headerlink" title="删除当前节点的有趣现象"></a>删除当前节点的有趣现象</h4><p>leader 可能不在新配置文件的节点之中</p><p>在原始论文中，leader 会持续工作直到 C_new 被提交。</p><p>当 C_new 被 committed 后，任何不在 C_new 中的 leader，立刻退化为 follower。 在试图提交 C_new 时，不在 C_new 的 leader 不参与计票。</p><p>在其博士论文中，当一个 leader 发现自己不在新配置文件中，在 C_new 提交后， 可以采取 3.10 节（p46）提到的 leadership transfer 机制，交出自己的管理权。</p><p>上述两种做法都导致两个很奇特的现象：</p><ul><li>这个 leader 可能会管理一个不包括自己的集群；</li><li>一个服务器可能在自己的配置文件都不包含自己的情况下参与选举，甚至成为 leader。</li></ul><h2 id="日志打包"><a href="#日志打包" class="headerlink" title="日志打包"></a>日志打包</h2><p>当日志 entries 数量过多时，节点间同步会耗费太多时间，最简单的优化办法就是定期做 snapshot。</p><p>snapshot 会包括：</p><ul><li>状态机当前的状态；</li><li>最后一块 entry 的 index 和 term（为了兼容其他 RPC 请求的参数）；<br>当前集群配置信息。</li><li>各个节点自行择机完成自己的 snapshot。</li></ul><p>如果 leader 发现需要发给某一个 follower 的 nextIndex 已经被做成了 snapshot， 则需要将 snapshot 发送给该 follower。</p><p>当 follower 接收到 snapshot 后，需要做出判断：</p><ul><li>如果 snapshot 领先于自己的 log，则使用 snapshot 完全替换自己的所有的 log；</li><li>如果 snapshot 落后于自己的 log，则使用 snapshot 替换掉该部分的 log，而保留后续的 log。</li></ul><p>snapshot 可能会带来两个问题：</p><ol><li><p>何时 snapshot？<br>一个简单的策略是设置一个固定的最大磁盘容量，当 log 超过这个容量时，就触发 snapshot。</p></li><li><p>对状态机写 snapshot 时，会影响新的更新操作。<br>建议采用 <code>copy-on-write</code> 操作，来尽可能少的影响新的更新操作。</p></li></ol><h2 id="禅让"><a href="#禅让" class="headerlink" title="禅让"></a>禅让</h2><p>有时候，会希望取消当前 leader 的管理权，比如：</p><ul><li>leader 节点因为运维原因需要重启；</li><li>有其他更适合当 leader 的节点；</li></ul><p>直接将 leader 节点停机的话，其他节点会等待 electionTimeout 后进入选举状态， 这期间会集群会停止响应。为了避免这一段不可用的时间，可以采用禅让机制（<code>leadership transfer</code>）。</p><p>禅让的步骤为：</p><ol><li>leader 停止响应客户端请求；</li><li>leader 向 target 节点发起一次日志同步；</li><li>leader 向 target 发起一次 TimeoutNowRPC，target 收到该请求后立刻发起一轮投票。</li></ol><p>etcd 中实现了更多的细节（也有一些改动）：</p><ol><li>leader 先检查禅让对象（leadTransferee）的身份，如果是 follower，直接忽略；</li><li>leader 检查是否有正在进行的禅让，如果有，则中止之前的禅让状态，开始处理最新的请求；</li><li>检查禅让对象是否是自己，如果是，忽略；</li><li>将禅让状态信息计入 leader 的状态，并且重置 electionElapsed（因为禅让应该在 electionTimeout 内完成）；</li><li>检查禅让对象的日志是否是最新的</li><li>如果禅让对象已经是最新，则直接发送 TimeoutNowRPC</li><li>如果不是，则发送 AppendEntriesRPC，待节点响应成功后，再发送 TimeoutNowRPC</li></ol><p>可以看出，在 etcd 中，leader 除了重置 electionElapsed 外，不会改动自己的状态。 既不会停止对客户端的响应，同时还会继续发送心跳。</p><p>因为 target 机器会更新自己的 term，而且率先发起投票，其有很大的概率赢得选举。 需要注意的是，target 发起的 RequestVoteRPC 中的 <code>isLeaderTransfer=true</code>， 以防止被其他节点忽略。</p><p>如果 target 机器没能在一次 electionTimeout 内完成选举，那么 leader 认为本次禅让失败， 立刻恢复响应客户端的请求。（这时可以再次重新发起一次禅让请求）</p><p>在 etcd/raft 中，RequestVoteRPC.context 会被设置为 campaignTransfer, 表明本次投票请求来源于 leader transfer，可以强行打断 follower 的租约发起选举。</p><h2 id="预投票"><a href="#预投票" class="headerlink" title="预投票"></a>预投票</h2><p>一个暂时脱离集群网络的节点，在重新加入集群后会干扰到集群的运行。</p><p>因为当一个节点和集群失去联系后，在等待 electionTimeout 后，它就会增加自己的 term 并发起选举， 因为联系不上其他节点，所以在 electionTimeout 后，它会继续增加自己的 term 并继续发起选举。</p><p>一段时间以后，它的 term 就会显著的高于原集群的 term。如果此后该节点重新和集群恢复了联络， 它的高 term 会导致 leader 立刻退位，并重新举行选举。</p><p>为了避免这一情形，引入了 Pre-Vote 的机制。在该机制下，一个 candidate 必须在获得了多数赞同的情形下， 才会增加自己的 term。一个节点在满足下述条件时，才会赞同一个 candidate：</p><ul><li>该 candidate 的日志足够新；</li><li>当前节点已经和 leader 失联（electionTimeout）。</li></ul><p>也就是说，candidate 会先发起一轮 Pre-Vote，获得多数同意后，更新自己的 term， 再发起一轮 RequestVoteRPC。</p><p>这种情形下，脱离集群的节点，只会不断的发起 Pre-Vote，而不会更新自己的 term。</p><p>在 etcd 的实现中，如果某个节点赞同了某个 candidate， 是不需要更新自己的状态的，它依然可以赞同其他 candidate。 而且，即使收到的 PreVote 的 term 大于自己，也不会更新自己的 term。 也就是说，PreVote 不会改变其他节点的任何状态。</p><p>etcd 中还有一个设计是，当发起 PreVote 的时候，针对的是下一轮的 term， 所以会向所有的节点发送一个 term+1 的 PreVoteReq。</p><pre><code class="hljs GO"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *raft)</span> <span class="hljs-title">campaign</span><span class="hljs-params">(t CampaignType)</span></span> &#123;<span class="hljs-keyword">var</span> term <span class="hljs-keyword">uint64</span><span class="hljs-keyword">var</span> voteMsg pb.MessageType<span class="hljs-keyword">if</span> t == campaignPreElection &#123;r.becomePreCandidate()voteMsg = pb.MsgPreVote<span class="hljs-comment">// 这里需要注意的是，PreVote 会针对“下一轮 term”发起投票，</span><span class="hljs-comment">// 而 Vote 则是针对当前 term</span><span class="hljs-comment">// PreVote RPCs are sent for the next term before we've incremented r.Term.</span>term = r.Term + <span class="hljs-number">1</span>&#125; <span class="hljs-keyword">else</span> &#123;r.becomeCandidate()voteMsg = pb.MsgVoteterm = r.Term&#125;        <span class="hljs-comment">// ...</span>        <span class="hljs-comment">// 发送投票请求</span>    r.send(pb.Message&#123;Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx&#125;)        <span class="hljs-comment">// ...</span>&#125;</code></pre><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>所有节点间仅通过三种类型的 RPC 进行通信：</p><ul><li><code>AppendEntriesRPC</code>：最常用的，leader 向 follower 发送心跳或同步日志。</li><li><code>RequestVoteRPC</code>：选举时，candidate 发起的竞选请求。</li><li><code>InstallsnapshotRPC</code>：用于 leader 下发 snapshot。</li></ul><p>在 Diego 后续的博士论文中，又增加了一些 RPCs：</p><ul><li><code>AddServerRPC</code>：添加单台节点。</li><li><code>RemoveServerRPC</code>：移除一个节点。</li><li><code>TimeoutNowRPC</code>：立刻发起竞选。<br>（实际上 etcd 的实现中定义了几十种消息类型，甚至把内部事件也封装为消息一并处理。）</li></ul><h3 id="AppendEntriesRPC"><a href="#AppendEntriesRPC" class="headerlink" title="AppendEntriesRPC"></a>AppendEntriesRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 当前的 term；</li><li><code>leaderId</code>：leader 的 节点id，让 follower 可以重定向客户端的连接；</li><li><code>prevLogIndex</code>：前一块 entry 的 index；</li><li><code>prevlogterm</code>：前一块 entry 的 term；</li><li><code>entries[]</code>：给 follower 发送的 entry，可以一次发送多个，heartbeat 时该项可缺省；</li><li><code>leaderCommit</code>：leader 当前的 <code>committed index</code>，follower 收到后可用于自己的状态机。</li></ul><p>返回：</p><ul><li><code>term</code>：响应者自己的 term；</li><li><code>success</code>：bool，是否接受请求。<br>该请求通过 leaderCommit 通知 follower 提交相应的 entries 到。通过 entries[] 复制 leader 的日志到所有的 follower。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，立刻返回 false</li><li>如果 prevLogIndex 不匹配，返回 false</li><li>如果自己有块 entry 和新的 entry 不匹配（在相同的 index 上有不同的 term）， 删除自己的那一块以及之后的所有 entry；</li><li>把新的 entries 添加到自己的 log；<br>5 。如果 <code>leaderCommit &gt; commitindex</code>，将 commitIndex 设置为 <code>min(leaderCommit, last index)</code>， 并且提交相应的 entries。</li></ol><h3 id="RequestVoteRPC"><a href="#RequestVoteRPC" class="headerlink" title="RequestVoteRPC"></a>RequestVoteRPC</h3><p>参数：</p><ul><li><code>term</code>：candidate 当前的 term；</li><li><code>candidateId</code>：candidate 的节点 id</li><li><code>lastlogindex</code>：candidate 最后一个 entry 的 index；</li><li><code>lastlogterm</code>：candidate 最后一个 entry 的 term。</li><li><code>isleaderTransfer</code>：用于表明该请求来自于禅让，无需等待 electionTimeout，必须立刻响应。</li><li><code>isPreVote</code>：用来表明当前是 PreVote 还是真实投票</li></ul><p>返回：</p><ul><li><code>term</code>：响应者当前的 term；</li><li><code>voteGranted</code>：bool，是否同意投票。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，返回 false；</li><li>如果 votedFor 为空或者为该 <code>candidated id</code>，且日志项不落后于自己，则同意投票。</li></ol><h3 id="InstallsnapshotRPC"><a href="#InstallsnapshotRPC" class="headerlink" title="InstallsnapshotRPC"></a>InstallsnapshotRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 的 term</li><li><code>leaderId</code>：leader 的 节点 id</li><li><code>lastIncludedindex</code>：snapshot 中最后一块 entry 的 index；</li><li><code>lastIncludedterm</code>：snapshot 中最后一块 entry 的 term；</li><li><code>offset</code>：该份 chunk 的 offset；</li><li><code>data[]</code>：二进制数据；</li><li><code>done</code>：是否是最后一块 chunk</li></ul><p>返回：</p><ul><li><code>term</code>：follower 当前的 term</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code> 就立即回复</li><li>如果是第一个分块（offset 为 0）就创建一个新的快照</li><li>在指定偏移量写入数据</li><li>如果 done 是 false，则继续等待更多的数据</li><li>保存快照文件，丢弃索引值小于快照的日志</li><li>如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保持</li><li>丢弃整个日志</li><li>使用快照重置状态机</li></ol><h3 id="AddServerRPC"><a href="#AddServerRPC" class="headerlink" title="AddServerRPC"></a>AddServerRPC</h3><p>参数：</p><ul><li><code>newServer</code>：新节点地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否添加成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>如果没有在 electionTimeout 内处理，则返回 TIMEOUT；</li><li>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="RemoveServerRPC"><a href="#RemoveServerRPC" class="headerlink" title="RemoveServerRPC"></a>RemoveServerRPC</h3><p>参数：</p><ul><li><code>oldServer</code>：要删除的节点的地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否删除成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="TimeoutNowRPC"><a href="#TimeoutNowRPC" class="headerlink" title="TimeoutNowRPC"></a>TimeoutNowRPC</h3><p>由 leader 发起，告知 target 节点立刻发起竞选，无视 electionTimeout。主要用于禅让。</p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>一致性协议</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CAP 定理介绍</title>
    <link href="/cap-theory/"/>
    <url>/cap-theory/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在互联网行业飞速发展的 21 世纪，分布式系统正变得越来越重要，大型互联网公司如 Google，Amazon，MicroSoft，Alibaba，Tencent 等之所以被认为技术很厉害，很大程度上是因为其后台十分强悍，而这些后台一定是由若干个大的分布式系统组成的，因此理解分布式系统的运行原理对于程序员有非常重要的意义。</p><p>CAP 定理是分布式系统方向一个比较宽泛但很重要的基本定理，也可以作为理解分布式系统的起点。这篇博客将详细介绍 CAP 定理并简单证明，最后谈一谈 CAP 定理在工业界的应用。</p><h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>2000年，柏克莱加州大学（University of California, Berkeley）的计算机科学家 Eric Brewer 在分布式计算原则研讨会（Symposium on Principles of Distributed Computing）提出，分布式系统有三个指标。</p><ul><li>Consistency</li><li>Availability</li><li>Partition tolerance</li></ul><p>它们的第一个字母分别是 C、A、P。</p><p>Eric Brewer 说，这三个指标不可能同时做到，这个结论就叫做 CAP 定理。</p><p>需要注意的是，尽管我们常说某个系统能够满足 CAP 属性中的 2 个，但并不是必须满足 2 个，许多系统只具有 0 或 1 个 CAP 属性。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h3><p>我们知道 ACID 中事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行前后，数据库都必须处于一致性状态。也就是说，事务的执行结果必须是使数据库从一个一致性状态转变到另一个一致性状态。</p><p>和 ACID 中的一致性不同，分布式环境中的一致性是指数据在多个副本之间是否能够保持一致的特性。</p><p>分布式系统中，数据一般会存在不同节点的副本中，如果对第一个节点的数据成功进行了更新操作，而第二个节点上的数据却没有得到相应更新，这时候读取第二个节点的数据依然是更新前的数据，即脏数据，这就是分布式系统数据不一致的情况。</p><p>在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都能读取到最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。</p><h3 id="Availability"><a href="#Availability" class="headerlink" title="Availability"></a>Availability</h3><p>可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果，如果超过了这个时间范围，那么系统就被认为是不可用的。</p><p>“有限的时间内”是在系统的运行指标，不同系统会有差别。例如搜索引擎通常在 0.5 秒内需要给出用户检索结果。</p><p>”返回结果”是可用性的另一个重要指标，它要求系统完成对用户请求的处理后，返回一个正常的响应结果，要明确的反映出对请求处理的成功或失败。如果返回的结果是系统错误，比如”OutOfMemory”等报错信息，则认为此时系统是不可用的。</p><h3 id="Partition-Tolerance"><a href="#Partition-Tolerance" class="headerlink" title="Partition Tolerance"></a>Partition Tolerance</h3><p>一个分布式系统中，节点组成的网络本来应该是连通的。然而可能因为某些故障，使得有些节点之间不连通了，整个网络就分成了几块区域，而数据就散布在了这些不连通的区域中，这就叫分区。</p><p>当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。</p><p>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项仍然能在其他区中读取，容忍性就提高了。然而，把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。</p><p>总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。</p><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><h3 id="简单理解"><a href="#简单理解" class="headerlink" title="简单理解"></a>简单理解</h3><p>根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。理解 CAP 理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了 C 性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了 A 性质。除非两个节点可以互相通信，才能既保证 C 又保证 A，这又会导致丧失 P 性质。</p><h3 id="详细证明"><a href="#详细证明" class="headerlink" title="详细证明"></a>详细证明</h3><p><img src="/cap-theory/base.png" srcset="/img/loading.gif" alt></p><p>我们现在有两个网络 N1 和 N2，每个网络中都存在一个服务用于从 db 获取数据，初始状态下，db 中存储的数据都是 V0。</p><p><img src="/cap-theory/p.png" srcset="/img/loading.gif" alt></p><p>正常情况下，在网络 N1 通过服务 A 更新 V0 到 V1，更新成功后发送消息 M 使 N2 的 db 中的 V0 变为 V1，此时我们通过服务 B 获取数据时，获取到 V1。</p><pre><code class="hljs stata">此时满足 <span class="hljs-keyword">CA</span>，没有分区故不满足 P。</code></pre><p><img src="/cap-theory/withoutp.png" srcset="/img/loading.gif" alt></p><p>但是一旦发生了网络分区，此时我们通过服务 A 更新数据到 V1 后，由于网络错误，V1 值同步不到 N2 网络中去，此时我们调用服务 B 去请求数据的时候，我们必须从 C 和 A 选一个，如果选择 C，我们需要等到数据同步到 N2，但是从服务 B 获取数据肯定是失败了，失去了 A。如果选择 A，那么从 B 我们获取到的数据不是最新的，失去了 C。</p><pre><code class="hljs stata">此时有分区故满足 P，<span class="hljs-keyword">CA</span> 只能满足一个。</code></pre><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="取舍策略"><a href="#取舍策略" class="headerlink" title="取舍策略"></a>取舍策略</h3><h4 id="CP-without-A"><a href="#CP-without-A" class="headerlink" title="CP without A"></a>CP without A</h4><p>如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在 CAP 三者中保障 CP 而舍弃 A。</p><p>一个保证了 CP 而一个舍弃了 A 的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。</p><p>设计成 CP 的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成 CP 的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如 Redis、HBase 等，还有分布式系统中常用的 Zookeeper 也是在 CAP 三者之中选择优先保证 CP 的。</p><p>无论是像 Redis、HBase 这种分布式存储系统，还是像 Zookeeper 这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？</p><p>ZooKeeper 是个 CP（一致性+分区容错性）的，即任何时刻对 ZooKeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper 可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成 CP 而不是 AP 特性的了。</p><h4 id="AP-wihtout-C"><a href="#AP-wihtout-C" class="headerlink" title="AP wihtout C"></a>AP wihtout C</h4><p>要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。</p><p>这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到 N 个 9，所以，对于很多业务系统来说，比如淘宝的购物，12306 的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p><p>你在 12306 买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。</p><p>但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。</p><p>对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃 C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p><h4 id="CA-without-P"><a href="#CA-without-P" class="headerlink" title="CA without P"></a>CA without P</h4><p>这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃 P，意味着要舍弃分布式系统。那也就没有必要再讨论 CAP 理论了。这也是为什么在前面的 CAP 证明中，我们以系统满足 P 为前提论述了无法同时满足 C 和 A。</p><p>比如我们熟知的关系型数据库，如 Mysql 和 Oracle 就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把 P 也考虑进来。</p><p>其实，在 CAP 理论中。C，A，P 三者并不是平等的，CAP 之父在《Spanner，真时，CAP 理论》一文中写到：</p><blockquote><p>如果说 Spanner 真有什么特别之处，那就是谷歌的广域网。Google 通过建立私有网络以及强大的网络工程能力来保证 P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。</p></blockquote><p>从 Google 的经验中可以得到的结论是，一直以来我们可能被 CAP 理论蒙蔽了双眼，CAP 三者之间并不对称，C 和 A 不是 P 的原因（P 不能和 CA trade-off，CP 和 AP 中不存在 trade-off，trade-off 在 CA 之间）。提高一个系统的抗毁能力或者说提高 P（分区容忍能力）是通过提高基础设施的稳定性来获得的，而不是通过降低 C 和 A 来获得的，也就说牺牲 C 和 A 也不能提高 P。</p><p>所以，对于一个分布式系统来说。P 是一个基本要求，CAP 三者中，只能在 CA 两者之间做权衡，并且要想尽办法提升 P。P 提升的越好，CA 同时满足就越有可能。</p><h3 id="业界应用分析"><a href="#业界应用分析" class="headerlink" title="业界应用分析"></a>业界应用分析</h3><div class="table-container"><table><thead><tr><th>应用</th><th>类型</th><th>解释</th></tr></thead><tbody><tr><td>MySQL</td><td>CA</td><td>主从模式为 AP</td></tr><tr><td>Spanner</td><td>CA/CP</td><td>技术实现是 CP 但号称是 CA，宣称 CA 系统并不意味着 100％ 的可用性</td></tr><tr><td>分布式协议-Raft/ZAB/Paxos</td><td>CP</td><td>在分区后，对于 A，只有分区内节点大于 Quorum 才对外服务</td></tr><tr><td>分布式事务-2PC</td><td>CP</td><td>锁住资源,该资源其他请求阻塞</td></tr><tr><td>分布式事务-TCC</td><td>AP</td><td>最终一致性</td></tr><tr><td>分布式事务-最大努力尝试</td><td>AP</td><td>最终一致性</td></tr><tr><td>DNS 服务</td><td>AP</td><td>最终一致性</td></tr></tbody></table></div><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，因此分区容错性也就成为了一个分布式系统必然要面对的问题，那么就只能在 C 和 A 之间进行取舍。</p><p>对于某些安全性要求极高的项目，比如银行的转账系统，涉及到金钱的对于数据一致性不能做出一丝的让步，C 必须保证，出现网络故障的话，宁可停止服务，也不能冒着出错误的风险继续提供服务。</p><p>对于网站，DNS 服务等，其内容的实时性不是特别严格，则可以牺牲一定的一致性，保证最高的可用性是最好的选择。</p><p>个人认为，CAP 定理的核心在于，在网络分区的情况下，我们需要对 C 和 A 做出相应的妥协，我们不可能完全满足 CA，但是我们可以合理控制 C 和 A 之间的比例让我们的应用/中间件正常提供服务，同时也尽量提升基础设施的稳定性来保障 P。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>《Spanner，真时，CAP 理论》是 Google VP，CAP 理论之父在情人节当天撰写的，主要介绍了 Google 的 Spanner 数据库的真时（TrueTime）服务和 CA 特性，以及结合 CAP 理论的一些思考，建议阅读，阅读 Spanner 论文后阅读更佳。</p><ul><li><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45855.pdf" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》</a></p></li><li><p><a href="https://toutiao.io/posts/zdqrx0/preview" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》中文</a></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iTerm2 快捷键介绍</title>
    <link href="/iTerm2-hotkeys/"/>
    <url>/iTerm2-hotkeys/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>iTerm2 是 MacOS 独有的终端工具，其有许多快捷键可以使用。为了便于开发并节约之后再次在搜索引擎上查询的时间成本，特写此博客以供自己日后查看。</p><h2 id="快捷键介绍"><a href="#快捷键介绍" class="headerlink" title="快捷键介绍"></a>快捷键介绍</h2><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><ul><li>新建标签：Command + T</li><li>关闭标签：Command + W</li><li>切换标签：Command + 数字 或 Command + 左右方向键</li></ul><h3 id="分屏"><a href="#分屏" class="headerlink" title="分屏"></a>分屏</h3><ul><li>垂直分屏：Command + D</li><li>水平分屏：Command + Shift + D</li><li>切换屏幕：Command + Option + 方向键 或 Command + [ / ]</li></ul><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><ul><li>局部搜索(包含单个终端)：Command + F</li><li>全局搜索(包含所有Tab)：Command + Option + E</li><li>搜索历史指令：Ctrl + R</li></ul><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><ul><li>查看历史命令：Command + ;</li><li>查看剪贴板历史：Command + Shift + H</li><li>上一条命令：Ctrl + P 或 上方向键</li></ul><h3 id="单行"><a href="#单行" class="headerlink" title="单行"></a>单行</h3><ul><li>光标到行首：Ctrl + A</li><li>光标到行尾：Ctrl + E</li><li>删除当前行：Ctrl + U</li><li>删除当前光标的字符：Ctrl + D</li><li>删除光标之前的字符：Ctrl + H</li><li>删除光标之前的单词：Ctrl + W</li><li>删除到文本末尾：Ctrl + K</li></ul><h3 id="内容大小"><a href="#内容大小" class="headerlink" title="内容大小"></a>内容大小</h3><ul><li>放大终端：Command + +</li><li>缩小终端：Command + - </li></ul><h3 id="常用快捷功能"><a href="#常用快捷功能" class="headerlink" title="常用快捷功能"></a>常用快捷功能</h3><ul><li>清屏：Command + R 或 Crtl + L</li><li>切换全屏：Command + Enter</li><li>选中即复制：在 iTerm2 界面，选择了一行就已经复制了</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>开发工具配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的高效 Macbook 工作环境配置</title>
    <link href="/mac-configuration/"/>
    <url>/mac-configuration/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>工欲善其事，必先利其器，工具永远都是用来解决问题的，没必要为了工具而工具，一切工具都是为了能快速准确的完成工作和学习任务而服务。</p><p>我呢，在使用了 Windows，Ubuntu 和 MacOS 三种操作系统之后。结合种种体验和踩坑，最终还是觉得 MacOS 更舒适一点。每个人都有每个人的看法，每个人都有每个人的舒适点，MacOS 恰好捏住了我的舒适点。因此，我之后都将从 MacOS 上工作学习。</p><p>前一段时间我从公司实习离职，上交了公司发给我的 MacBook（<del>停止薅羊毛</del>），然而我又不想回到 Windows，于是打算自己买一台 MacBook。但是 MacBook 从 2016 年开始更换的蝶式键盘很让我恶心，姑且不说故障率高，触感实在太差劲了。尽管 2020 年新出的 16 寸 Pro 已经重回剪刀脚键盘了，但是我的需求是轻薄的 13 寸而不是 16 寸（<del>只是没钱而已</del>）。尽管听到业界的呼声说 2020 年的 MacBook 应该都会回到剪刀脚键盘，但由于 2020 年会换新模具，我也不想踩第一代模具的坑，因而暂且将目标定为 2021 年的 MacBook，目前一年多买个二手过渡下就可以了。</p><p>1 月份我在某宝平台上买了一台 2014 款 8+256 的二手 MacBook Pro，即使前期做了许多选店和辨伪的功课，拿到手之后却依然中招，总是无理由黑屏然后再无法开机一天，十分坑爹。所幸可以十五天无理由退换货，就赶快退了。之前早就听说二手 Mac 的水很深，被坑一次之后更加确信。接着我做了更多的功课，学到了许多辨伪技巧，浏览了许多店铺，也算有点心得，之后要是有时间可以写出来分享给大家。</p><p>前几天经过慎重选择我又在某东平台上入手了一台 2015 款 8+128 的二手 MacBook Pro。这次总算没什么问题，但比较有趣的一点是我买的 8+128 的，老板发给我的是 8+256 的，平白无故赚了 128G 的固态，只能说真的舒服了。</p><p><img src="/mac-configuration/mac-configuration.jpeg" srcset="/img/loading.gif" alt></p><p>这是一个新的 MacBook 刚打开后的主页，接下来我要通过一系列的配置使其成为一个十分符合我开发习惯的机器，可供大家参考。</p><h2 id="系统篇"><a href="#系统篇" class="headerlink" title="系统篇"></a>系统篇</h2><h3 id="触屏板"><a href="#触屏板" class="headerlink" title="触屏板"></a>触屏板</h3><ul><li>2016 年及之后的 MacBook 触屏板都有 Force touch 的功能，即可以按压两次来实现更多的功能，但是我一直用不来这个功能，因此我的第一件事就是调整触摸屏板，首先先关掉 Force touch 的功能，然后开启轻点来点按的点击方式，个人觉得这样才符合 MacBook 轻巧的特性嘛，每次都按下去多麻烦啊，现在手指轻轻一碰触摸板，就达到鼠标单击的顺滑效果。</li><li>除此以外，可以根据自己的习惯开启或关闭一些手势。</li></ul><p><img src="/mac-configuration/touch_1.png" srcset="/img/loading.gif" alt><br><img src="/mac-configuration/touch_2.png" srcset="/img/loading.gif" alt><br><img src="/mac-configuration/touch_3.png" srcset="/img/loading.gif" alt></p><h3 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h3><ul><li>由于 MacBook 默认的重复前延迟和按键重复配置太慢，限制了程序员们优秀的打字速度，所以建议都调整到最快的速度。</li><li>可以在闲置 5 分钟后关闭键盘背光灯来省点电。</li></ul><p><img src="/mac-configuration/keyboard.png" srcset="/img/loading.gif" alt></p><h3 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h3><ul><li>由于 MacBook 默认的切换大小写的方式是长按 Caps 键，时间较慢需要等待，较为影响开发效率，建议关闭长按改为短按，配合极低的按键延迟会十分舒爽。</li></ul><p><img src="/mac-configuration/input.png" srcset="/img/loading.gif" alt></p><ul><li>建议安装搜狗输入法 Mac 版替代系统自带输入法。</li></ul><h3 id="快速锁定屏幕"><a href="#快速锁定屏幕" class="headerlink" title="快速锁定屏幕"></a>快速锁定屏幕</h3><ul><li><p>如果你长时间离开电脑，最好锁定你的屏幕，以防止数据泄露。 那如何快速的锁定你的 MacBook 呢？ 答案是只需要一摸触摸板就可以了。</p><ul><li><p>打开系统偏好设置，点击桌面与屏幕保护程序图标，选择屏幕保护程序这个 Tab，再点击触发角，在弹出的如下界面里面，右下角选择将显示器置入睡眠状态，再确定即可。</p><p><img src="/mac-configuration/screen_saver.png" srcset="/img/loading.gif" alt></p></li><li><p>再打开系统偏好设置，点击安全性与隐私图标，在通用 Tab 内，勾选为进入睡眠或开始屏幕保护程序<strong>立即</strong>要求输入密码。</p><p><img src="/mac-configuration/screen_security.png" srcset="/img/loading.gif" alt></p></li></ul></li></ul><h2 id="开发环境篇"><a href="#开发环境篇" class="headerlink" title="开发环境篇"></a>开发环境篇</h2><h3 id="Xcode"><a href="#Xcode" class="headerlink" title="Xcode"></a>Xcode</h3><ul><li><p>首先安装 Xcode，然后使用下面的命令安装 Xcode command line tools，这将为我们安装很多终端下面常用的命令，将来很可能会使用到。</p>  <pre><code class="hljs Shell">xcode-select --install</code></pre></li></ul><h3 id="Homebrew"><a href="#Homebrew" class="headerlink" title="Homebrew"></a>Homebrew</h3><ul><li><p>Homebrew 是一款终端下的命令程序包管理器，安装非常简单，复制如下命令在终端下运行，按回车并输入密码后等待安装成功：</p>  <pre><code class="hljs Shell">/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"</code></pre></li></ul><h3 id="iTerm2-Zsh-Z"><a href="#iTerm2-Zsh-Z" class="headerlink" title="iTerm2 + Zsh + Z"></a>iTerm2 + Zsh + Z</h3><ul><li>常用终端 iTerm2 + 优秀 Shell Zsh + 扁平目录跳转命令 Z，安装好之后开发十分舒适。具体安装可参考这个<a href="https://www.zcfy.cc/article/become-a-command-line-power-user-with-oh-my-zsh-and-z" target="_blank" rel="noopener">博客</a>。</li></ul><h3 id="快捷键迅速打开-iTerm2"><a href="#快捷键迅速打开-iTerm2" class="headerlink" title="快捷键迅速打开 iTerm2"></a>快捷键迅速打开 iTerm2</h3><ul><li>可以设置快捷键再 Home 页面输入 Command + , 直接打开 iTerm2，这样就不用再去点击 iTerm2 了。</li></ul><p><img src="/mac-configuration/iTerm2_hotkey.png" srcset="/img/loading.gif" alt></p><ul><li>可以设置 iTerm2 默认占满全屏，这样子快捷键打开之后就直接是一个全屏的 iTerm2 可以使用了</li></ul><p><img src="/mac-configuration/iTerm2_screen.png" srcset="/img/loading.gif" alt> </p><h3 id="VScode命令行迅速打开"><a href="#VScode命令行迅速打开" class="headerlink" title="VScode命令行迅速打开"></a>VScode命令行迅速打开</h3><ul><li>打开VScode后输入 Command + Shift + P 打开命令面板，再输入 code，再确定</li></ul><p><img src="/mac-configuration/vscode_code.png" srcset="/img/loading.gif" alt></p><h3 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h3><ul><li>创建新的公钥私钥并与自己的 Github 账户连起来，这样就可以开始在 Github 遨游啦。</li></ul><h2 id="常用软件"><a href="#常用软件" class="headerlink" title="常用软件"></a>常用软件</h2><ul><li>网易云音乐</li><li>微信</li><li>QQ</li><li>腾讯会议</li><li>V2Ray</li><li>Chrome</li><li>VScode</li><li>GoLand</li><li>IDEA</li><li>JProfile</li><li>Docker</li><li>…</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>开发工具配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
