<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Raft 协议介绍</title>
    <link href="/raft/"/>
    <url>/raft/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="共识算法"><a href="#共识算法" class="headerlink" title="共识算法"></a>共识算法</h3><p>共识算法允许一组节点像一个整体一样一起工作，即使其中一些节点出现故障也能够继续工作下去。更详细点，可以认为每一个节点上都运行着一个状态机和一组日志。在客户端提交命令后，状态机负责执行命令并返回结果，同时还可能改变自己的状态；日志则记录了所有可能会影响到状态机的命令。任何一个处于初始状态的状态机都可以通过重新按顺序执行这组日志，来让自己恢复到最终状态。</p><p>共识算法常被用来确保每一个节点上的状态机一定都会按相同的顺序执行相同的命令， 并且最终会处于相同的状态。换句话说，可以理解为共识算法就是用来确保每个节点上的日志顺序都是一致的。（不过需要注意的是，只确保“提交给状态机的日志”顺序是一致的，而有些日志项可能只是暂时添加，尚未决定要提交给状态机）。正因为如此，共识算法在构建可容错的大规模分布式系统中扮演着重要的角色。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_rsm.png" srcset="/img/loading.gif" alt=""></p><p>上图就是每个节点上的状态机，日志和同步模块与客户端交互的过程。</p><p>当然，实际使用系统中的共识算法一般满足以下特性：</p><ul><li>在非拜占庭条件（无恶意欺骗）下保证共识的一致性；</li><li>在多数节点存活时，保持可用性；</li><li>不依赖于时间，错误的时钟和高延迟只会导致可用性问题，而不会导致一致性问题；</li><li>在多数节点一致后就返回结果，而不会受到个别慢节点的影响。<br>（注：“多数”永远指的是配置文件中所有节点的多数，而不是存活节点的多数）<br>（注：非拜占庭条件，指的就是每一个节点都是诚实可信的，每一次信息的传递都是真实的且符合协议要求的，当节点无法满足协议所要求的条件时，就停止服务，节点仅会因为网络延迟或崩溃出现不一致，而不会有节点传递错误的数据或故意捏造假数据。）</li></ul><h3 id="Raft-的由来与宗旨"><a href="#Raft-的由来与宗旨" class="headerlink" title="Raft 的由来与宗旨"></a>Raft 的由来与宗旨</h3><p>众所周知，Paxos 是一个非常划时代的共识算法。在 Raft 出现之前的 10 年里，Paxos 几乎统治着共识算法这一领域：因为绝大多数共识算法的实现都是基于 Paxos 或者受其影响，同时 Paxos 也成为了教学领域里讲解共识问题时的示例。</p><p>但是不幸的是，尽管有很多工作都在尝试降低 Paxos 的复杂性，但是它依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。比如 <code>Google Chubby</code> 的论文就提到，因为 Paxos 的描述和现实差距太大，所以最终人们总会实现一套未经证实的类 Paxos 协议。</p><p>基于以上背景，<code>Diego Ongaro</code> 在就读博士期间，深入研究 Paxos 协议后提出了 Raft 协议，旨在提供更为易于理解的共识算法。Raft 的宗旨在于可实践性和可理解性，并且相比 Paxos 几乎没有牺牲多少性能。</p><blockquote><p>趣闻：<a href="https://groups.google.com/forum/#!topic/raft-dev/95rZqptGpmU" target="_blank" rel="noopener">Raft 名字的来源</a>。简而言之，其名字即来自于 <code>R{eliable|plicated|dundant} And Fault-Tolerant</code>， 也来自于这是一艘可以帮助你逃离 Paxos 小岛的救生筏（Raft）。</p></blockquote><h3 id="工业界的实现"><a href="#工业界的实现" class="headerlink" title="工业界的实现"></a>工业界的实现</h3><ul><li><code>Tidb</code></li><li><code>Consul</code></li><li><code>etcd</code></li><li>…</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>这一部分会简单介绍 Raft 的一些基本概念。若暂时没看懂并没有关系，后面会一一介绍清楚，带着问题耐心读完此博客即可。</p><h3 id="Raft-的子问题"><a href="#Raft-的子问题" class="headerlink" title="Raft 的子问题"></a>Raft 的子问题</h3><p>Raft 将共识算法这个难解决的问题分解成了多个易解决，相对独立的子问题，这些问题都会在接下来的章节中进行介绍。</p><ul><li><code>选主</code>：选出集群的 leader 来统筹全局。</li><li><code>日志同步</code>：leader 负责从客户端接收请求，并且在集群中扩散同步。</li><li><code>安全</code>：各节点间状态机的一致性保证。</li></ul><p>在完整的论文和 etcd 实现中，其实还有一些问题：</p><ul><li><code>配置变更</code>：集群动态增删节点。</li><li><code>禅让</code>：能够将 leader 迅速转给另一个 follower。</li><li><code>Pre-Vote</code>：在竞选开始时先进行一轮申请，若被允许再转变为 candidate，这样有助于防止某些异常节点扰乱整个集群的正常工作。</li><li>…</li></ul><h3 id="Raft-的节点类型"><a href="#Raft-的节点类型" class="headerlink" title="Raft 的节点类型"></a>Raft 的节点类型</h3><p>Raft 将所有节点分为三个身份：</p><ul><li><code>leader</code>：集群内最多只会有一个 leader，负责发起心跳，响应客户端，创建日志，同步日志。</li><li><code>candidate</code>：leader 选举过程中的临时角色，由 follower 转化而来，发起投票参与竞选。</li><li><code>follower</code>：接受 leader 的心跳和日志同步数据，投票给 candidate。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_state.png" srcset="/img/loading.gif" alt=""></p><p>上图可以看出 Raft 中节点状态之间变迁的条件。</p><p>在完整的论文和 etcd 实现中，其实又增加了几种中间状态：</p><ul><li><code>Learner</code>：新加入的节点，不具有选举权，需要从 leader 同步完数据后， 才能转变为 follower。严格来说，learner 并不算集群成员。</li><li><code>Pre-Candidate</code>：刚刚发起竞选，还在等待 <code>Pre-Vote</code> 结果的临时状态， 取决于 <code>Pre-Vote</code> 的结果，可能进化为 candidate，可能退化为 follower。<br>（注：此两种节点状态的流程和作用后面章节会介绍，可先无视此两个状态）</li></ul><h3 id="Raft-的节点状态"><a href="#Raft-的节点状态" class="headerlink" title="Raft 的节点状态"></a>Raft 的节点状态</h3><p>每一个节点都应该有的持久化状态：</p><ul><li><code>currentterm</code>：当前任期。</li><li><code>votedFor</code>：在当前 term，给哪个节点投了票，值为 NULL 或 <code>candidate id</code>。</li><li><code>log[]</code>：已经 committed 的日志。</li></ul><p>每一个节点都应该有的可以非持久化的状态：</p><ul><li><code>commitindex</code>：已提交的最大 index。</li><li><code>lastApplied</code>：已被状态机应用的最大 index。<br>（注：这两个不需要持久化是因为状态机本身是非持久化的，而状态机的状态可以通过 log[] 来恢复）</li></ul><p>leader 的非持久化状态：</p><ul><li><code>nextindex[]</code>：为每一个 follower 保存的，应该发送的下一份 <code>entry index</code>；<br>初始化为 Last index + 1。</li><li><code>matchindex[]</code>：已确认的，已经同步到每一个 follower 的 <code>entry index</code><br>初始化为 0，单调递增。<br>（注：每次选举后，都应该立刻重新初始化）</li></ul><h3 id="Raft-的任期概念"><a href="#Raft-的任期概念" class="headerlink" title="Raft 的任期概念"></a>Raft 的任期概念</h3><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_term.png" srcset="/img/loading.gif" alt=""></p><p>Raft 将时间划分成为任意不同长度的 term。term 用连续的数字进行表示。每一个 term 的开始都是一次选举，一个或多个 candidate 会试图成为 leader。如果一个  candidate 赢得了选举，它就会在该 term 的剩余时间担任 leader。在某些情况下，选票会被瓜分，有可能没有选出 leader，那么，将会开始另一个 term，并且立刻开始下一次选举。Raft 保证在给定的一个 term 最多只有一个 leader。</p><p>不同的服务器节点可能多次观察到 term 之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个 term 全程。term 在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如过期的 leader。</p><p>每个节点都会存储当前 term 号，这一编号在整个时间内单调增长。当服务器之间通信的时候会交换当前 term 号；如果一个服务器的当前 term 号比其他人小，那么他会更新自己的 term 到较大的 term 值。如果一个 candidate 或者 leader 发现自己的 term 过期了，那么他会立即退回 follower。如果一个节点接收到一个包含过期 term 号的请求，那么它会直接拒绝这个请求。</p><h3 id="Raft-的日志组成"><a href="#Raft-的日志组成" class="headerlink" title="Raft 的日志组成"></a>Raft 的日志组成</h3><ul><li><p><code>entry</code>：Raft 中，将每一个事件都称为一个 entry，每一个 entry 都有一个表明它在 log 中位置的 index（之所以从 1 开始是为了方便 <code>prevLogIndex</code> 从 0 开始）。只有 leader 可以创建 entry。entry 的内容为 <code>&lt;term, index, cmd&gt;</code>，其中 cmd 是可以应用到状态机的操作。被提交给状态机后，entry 被称为是 committed 的。</p></li><li><p><code>log</code>：由 entry 构成的数组，只有 leader 可以改变其他节点的 log。 entry 总是先被添加进 log（写操作都应该立刻持久化），然后才发起共识请求，通过后才会被 leader 提交给状态机。follower 只能从 leader 那获取到当前已经 commit 日志的最大索引号，然后应用到自己的状态机。</p></li></ul><h3 id="Raft-的保证"><a href="#Raft-的保证" class="headerlink" title="Raft 的保证"></a>Raft 的保证</h3><ul><li><code>Election Safety</code>：最多只会有一个 leader。</li><li><code>Leader Append-Only</code>：leader 的日志是只增的。</li><li><code>Log Matching</code>：如果两个节点的日志中有两个 entry 有相同的 index 和 term，那么它们就是相同的 entry。</li><li><code>Leader Completeness</code>：一旦一个操作被提交了，那么在之后的 term 中，该操作都会存在于日志中。</li><li><code>State Machine Safety</code>：一致性，一旦一个节点应用了某个 index 的操作到状态机，那么其他所有节点应用的该 index 的操作都是一致的。</li></ul><h2 id="选主"><a href="#选主" class="headerlink" title="选主"></a>选主</h2><p>Raft 使用心跳来维持 leader 身份。任何节点都以 follower 的身份启动。 leader 会定期的发送心跳给所有的 followers 以确保自己的身份。 每当 follower 收到心跳后，就刷新自己的 electionElapsed，重新计时。</p><p>（后文中，会将预设的选举超时称为 electionTimeout，而将当前经过的选举耗时称为 electionElapsed。）</p><p>一旦一个 follower 在指定的时间内没有收到任何 RPC（称为 electionTimeout），则会发起一次选举。 当 follower 试图发起选举后，其身份转变为 candidate，在增加自己的 term 后， 会向所有节点发起 RequestVoteRPC 请求，candidate 的状态会一直持续直到：</p><ul><li>赢得选举</li><li>其他节点赢得选举</li><li>一轮选举结束，无人胜出</li></ul><p>选举的方式非常简单，谁能获取到多数选票 <code>(N/2 + 1)</code>，谁就成为 leader。 在一个 candidate 节点等待投票响应的时候，它有可能会收到其他节点声明自己是 leader 的心跳， 此时有两种情况：</p><ul><li>该请求的 term 和自己一样或更大：说明对方已经成为 leader，自己立刻退为 follower；</li><li>该请求的 term 小于自己：拒绝请求。</li></ul><p>在 etcd 的实现中，如果 candidate 收到 term 大于自己的 RequestVote，也会退为 follower。 （准确的说是，收到一切 term 更大的，除了 RequestPreVote、PreVoteResp 外的所有消息，都退为 follower）。</p><p>follower 收到 candidate 的 RequestVote 后，会检查自己是否已经投过票。 不过如果来源于同一个 candidate，那么 follower 可以在同一 term 内多次投给同一个 candidate。</p><pre><code class="GO">// 检查自己是否已经投过票了，如果投票请求来自同一节点，可以重复投票。// We can vote if this is a repeat of a vote we&#39;ve already cast...canVote := r.Vote == m.From ||    // ...we haven&#39;t voted and we don&#39;t think there&#39;s a leader yet in this term...    (r.Vote == None &amp;&amp; r.lead == None) ||    // ...or this is a PreVote for a future term...    (m.Type == pb.MsgPreVote &amp;&amp; m.Term &gt; r.Term)</code></pre><p>为了防止在同一时间有太多的 follower 转变为 candidate 导致无法选出绝对多数， Raft 采用了随机选举超时（<code>randomized election timeouts</code>）的机制， 每一个 candidate 在发起选举后，都会记录一个选举超时（在 <code>150-300ms</code> 间）， 一旦超时后仍然没有完成选举，则增加自己的 term，然后发起新一轮选举。 在这种情况下，应该能在较短的时间内确认出 leader。 （因为 term 较大的有更大的概率压倒其他节点）</p><p>etcd 中将随机选举超时设置为 <code>[electiontimeout, 2 * electiontimeout - 1]</code>。</p><p>如果一个 leader 在 electionTimeout 内无法完成一次多数节点的 heartbeat， 说明该 leader 很可能已经与集群失去联系了，那么该 leader 应该向所有的客户端请求返回 fail， 并且退回到 follower。</p><h2 id="日志同步"><a href="#日志同步" class="headerlink" title="日志同步"></a>日志同步</h2><p>leader 被选举后，则负责所有的客户端请求。每一个客户端请求都包含一个命令，该命令可以被作用到 RSM。</p><p>leader 收到客户端请求后，会生成一个 entry，包含 &lt;index, term number, cmd&gt;。 在将这个 entry 添加到自己的日志末尾后，向所有的节点广播该 entry。</p><p>follower 如果同意接受该 entry，则在将 entry 添加到自己的日志后，返回同意。</p><p>如果 leader 收到了多数的成功答复，则将该 entry 应用到自己的 RSM， 之后，可以称该 entry 是 committed 的。该 committed 信息会随着 AppendEntriesRPC 被传达到其他节点。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_log.png" srcset="/img/loading.gif" alt=""></p><p>Raft 保证下列两个性质：</p><p>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd；<br>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们前面的 entry 也一定相同。<br>通过“仅有 leader 可以生成 entry”来确保第一个性质， 第二个性质则通过一致性检查（consistency check）来保证，该检查包含几个步骤：</p><p>leader 在通过 AppendEntriesRPC 和 follower 通讯时，会带上上一块 entry 的信息， 而 follower 在收到后会对比自己的日志 ， 如果发现这个 entry 的信息（index、term）和自己日志内的不符合，则会拒绝该请求。<br>一旦 leader 发现有 follower 拒绝了请求，则会与该 follower 再进行一轮一致性检查， 找到双方最大的共识点，然后用 leader 的 entries 记录覆盖 follower 所有在最大共识点之后的数据。</p><p>寻找共识点时，leader 还是通过 AppendEntriesRPC 和 follower 进行一致性检查， 方法是发送再上一块的 entry， 如果 follower 依然拒绝，则 leader 再尝试发送更前面的一块，直到找到双方的共识点。 因为分歧发生的概率较低，而且一般很快能够得到纠正，所以这里的逐块确认一般不会造成性能问题。<br>每个 leader 都会为每一个 follower 保存一个 nextIndex 的变量， 标志了下一个需要发送给该 follower 的 entry 的 index。 在 leader 刚当选时，该值初始化为该 leader 的 log 的 index+1。 一旦 follower 拒绝了 entry，则 leader 会执行 nextIndex–，然后再次发送。</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><h3 id="选举限制"><a href="#选举限制" class="headerlink" title="选举限制"></a>选举限制</h3><p>因为 leader 的强势地位，所以 Raft 在投票阶段就确保选举出的 leader 一定包含了整个集群中目前已 committed 的所有日志。</p><p>当 candidate 发送 RequestVoteRPC 时，会带上最后一个 entry 的信息。 所有的节点收到该请求后，都会比对自己的日志，如果发现自己的日志更新一些，则会拒绝投票给该 candidate。 （Pre-Vote 同理，如果 follower 认为 Pre-Candidate 没有资格的话，会拒绝 PreVote）</p><p>判断日志新旧的方式：获取请求的 entry 后，比对自己日志中的最后一个 entry。 首先比对 term，如果自己的 term 更大，则拒绝请求。 如果 term 一样，则比对 index，如果自己的 index 更大（说明自己的日志更长），则拒绝请求。</p><pre><code class="GO">func (l *raftLog) isUpToDate(lasti, term uint64) bool {    return term &gt; l.lastTerm() || (term == l.lastTerm() &amp;&amp; lasti &gt;= l.lastIndex())}</code></pre><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_leader_restriction.png" srcset="/img/loading.gif" alt=""></p><p>在上图中，raft 为了避免出现一致性问题，要求 leader 绝不会提交过去的 term 的 entry （即使该 entry 已经被复制到了多数节点上）。leader 永远只提交当前 term 的 entry， 过去的 entry 只会随着当前的 entry 被一并提交。（上图中的 c，term2 只会跟随 term4 被提交。）</p><p>如果一个 candidate 能取得多数同意，说明它的日志已经是多数节点中最完备的， 那么也就可以认为该 candidate 已经包含了整个集群的所有 committed entries。</p><p>因此 leader 当选后，应该立刻发起 AppendEntriesRPC 提交一个 no-op entry。</p><h3 id="节点崩溃"><a href="#节点崩溃" class="headerlink" title="节点崩溃"></a>节点崩溃</h3><p>如果 leader 崩溃，集群中的所有节点在 electionTimeout 时间内没有收到 leader的心跳信息就会触发新一轮的选主。总而言之，最终集群总会选出唯一的 leader 。按论文中的说法，即使一次RPC高达 <code>30～40ms</code> 时，<code>99.9%</code> 的选举依然可以在 <code>3s</code> 内完成，但一般一个机房内一次 RPC 只需 1ms。当然，选主期间整个集群对外是不可用的。 </p><p>如果 follower 和 candidate 奔溃相对而言就简单很多， 因为 Raft 所有的 RPC 都是幂等的，所以 Raft 中所有的请求，只要超时，就会无限的重试。follower 和 candidate 崩溃恢复后，可以收到新的请求，然后按照上面谈论过的追加或拒绝 entry 的方式处理请求。</p><h3 id="时间与可用性"><a href="#时间与可用性" class="headerlink" title="时间与可用性"></a>时间与可用性</h3><p>Raft 原则上可以在绝大部分延迟情况下保证一致性， 不过为了保证选择和 leader 的正常工作，最好能满足下列时间条件：</p><pre><code>broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF</code></pre><ul><li><code>broadcastTime</code>：向其他节点并发发送消息的平均响应时间；</li><li><code>electionTimeout</code>：follower 判定 leader 已经故障的时间（heartbeat 的最长容忍间隔）；</li><li><code>MTBF(mean time between failures)</code>：单台机器的平均健康时间；</li></ul><p>一般来说，broadcastTime 一般为 <code>0.5～20ms</code>（需要磁盘持久化），MTBF 一般为一两个月， electionTimeout 可以设置为 <code>10～500ms</code>。</p><h2 id="配置更改"><a href="#配置更改" class="headerlink" title="配置更改"></a>配置更改</h2><h3 id="一次变更一台"><a href="#一次变更一台" class="headerlink" title="一次变更一台"></a>一次变更一台</h3><h4 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h4><p>因为在 Raft 算法中，集群中每一个节点都存有整个集群的信息，而集群的成员有可能会发生变更（节点增删、替换节点等）。 Raft 限制一次性只能增／删一个节点，在一次变更结束后，才能继续进行下一次变更。</p><p>如果一次性只变更一个节点，那么只需要简单的要求“在新／旧集群中，都必须取得多数（N/2+1）”， 那么这两个多数中必然会出现交集，这样就可以保证不会因为配置不一致而导致脑裂。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_singlechange.png" srcset="/img/loading.gif" alt=""></p><p>当 leader 收到集群变更的请求后，就会生成一个特殊的 entry 项用来保存配置， 在将配置项添加到 log 后，该配置立刻生效（也就是说任何节点在收到新配置后，就立刻启用新配置）。 然后 leader 将该 entry 扩散至多数节点，成功后则提交该 entry。 一旦一个新配置项被 committed，则视为该次变更已结束，可以继续处理下一次变更了。</p><p>为了保证可用性，需要新增一项规则，节点在响应 RPC 时，不考虑来源节点是否在自己的配置文件之中。 也就是说，即使收到了一个并不在自己配置文件之中的节点发来的 RPC， 也需要正常处理和响应，包括 AppendEntriesRPC 和 RequestVoteRPC。</p><h3 id="一次变更多台"><a href="#一次变更多台" class="headerlink" title="一次变更多台"></a>一次变更多台</h3><h4 id="方式-1"><a href="#方式-1" class="headerlink" title="方式"></a>方式</h4><p>这种变更方式可以一次性变更多个节点（arbitrary configuration）。</p><p>当集群成员在变更时，为了保证服务的可用性（不发生中断），以及避免因为节点变更导致的一致性问题， Raft 提出了两阶段变更，当接收到新的配置文件后，集群会首先进入 joint consensus 状态， 待新的配置文件提交成功后，再回到普通状态。</p><p>更具体的，joint consensus 指的是包含新／旧配置文件全部节点的中间状态：</p><ul><li>entries 会被复制到新／旧配置文件中的所有节点；</li><li>新／旧配置文件中的任何一个节点都有可能被选为 leader；</li><li>共识（选举或提交）需要同时在新／旧配置文件中分别获取到多数同意（<code>separate majorities</code>）</li></ul><p>（注：<code>separate majorities</code>的意思是需要新／旧集群中的多数都同意。比如如果是从 3 节点切换为全新的 9 节点， 那么要求旧节点中的 2 节点，和新节点中的 4 节点都同意，才被认为达成了一次共识。）</p><p>所以，在一次配置变更中，一共有三个状态：</p><ul><li><code>C_new</code>：使用旧的配置文件；</li><li><code>C_old,new</code>：同时使用新旧配置文件，也就是新／旧节点的并集；</li><li><code>C_new</code>：使用新的配置文件。</li></ul><p>配置文件使用特殊的 entries 进行存储，一个节点一旦获取到新的配置文件， 即使该配置 entry 并没有 committed，也会立刻使用该配置。 所以一次完整的配置变更可以表示为下图：</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/raft_jointchange.png" srcset="/img/loading.gif" alt=""></p><ol><li>C_old,new 被创建，集群进入 joint consensus，leader 开始传播该 entry；</li><li>C_old,new 被 committed，也就是说此时多数节点都拥有了 C_old,new，此后 C_old 已经不再可能被选为 leader；</li><li>leader 创建并传播 C_new；</li><li>C_new 被提交，此后不在 C_new 内的节点不允许被选为 leader，如有 leader 不在 C_new 则自行退位。</li></ol><h4 id="删除当前节点的有趣现象"><a href="#删除当前节点的有趣现象" class="headerlink" title="删除当前节点的有趣现象"></a>删除当前节点的有趣现象</h4><p>leader 可能不在新配置文件的节点之中</p><p>在原始论文中，leader 会持续工作直到 C_new 被提交。</p><p>当 C_new 被 committed 后，任何不在 C_new 中的 leader，立刻退化为 follower。 在试图提交 C_new 时，不在 C_new 的 leader 不参与计票。</p><p>在其博士论文中，当一个 leader 发现自己不在新配置文件中，在 C_new 提交后， 可以采取 3.10 节（p46）提到的 leadership transfer 机制，交出自己的管理权。</p><p>上述两种做法都导致两个很奇特的现象：</p><ul><li>这个 leader 可能会管理一个不包括自己的集群；</li><li>一个服务器可能在自己的配置文件都不包含自己的情况下参与选举，甚至成为 leader。</li></ul><h2 id="日志打包"><a href="#日志打包" class="headerlink" title="日志打包"></a>日志打包</h2><p>当日志 entries 数量过多时，节点间同步会耗费太多时间，最简单的优化办法就是定期做 snapshot。</p><p>snapshot 会包括：</p><ul><li>状态机当前的状态；</li><li>最后一块 entry 的 index 和 term（为了兼容其他 RPC 请求的参数）；<br>当前集群配置信息。</li><li>各个节点自行择机完成自己的 snapshot。</li></ul><p>如果 leader 发现需要发给某一个 follower 的 nextIndex 已经被做成了 snapshot， 则需要将 snapshot 发送给该 follower。</p><p>当 follower 接收到 snapshot 后，需要做出判断：</p><ul><li>如果 snapshot 领先于自己的 log，则使用 snapshot 完全替换自己的所有的 log；</li><li>如果 snapshot 落后于自己的 log，则使用 snapshot 替换掉该部分的 log，而保留后续的 log。</li></ul><p>snapshot 可能会带来两个问题：</p><ol><li><p>何时 snapshot？<br>一个简单的策略是设置一个固定的最大磁盘容量，当 log 超过这个容量时，就触发 snapshot。</p></li><li><p>对状态机写 snapshot 时，会影响新的更新操作。<br>建议采用 <code>copy-on-write</code> 操作，来尽可能少的影响新的更新操作。</p></li></ol><h2 id="禅让"><a href="#禅让" class="headerlink" title="禅让"></a>禅让</h2><p>有时候，会希望取消当前 leader 的管理权，比如：</p><ul><li>leader 节点因为运维原因需要重启；</li><li>有其他更适合当 leader 的节点；</li></ul><p>直接将 leader 节点停机的话，其他节点会等待 electionTimeout 后进入选举状态， 这期间会集群会停止响应。为了避免这一段不可用的时间，可以采用禅让机制（<code>leadership transfer</code>）。</p><p>禅让的步骤为：</p><ol><li>leader 停止响应客户端请求；</li><li>leader 向 target 节点发起一次日志同步；</li><li>leader 向 target 发起一次 TimeoutNowRPC，target 收到该请求后立刻发起一轮投票。</li></ol><p>etcd 中实现了更多的细节（也有一些改动）：</p><ol><li>leader 先检查禅让对象（leadTransferee）的身份，如果是 follower，直接忽略；</li><li>leader 检查是否有正在进行的禅让，如果有，则中止之前的禅让状态，开始处理最新的请求；</li><li>检查禅让对象是否是自己，如果是，忽略；</li><li>将禅让状态信息计入 leader 的状态，并且重置 electionElapsed（因为禅让应该在 electionTimeout 内完成）；</li><li>检查禅让对象的日志是否是最新的</li><li>如果禅让对象已经是最新，则直接发送 TimeoutNowRPC</li><li>如果不是，则发送 AppendEntriesRPC，待节点响应成功后，再发送 TimeoutNowRPC</li></ol><p>可以看出，在 etcd 中，leader 除了重置 electionElapsed 外，不会改动自己的状态。 既不会停止对客户端的响应，同时还会继续发送心跳。</p><p>因为 target 机器会更新自己的 term，而且率先发起投票，其有很大的概率赢得选举。 需要注意的是，target 发起的 RequestVoteRPC 中的 <code>isLeaderTransfer=true</code>， 以防止被其他节点忽略。</p><p>如果 target 机器没能在一次 electionTimeout 内完成选举，那么 leader 认为本次禅让失败， 立刻恢复响应客户端的请求。（这时可以再次重新发起一次禅让请求）</p><p>在 etcd/raft 中，RequestVoteRPC.context 会被设置为 campaignTransfer, 表明本次投票请求来源于 leader transfer，可以强行打断 follower 的租约发起选举。</p><h2 id="提前投票"><a href="#提前投票" class="headerlink" title="提前投票"></a>提前投票</h2><p>一个暂时脱离集群网络的节点，在重新加入集群后会干扰到集群的运行。</p><p>因为当一个节点和集群失去联系后，在等待 electionTimeout 后，它就会增加自己的 term 并发起选举， 因为联系不上其他节点，所以在 electionTimeout 后，它会继续增加自己的 term 并继续发起选举。</p><p>一段时间以后，它的 term 就会显著的高于原集群的 term。如果此后该节点重新和集群恢复了联络， 它的高 term 会导致 leader 立刻退位，并重新举行选举。</p><p>为了避免这一情形，引入了 Pre-Vote 的机制。在该机制下，一个 candidate 必须在获得了多数赞同的情形下， 才会增加自己的 term。一个节点在满足下述条件时，才会赞同一个 candidate：</p><ul><li>该 candidate 的日志足够新；</li><li>当前节点已经和 leader 失联（electionTimeout）。</li></ul><p>也就是说，candidate 会先发起一轮 Pre-Vote，获得多数同意后，更新自己的 term， 再发起一轮 RequestVoteRPC。</p><p>这种情形下，脱离集群的节点，只会不断的发起 Pre-Vote，而不会更新自己的 term。</p><p>在 etcd 的实现中，如果某个节点赞同了某个 candidate， 是不需要更新自己的状态的，它依然可以赞同其他 candidate。 而且，即使收到的 PreVote 的 term 大于自己，也不会更新自己的 term。 也就是说，PreVote 不会改变其他节点的任何状态。</p><p>etcd 中还有一个设计是，当发起 PreVote 的时候，针对的是下一轮的 term， 所以会向所有的节点发送一个 term+1 的 PreVoteReq。</p><pre><code class="GO">func (r *raft) campaign(t CampaignType) {    var term uint64    var voteMsg pb.MessageType    if t == campaignPreElection {        r.becomePreCandidate()        voteMsg = pb.MsgPreVote        // 这里需要注意的是，PreVote 会针对“下一轮 term”发起投票，        // 而 Vote 则是针对当前 term        // PreVote RPCs are sent for the next term before we&#39;ve incremented r.Term.        term = r.Term + 1    } else {        r.becomeCandidate()        voteMsg = pb.MsgVote        term = r.Term    }    // ...    // 发送投票请求    r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx})    // ...}</code></pre><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>所有节点间仅通过三种类型的 RPC 进行通信：</p><ul><li><code>AppendEntriesRPC</code>：最常用的，leader 向 follower 发送心跳或同步日志。</li><li><code>RequestVoteRPC</code>：选举时，candidate 发起的竞选请求。</li><li><code>InstallsnapshotRPC</code>：用于 leader 下发 snapshot。</li></ul><p>在 Diego 后续的博士论文中，又增加了一些 RPCs：</p><ul><li><code>AddServerRPC</code>：添加单台节点。</li><li><code>RemoveServerRPC</code>：移除一个节点。</li><li><code>TimeoutNowRPC</code>：立刻发起竞选。<br>（实际上 etcd 的实现中定义了几十种消息类型，甚至把内部事件也封装为消息一并处理。）</li></ul><h3 id="AppendEntriesRPC"><a href="#AppendEntriesRPC" class="headerlink" title="AppendEntriesRPC"></a>AppendEntriesRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 当前的 term；</li><li><code>leaderId</code>：leader 的 节点id，让 follower 可以重定向客户端的连接；</li><li><code>prevLogIndex</code>：前一块 entry 的 index；</li><li><code>prevlogterm</code>：前一块 entry 的 term；</li><li><code>entries[]</code>：给 follower 发送的 entry，可以一次发送多个，heartbeat 时该项可缺省；</li><li><code>leaderCommit</code>：leader 当前的 <code>committed index</code>，follower 收到后可用于自己的状态机。</li></ul><p>返回：</p><ul><li><code>term</code>：响应者自己的 term；</li><li><code>success</code>：bool，是否接受请求。<br>该请求通过 leaderCommit 通知 follower 提交相应的 entries 到。通过 entries[] 复制 leader 的日志到所有的 follower。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，立刻返回 false</li><li>如果 prevLogIndex 不匹配，返回 false</li><li>如果自己有块 entry 和新的 entry 不匹配（在相同的 index 上有不同的 term）， 删除自己的那一块以及之后的所有 entry；</li><li>把新的 entries 添加到自己的 log；<br>5 。如果 <code>leaderCommit &gt; commitindex</code>，将 commitIndex 设置为 <code>min(leaderCommit, last index)</code>， 并且提交相应的 entries。</li></ol><h3 id="RequestVoteRPC"><a href="#RequestVoteRPC" class="headerlink" title="RequestVoteRPC"></a>RequestVoteRPC</h3><p>参数：</p><ul><li><code>term</code>：candidate 当前的 term；</li><li><code>candidateId</code>：candidate 的节点 id</li><li><code>lastlogindex</code>：candidate 最后一个 entry 的 index；</li><li><code>lastlogterm</code>：candidate 最后一个 entry 的 term。</li><li><code>isleaderTransfer</code>：用于表明该请求来自于禅让，无需等待 electionTimeout，必须立刻响应。</li><li><code>isPreVote</code>：用来表明当前是 PreVote 还是真实投票</li></ul><p>返回：</p><ul><li><code>term</code>：响应者当前的 term；</li><li><code>voteGranted</code>：bool，是否同意投票。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，返回 false；</li><li>如果 votedFor 为空或者为该 <code>candidated id</code>，且日志项不落后于自己，则同意投票。</li></ol><h3 id="InstallsnapshotRPC"><a href="#InstallsnapshotRPC" class="headerlink" title="InstallsnapshotRPC"></a>InstallsnapshotRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 的 term</li><li><code>leaderId</code>：leader 的 节点 id</li><li><code>lastIncludedindex</code>：snapshot 中最后一块 entry 的 index；</li><li><code>lastIncludedterm</code>：snapshot 中最后一块 entry 的 term；</li><li><code>offset</code>：该份 chunk 的 offset；</li><li><code>data[]</code>：二进制数据；</li><li><code>done</code>：是否是最后一块 chunk</li></ul><p>返回：</p><ul><li><code>term</code>：follower 当前的 term</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code> 就立即回复</li><li>如果是第一个分块（offset 为 0）就创建一个新的快照</li><li>在指定偏移量写入数据</li><li>如果 done 是 false，则继续等待更多的数据</li><li>保存快照文件，丢弃索引值小于快照的日志</li><li>如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保持</li><li>丢弃整个日志</li><li>使用快照重置状态机</li></ol><h3 id="AddServerRPC"><a href="#AddServerRPC" class="headerlink" title="AddServerRPC"></a>AddServerRPC</h3><p>参数：</p><ul><li><code>newServer</code>：新节点地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否添加成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>如果没有在 electionTimeout 内处理，则返回 TIMEOUT；<br>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="RemoveServerRPC"><a href="#RemoveServerRPC" class="headerlink" title="RemoveServerRPC"></a>RemoveServerRPC</h3><p>参数：</p><ul><li><code>oldServer</code>：要删除的节点的地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否删除成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>如果没有在 electionTimeout 内处理，则返回 TIMEOUT；<br>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="TimeoutNowRPC"><a href="#TimeoutNowRPC" class="headerlink" title="TimeoutNowRPC"></a>TimeoutNowRPC</h3><p>由 leader 发起，告知 target 节点立刻发起竞选，无视 electionTimeout。主要用于禅让。</p><h2 id="客户端"><a href="#客户端" class="headerlink" title="客户端"></a>客户端</h2><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2>]]></content>
    
    
    <categories>
      
      <category>分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CAP 定理介绍</title>
    <link href="/cap-theory/"/>
    <url>/cap-theory/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在互联网行业飞速发展的 21 世纪，分布式系统正变得越来越重要，大型互联网公司如 Google, Amazon, MicroSoft, Alibaba, Tencent 等之所以被认为技术很厉害，很大程度上是因为其后台十分强悍，而这些后台一定是由若干个大的分布式系统组成的，因此理解分布式系统的运行原理对于程序员有非常重要的意义。</p><p>CAP 定理是分布式系统方向一个比较宽泛但很重要的基本定理，也可以作为理解分布式系统的起点。这篇博客将详细介绍 CAP 定理并简单证明，最后谈一谈 CAP 定理在工业界的应用。</p><h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>2000年，柏克莱加州大学（University of California, Berkeley）的计算机科学家 Eric Brewer 在分布式计算原则研讨会（Symposium on Principles of Distributed Computing）提出，分布式系统有三个指标。</p><ul><li>Consistency</li><li>Availability</li><li>Partition tolerance</li></ul><p>它们的第一个字母分别是 C、A、P。</p><p>Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。</p><p>需要注意的是，尽管我们常说某个系统能够满足 CAP 属性中的 2 个，但并不是必须满足 2 个，许多系统只具有 0 或 1 个 CAP 属性。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h3><p>我们知道 ACID 中事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行前后，数据库都必须处于一致性状态。也就是说，事务的执行结果必须是使数据库从一个一致性状态转变到另一个一致性状态。</p><p>和 ACID 中的一致性不同，分布式环境中的一致性是指数据在多个副本之间是否能够保持一致的特性。</p><p>分布式系统中，数据一般会存在不同节点的副本中，如果对第一个节点的数据成功进行了更新操作，而第二个节点上的数据却没有得到相应更新，这时候读取第二个节点的数据依然是更新前的数据，即脏数据，这就是分布式系统数据不一致的情况。</p><p>在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都能读取到最新的值，那么这样的系统就被认为具有强一致性（或严格的一致性）。</p><h3 id="Availability"><a href="#Availability" class="headerlink" title="Availability"></a>Availability</h3><p>可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果，如果超过了这个时间范围，那么系统就被认为是不可用的。</p><p>“有限的时间内”是在系统的运行指标，不同系统会有差别。例如搜索引擎通常在 0.5 秒内需要给出用户检索结果。</p><p>”返回结果”是可用性的另一个重要指标，它要求系统完成对用户请求的处理后，返回一个正常的响应结果，要明确的反映出对请求处理的成功或失败。如果返回的结果是系统错误，比如”OutOfMemory”等报错信息，则认为此时系统是不可用的。</p><h3 id="Partition-Tolerance"><a href="#Partition-Tolerance" class="headerlink" title="Partition Tolerance"></a>Partition Tolerance</h3><p>一个分布式系统中，节点组成的网络本来应该是连通的。然而可能因为某些故障，使得有些节点之间不连通了，整个网络就分成了几块区域，而数据就散布在了这些不连通的区域中，这就叫分区。</p><p>当你一个数据项只在一个节点中保存，那么分区出现后，和这个节点不连通的部分就访问不到这个数据了。这时分区就是无法容忍的。</p><p>提高分区容忍性的办法就是一个数据项复制到多个节点上，那么出现分区之后，这一数据项仍然能在其他区中读取，容忍性就提高了。然而，把数据复制到多个节点，就会带来一致性的问题，就是多个节点上面的数据可能是不一致的。要保证一致，每次写操作就都要等待全部节点写成功，而这等待又会带来可用性的问题。</p><p>总的来说就是，数据存在的节点越多，分区容忍性越高，但要复制更新的数据就越多，一致性就越难保证。为了保证一致性，更新所有节点数据所需要的时间就越长，可用性就会降低。</p><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><h3 id="简单理解"><a href="#简单理解" class="headerlink" title="简单理解"></a>简单理解</h3><p>根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。理解 CAP 理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了 C 性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了 A 性质。除非两个节点可以互相通信，才能既保证 C 又保证 A，这又会导致丧失 P 性质。</p><h3 id="详细证明"><a href="#详细证明" class="headerlink" title="详细证明"></a>详细证明</h3><p><img src="http://q5ijnj5w7.bkt.clouddn.com/cap_base.png" srcset="/img/loading.gif" alt=""></p><p>我们现在有两个网络 N1 和 N2，每个网络中都存在一个服务用于从 db 获取数据，初始状态下，db 中存储的数据都是 V0。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/cap_p.png" srcset="/img/loading.gif" alt=""></p><p>正常情况下，在网络 N1 通过服务 A 更新 V0 到 V1，更新成功后发送消息 M 使 N2 的 db 中的 V0 变为 V1，此时我们通过服务 B 获取数据时，获取到 V1。</p><pre><code>此时满足 CA，没有分区故不满足 P。</code></pre><p><img src="http://q5ijnj5w7.bkt.clouddn.com/cap_withoutp.png" srcset="/img/loading.gif" alt=""></p><p>但是一旦发生了网络分区，此时我们通过服务 A 更新数据到 V1 后，由于网络错误，V1 值同步不到 N2 网络中去，此时我们调用服务 B 去请求数据的时候，我们必须从 C 和 A 选一个，如果选择 C，我们需要等到数据同步到 N2，但是从服务 B 获取数据肯定是失败了，失去了 A。如果选择 A，那么从 B 我们获取到的数据不是最新的，失去了 C。</p><pre><code>此时有分区故满足 P，CA 只能满足一个。</code></pre><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="取舍策略"><a href="#取舍策略" class="headerlink" title="取舍策略"></a>取舍策略</h3><h4 id="CP-without-A"><a href="#CP-without-A" class="headerlink" title="CP without A"></a>CP without A</h4><p>如果一个分布式系统不要求强的可用性，即容许系统停机或者长时间无响应的话，就可以在 CAP 三者中保障 CP 而舍弃 A。</p><p>一个保证了 CP 而一个舍弃了 A 的分布式系统，一旦发生网络故障或者消息丢失等情况，就要牺牲用户的体验，等待所有数据全部一致了之后再让用户访问系统。</p><p>设计成 CP 的系统其实也不少，其中最典型的就是很多分布式数据库，他们都是设计成 CP 的。在发生极端情况时，优先保证数据的强一致性，代价就是舍弃系统的可用性。如 Redis、HBase 等，还有分布式系统中常用的 Zookeeper 也是在 CAP 三者之中选择优先保证 CP 的。</p><p>无论是像 Redis、HBase 这种分布式存储系统，还是像 Zookeeper 这种分布式协调组件。数据的一致性是他们最最基本的要求。一个连数据一致性都保证不了的分布式存储要他有何用？</p><p>ZooKeeper 是个 CP（一致性+分区容错性）的，即任何时刻对 ZooKeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。但是它不能保证每次服务请求的可用性，也就是在极端环境下，ZooKeeper 可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。ZooKeeper 是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致。所以就不难理解为什么 ZooKeeper 被设计成 CP 而不是 AP 特性的了。</p><h4 id="AP-wihtout-C"><a href="#AP-wihtout-C" class="headerlink" title="AP wihtout C"></a>AP wihtout C</h4><p>要高可用并允许分区，则需放弃一致性。一旦网络问题发生，节点之间可能会失去联系。为了保证高可用，需要在用户访问时可以马上得到返回，则每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。</p><p>这种舍弃强一致性而保证系统的分区容错性和可用性的场景和案例非常多。前面我们介绍可用性的时候说到过，很多系统在可用性方面会做很多事情来保证系统的全年可用性可以达到 N 个 9，所以，对于很多业务系统来说，比如淘宝的购物，12306 的买票。都是在可用性和一致性之间舍弃了一致性而选择可用性。</p><p>你在 12306 买票的时候肯定遇到过这种场景，当你购买的时候提示你是有票的（但是可能实际已经没票了），你也正常的去输入验证码，下单了。但是过了一会系统提示你下单失败，余票不足。这其实就是先在可用性方面保证系统可以正常的服务，然后在数据的一致性方面做了些牺牲，会影响一些用户体验，但是也不至于造成用户流程的严重阻塞。</p><p>但是，我们说很多网站牺牲了一致性，选择了可用性，这其实也不准确的。就比如上面的买票的例子，其实舍弃的只是强一致性。退而求其次保证了最终一致性。也就是说，虽然下单的瞬间，关于车票的库存可能存在数据不一致的情况，但是过了一段时间，还是要保证最终一致性的。</p><p>对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，而且要保证服务可用性达到 N 个 9，即保证 P 和 A，舍弃 C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。</p><h4 id="CA-without-P"><a href="#CA-without-P" class="headerlink" title="CA without P"></a>CA without P</h4><p>这种情况在分布式系统中几乎是不存在的。首先在分布式环境下，网络分区是一个自然的事实。因为分区是必然的，所以如果舍弃 P，意味着要舍弃分布式系统。那也就没有必要再讨论 CAP 理论了。这也是为什么在前面的 CAP 证明中，我们以系统满足 P 为前提论述了无法同时满足 C 和 A。</p><p>比如我们熟知的关系型数据库，如 Mysql 和 Oracle 就是保证了可用性和数据一致性，但是他并不是个分布式系统。一旦关系型数据库要考虑主备同步、集群部署等就必须要把 P 也考虑进来。</p><p>其实，在 CAP 理论中。C，A，P 三者并不是平等的，CAP 之父在《Spanner，真时，CAP 理论》一文中写到：</p><blockquote><p>如果说 Spanner 真有什么特别之处，那就是谷歌的广域网。Google 通过建立私有网络以及强大的网络工程能力来保证 P，在多年运营改进的基础上，在生产环境中可以最大程度的减少分区发生，从而实现高可用性。</p></blockquote><p>从 Google 的经验中可以得到的结论是，一直以来我们可能被 CAP 理论蒙蔽了双眼，CAP 三者之间并不对称，C 和 A 不是 P 的原因（P 不能和 CA trade-off，CP 和 AP 中不存在 trade-off，trade-off 在 CA 之间）。提高一个系统的抗毁能力或者说提高 P（分区容忍能力）是通过提高基础设施的稳定性来获得的，而不是通过降低 C 和 A 来获得的。也就说牺牲 C 和 A 也不能提高 P。</p><p>所以，对于一个分布式系统来说。P 是一个基本要求，CAP 三者中，只能在 CA 两者之间做权衡，并且要想尽办法提升 P。P 提升的越好，CA 同时满足就越有可能。</p><h3 id="业界应用分析"><a href="#业界应用分析" class="headerlink" title="业界应用分析"></a>业界应用分析</h3><table><thead><tr><th>应用</th><th>类型</th><th>解释</th></tr></thead><tbody><tr><td>Mysql</td><td>CA</td><td>主从模式为 AP</td></tr><tr><td>Spanner</td><td>CA/CP</td><td>技术实现是 CP 但号称是 CA，宣称 CA 系统并不意味着 100％ 的可用性</td></tr><tr><td>分布式协议-Raft/ZAB/Paxos</td><td>CP</td><td>在分区后，对于 A，只有分区内节点大于 Quorum 才对外服务</td></tr><tr><td>分布式事务-2PC</td><td>CP</td><td>锁住资源,该资源其他请求阻塞</td></tr><tr><td>分布式事务-TCC</td><td>AP</td><td>最终一致性</td></tr><tr><td>分布式事务-最大努力尝试</td><td>AP</td><td>最终一致性</td></tr><tr><td>DNS服务</td><td>AP</td><td>最终一致性</td></tr></tbody></table><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于多数大型互联网应用的场景，主机众多、部署分散，而且现在的集群规模越来越大，所以节点故障、网络故障是常态，因此分区容错性也就成为了一个分布式系统必然要面对的问题，那么就只能在 C 和 A 之间进行取舍。</p><p>对于某些安全性要求极高的项目，比如银行的转账系统，涉及到金钱的对于数据一致性不能做出一丝的让步，C 必须保证，出现网络故障的话，宁可停止服务，也不能冒着出错误的风险继续提供服务。</p><p>对于网站，DNS 服务等，其内容的实时性不是特别严格，则可以牺牲一定的一致性，保证最高的可用性是最好的选择。</p><p>个人认为，CAP 定理的核心在于，在网络分区的情况下，我们需要对 C 和 A 做出相应的妥协，我们不可能完全满足 CA，但是我们可以合理控制 C 和 A 之间的比例让我们的应用/中间件正常提供服务，同时也尽量提升基础设施的稳定性来保障 P。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>《Spanner，真时，CAP 理论》是 Google VP，CAP 理论之父在情人节当天撰写的，主要介绍了 Google 的 Spanner 数据库的真时（TrueTime）服务和 CA 特性，以及结合 CAP 理论的一些思考，建议阅读，阅读 Spanner 论文后阅读更佳。</p><ul><li><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45855.pdf" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》</a></p></li><li><p><a href="https://toutiao.io/posts/zdqrx0/preview" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》中文</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>分布式系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iTerm2 快捷键介绍</title>
    <link href="/iTerm2-hotkeys/"/>
    <url>/iTerm2-hotkeys/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>iTerm2 是 MacOS 独有的终端工具，其有许多快捷键可以使用。为了便于开发并节约之后再次在搜索引擎上查询的时间成本，特写此博客以供自己日后查看。</p><h2 id="快捷键介绍"><a href="#快捷键介绍" class="headerlink" title="快捷键介绍"></a>快捷键介绍</h2><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><ul><li>新建标签：Command + T</li><li>关闭标签：Command + W</li><li>切换标签：Command + 数字 或 Command + 左右方向键</li></ul><h3 id="分屏"><a href="#分屏" class="headerlink" title="分屏"></a>分屏</h3><ul><li>垂直分屏：Command + D</li><li>水平分屏：Command + Shift + D</li><li>切换屏幕：Command + Option + 方向键 或 Command + [ / ]</li></ul><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><ul><li>局部搜索(包含单个终端)：Command + F</li><li>全局搜索(包含所有Tab)：Command + Option + E</li><li>搜索历史指令：Ctrl + R</li></ul><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><ul><li>查看历史命令：Command + ;</li><li>查看剪贴板历史：Command + Shift + H</li><li>上一条命令：Ctrl + P 或 上方向键</li></ul><h3 id="单行"><a href="#单行" class="headerlink" title="单行"></a>单行</h3><ul><li>光标到行首：Ctrl + A</li><li>光标到行尾：Ctrl + E</li><li>删除当前行：Ctrl + U</li><li>删除当前光标的字符：Ctrl + D</li><li>删除光标之前的字符：Ctrl + H</li><li>删除光标之前的单词：Ctrl + W</li><li>删除到文本末尾：Ctrl + K</li></ul><h3 id="内容大小"><a href="#内容大小" class="headerlink" title="内容大小"></a>内容大小</h3><ul><li>放大终端：Command + +</li><li>缩小终端：Command + - </li></ul><h3 id="常用快捷功能"><a href="#常用快捷功能" class="headerlink" title="常用快捷功能"></a>常用快捷功能</h3><ul><li>清屏：Command + R 或 Crtl + L</li><li>切换全屏：Command + Enter</li><li>选中即复制：在 iTerm2 界面，选择了一行就已经复制了</li></ul>]]></content>
    
    
    <categories>
      
      <category>开发工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的高效 Macbook 工作环境配置</title>
    <link href="/mac-configuration/"/>
    <url>/mac-configuration/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>工欲善其事，必先利其器，工具永远都是用来解决问题的，没必要为了工具而工具，一切工具都是为了能快速准确的完成工作和学习任务而服务。</p><p>我呢，在使用了 Windows，Ubuntu 和 MacOS 三种操作系统之后。结合种种体验和踩坑，最终还是觉得 MacOS 更舒适一点。每个人都有每个人的看法，每个人都有每个人的舒适点，MacOS 恰好捏住了我的舒适点。因此，我之后都将从 MacOS 上工作学习。</p><p>前一段时间我从公司实习离职，上交了公司发给我的 MacBook（<del>停止薅羊毛</del>），然而我又不想回到 Windows，于是打算自己买一台 MacBook。但是 MacBook 从 2016 年开始更换的蝶式键盘很让我恶心，姑且不说故障率高，触感实在太差劲了。尽管 2020 年新出的 16 寸 Pro 已经重回剪刀脚键盘了，但是我的需求是轻薄的 13 寸而不是 16 寸（<del>只是没钱而已</del>）。尽管听到业界的呼声说 2020 年的 MacBook 应该都会回到剪刀脚键盘，但由于 2020 年会换新模具，我也不想踩第一代模具的坑，因而暂且将目标定为 2021 年的 MacBook，目前一年多买个二手过渡下就可以了。</p><p>1 月份我在某宝平台上买了一台 2014 款 8+256 的二手 MacBook Pro，即使前期做了许多选店和辨伪的功课，拿到手之后却依然中招，总是无理由黑屏然后再无法开机一天，十分坑爹。所幸可以十五天无理由退换货，就赶快退了。之前早就听说二手 Mac 的水很深，被坑一次之后更加确信。接着我做了更多的功课，学到了许多辨伪技巧，浏览了许多店铺，也算有点心得，之后要是有时间可以写出来分享给大家。</p><p>前几天经过慎重选择我又在某东平台上入手了一台 2015 款 8+128 的二手 MacBook Pro。这次总算没什么问题，但比较有趣的一点是我买的 8+128 的，老板发给我的是 8+256 的，平白无故赚了 128G 的固态，只能说真的舒服了。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/home.jpeg" srcset="/img/loading.gif" alt=""></p><p>这是一个新的 MacBook 刚打开后的主页，接下来我要通过一系列的配置使其成为一个十分符合我开发习惯的机器，可供大家参考。</p><h2 id="系统篇"><a href="#系统篇" class="headerlink" title="系统篇"></a>系统篇</h2><h3 id="触屏板"><a href="#触屏板" class="headerlink" title="触屏板"></a>触屏板</h3><ul><li>2016 年及之后的 MacBook 触屏板都有 Force touch 的功能，即可以按压两次来实现更多的功能，但是我一直用不来这个功能，因此我的第一件事就是调整触摸屏板，首先先关掉 Force touch 的功能，然后开启轻点来点按的点击方式，个人觉得这样才符合 MacBook 轻巧的特性嘛，每次都按下去多麻烦啊，现在手指轻轻一碰触摸板，就达到鼠标单击的顺滑效果。</li><li>除此以外，可以根据自己的习惯开启或关闭一些手势。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/touch_1.png" srcset="/img/loading.gif" alt=""><br><img src="http://q5ijnj5w7.bkt.clouddn.com/touch_2.png" srcset="/img/loading.gif" alt=""><br><img src="http://q5ijnj5w7.bkt.clouddn.com/touch_3.png" srcset="/img/loading.gif" alt=""></p><h3 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h3><ul><li>由于 MacBook 默认的重复前延迟和按键重复配置太慢，限制了程序员们优秀的打字速度，所以建议都调整到最快的速度。</li><li>可以在闲置 5 分钟后关闭键盘背光灯来省点电。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/keyboard.png" srcset="/img/loading.gif" alt=""></p><h3 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h3><ul><li>由于 MacBook 默认的切换大小写的方式是长按 Caps 键，时间较慢需要等待，较为影响开发效率，建议关闭长按改为短按，配合极低的按键延迟会十分舒爽。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/input.png" srcset="/img/loading.gif" alt=""></p><ul><li>建议安装搜狗输入法 Mac 版替代系统自带输入法。</li></ul><h3 id="快速锁定屏幕"><a href="#快速锁定屏幕" class="headerlink" title="快速锁定屏幕"></a>快速锁定屏幕</h3><ul><li><p>如果你长时间离开电脑，最好锁定你的屏幕，以防止数据泄露。 那如何快速的锁定你的 MacBook 呢？ 答案是只需要一摸触摸板就可以了。</p><ul><li><p>打开系统偏好设置，点击桌面与屏幕保护程序图标，选择屏幕保护程序这个 Tab，再点击触发角，在弹出的如下界面里面，右下角选择将显示器置入睡眠状态，再确定即可。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/screen_saver.png" srcset="/img/loading.gif" alt=""></p></li><li><p>再打开系统偏好设置，点击安全性与隐私图标，在通用 Tab 内，勾选为进入睡眠或开始屏幕保护程序<strong>立即</strong>要求输入密码。</p><p><img src="http://q5ijnj5w7.bkt.clouddn.com/screen_security.png" srcset="/img/loading.gif" alt=""></p></li></ul></li></ul><h2 id="开发环境篇"><a href="#开发环境篇" class="headerlink" title="开发环境篇"></a>开发环境篇</h2><h3 id="Xcode"><a href="#Xcode" class="headerlink" title="Xcode"></a>Xcode</h3><ul><li><p>首先安装 Xcode，然后使用下面的命令安装 Xcode command line tools，这将为我们安装很多终端下面常用的命令，将来很可能会使用到。</p><pre><code class="Shell">  xcode-select --install</code></pre></li></ul><h3 id="Homebrew"><a href="#Homebrew" class="headerlink" title="Homebrew"></a>Homebrew</h3><ul><li><p>Homebrew 是一款终端下的命令程序包管理器，安装非常简单，复制如下命令在终端下运行，按回车并输入密码后等待安装成功：</p><pre><code class="Shell">  ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</code></pre></li></ul><h3 id="iTerm2-Zsh-Z"><a href="#iTerm2-Zsh-Z" class="headerlink" title="iTerm2 + Zsh + Z"></a>iTerm2 + Zsh + Z</h3><ul><li>常用终端 iTerm2 + 优秀 Shell Zsh + 扁平目录跳转命令 Z，安装好之后开发十分舒适。具体安装可参考这个<a href="https://www.jianshu.com/p/a5f478a143dc" target="_blank" rel="noopener">博客</a>。</li></ul><h3 id="快捷键迅速打开-iTerm2"><a href="#快捷键迅速打开-iTerm2" class="headerlink" title="快捷键迅速打开 iTerm2"></a>快捷键迅速打开 iTerm2</h3><ul><li>可以设置快捷键再 Home 页面输入 Command + , 直接打开 iTerm2，这样就不用再去点击 iTerm2 了。</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/iTerm2_hotkey.png" srcset="/img/loading.gif" alt=""></p><ul><li>可以设置 iTerm2 默认占满全屏，这样子快捷键打开之后就直接是一个全屏的 iTerm2 可以使用了</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/iTerm2_screen.png" srcset="/img/loading.gif" alt=""> </p><h3 id="VScode命令行迅速打开"><a href="#VScode命令行迅速打开" class="headerlink" title="VScode命令行迅速打开"></a>VScode命令行迅速打开</h3><ul><li>打开VScode后输入 Command + Shift + P 打开命令面板，再输入 code，再确定</li></ul><p><img src="http://q5ijnj5w7.bkt.clouddn.com/vscode_code.png" srcset="/img/loading.gif" alt=""></p><h3 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h3><ul><li>创建新的公钥私钥并与自己的 Github 账户连起来，这样就可以开始在 Github 遨游啦。</li></ul><h2 id="常用软件"><a href="#常用软件" class="headerlink" title="常用软件"></a>常用软件</h2><ul><li>网易云音乐</li><li>微信</li><li>QQ</li><li>SSR</li><li>Chrome</li><li>VScode</li><li>IDEA</li><li>Docker</li><li>…</li></ul>]]></content>
    
    
    <categories>
      
      <category>开发工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
