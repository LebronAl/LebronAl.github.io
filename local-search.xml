<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Spanner 论文阅读</title>
    <link href="/spanner-thesis/"/>
    <url>/spanner-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>Google Spanner 是 Google 一篇跨时代的论文，开启了 NewSQL 时代的序幕。</p><p>其主要有三点特色：</p><ul><li>2PC + 共识组来避免 2PC 的无限超时阻塞。</li><li>GPS 原子钟同步技术以支持快速的只读事务。</li><li>支持 ACID 的全球型 NewSQL 数据库。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>暂仅搬运一些资料，之后有时间再补。</p><p><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-spanner.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="http://nil.csail.mit.edu/6.824/2020/video/13.html" target="_blank" rel="noopener">6.824 视频</a><br><a href="http://nil.csail.mit.edu/6.824/2020/papers/spanner.pdf" target="_blank" rel="noopener">论文</a><br><a href="https://cloud.google.com/spanner/docs/whitepapers?hl=zh-cn" target="_blank" rel="noopener">Spanner 白皮书</a><br><a href="https://toutiao.io/posts/zdqrx0/preview" target="_blank" rel="noopener">Spanner，True Time 和 CAP</a></p><h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><ol><li>在 Spanner 中，只读事务可以从本地数据中心读取数据，以提高性能。但是，如果只读事务向少数副本发出请求，它可能得到过时的数据。这种行为可能会破坏严格的可序列化保证。那么 Spanner 如何应对这种情况以保持其正确性条件呢?</li></ol><blockquote><p>在 Spanner 中，每个事务都会根据 TrueTime 的 Start Rule 选择一个时间戳作为事务 id：对于写请求，其会在提交事务时 2pc 的 prepare 阶段选择 TT.now().latest 作为事务 id；对于读请求，其会在读事务开始时选择 TT.now().latest 作为事务 id。</p><p>每个 replica 都会维护一个递增的 t<sub>safe</sub>  变量，对于读事务 T，当 t<sub>safe</sub>  &gt;= 读事务 T 的 tid 时，可以执行该读事务而不违背外部一致性。</p><p>对于 t_safe，其等于 min(t<sub>safe</sub><sup>paxos</sup>,t<sub>safe</sub><sup>tm</sup>)。对于 t<sub>safe</sub><sup>paxos</sup>，其等于当前 replica 能够看到的最新写事务的 tid，注意到 Paxos 的 leader 会将写入事务按照时间序发送，因此一旦某 replica 发现了已经存在 tid 大于 T 的写事务，则表明所有 tid 小于等于读事务 T 的已提交写入事务均已同步到本地。对于t <sub>safe</sub><sup>tm</sup>，需要了解每个 paxos group 都会有 replica 个数个 transaction manager，follower 的 transaction manager 可以根据 leader 发送过来的日志保持与 leader 的同步。如果当前 replica 的 transaction manager 不存在已 prepare 但还未 commit/abort 的事务，则 t<sub>safe</sub><sup>tm</sup> 为正无穷；否则为最小的已 prepare 但还未 commit/abort 事务的 tid – 1。对于 tid 大于 t<sub>safe</sub><sup>tm</sup> 的读事务，直接去读也是不安全的。因为这部分还未提交的事务可能会提交，直接读的话便会漏掉这些数据。</p><p>因此对于一个读事务 T，一旦某 replica 发现了本地维护的 t<sub>safe</sub> &gt;= 读事务 T 的 tid，则可以直接执行读事务，这不会违背外部一致性。否则该 replica 需要等待直到本地维护的 t<sub>safe</sub> 大于 读事务 T 的 tid 为止。</p><p>值得注意的是，这样的实现在部分场景下可能也有 liveness 的问题，比如该分片短期内没有新的写入事务且当前所有的事务都已 commit，即使所有 replica 都已经 catch up，t<sub>safe</sub> 仍然始终是最后一个写入事务的 tid。如果此时来了一个新的读事务，其似乎会被永远 block 住。因为该读事务的 tid 永远大于保持不动的 t<sub>safe</sub>。对于这种情况，该 replica 可以向 leader 发送一个 rpc，待 leader 等到 tt.now().earliest 大于该读事务的 tid 时返回 ack，此时该 replica 可以执行该读事务，因为未来产生的写事务 tid 都一定大于该读事务的 tid ，虽然其还未产生，但其一定不会对该读事务产生影响。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录一下 6.824 课程对 Google Spanner 所学。</p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式事务简介</title>
    <link href="/distributed-transactions/"/>
    <url>/distributed-transactions/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>事务是作为单个逻辑工作单元执行的一系列操作。一个逻辑工作单元必须有四个属性，称为原子性、一致性、隔离性和持久性 (ACID) 属性。分布式事务则是尝试在多节点的环境下实现这些语义。</p><p>分布式事务涉及的知识内容较多，本篇博客并没有将其彻底整理清楚，只是简单记录了一下 6.824 课程所学。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>对于分布式事务，可以将其细化为并发控制和原子提交两个子问题。前者是在说如何保证并发事务的串行隔离性，后者是在说当数据分片分布在不同的节点上时，如何保证事务在不同节点上提交与否的原子性。</p><h3 id="并发控制"><a href="#并发控制" class="headerlink" title="并发控制"></a>并发控制</h3><p>对于事务的并发控制模型，一般有两个方向：悲观事务和乐观事务。前者适合于数据竞争严重且重试代价大的场景，后者适用于数据竞争不严重且重试代价不大的场景。</p><h4 id="悲观事务"><a href="#悲观事务" class="headerlink" title="悲观事务"></a>悲观事务</h4><p>在事务执行过程中对事务所用到的数据都上锁。这里的锁从事务中第一次用到对应数据时开始，直到事务结束时才释放。</p><p>2PL（2-Phase-Lock）就是一种典型的悲观事务方法，但并不是到事务结束时才释放所有锁，而是事务过程中一旦不再使用某对象即可释放该对象的锁。因此，6.824 课程中介绍的 2PL 严格来说是 S2PL（Strict-2-Phase-Lock），即所有锁都是在事务结束的同时才释放的。S2PL 相比 2PL 的性能更差，但能够避免级联终止的发生，具体可以参考此<a href="https://niceaz.com/2019/03/24/isolation-2pl-mvcc/" target="_blank" rel="noopener">博客</a>。</p><p>不论是 2PL 还是 S2PL，都有可能导致死锁，因此一般要从两个方面入手来避免死锁：死锁检测、死锁预防</p><ul><li>死锁检测：为相互等待的事务之间维护一张 graph， 检测 graph 中是否有环。如果检测到优化，则根据一定的策略选择终止一个事务，打破循环等待。</li><li>死锁预防：按照一定顺序进行加锁，锁超时则终止等。</li></ul><p>S2PL 严格意义上不能解决幻读的问题，因为其加的都是行锁，所以其并不能实现串行隔离性，这一点与 6.824 课程中的介绍似乎有些许出入，等到之后有时间再认真研究一番。</p><h4 id="乐观事务"><a href="#乐观事务" class="headerlink" title="乐观事务"></a>乐观事务</h4><p>在事务执行过程中获取事务所用到的数据时并不上锁，直到计算完毕提交时才加锁对数据进行验证，若无变化则提交，否则 abort 或重新获取最新数据并计算。</p><p>由于分布式乐观事务在提交之前获取数据进行计算时并不需要加锁，因此一般也可以通过结合 RDMA 等技术来显著提升性能。</p><h3 id="原子提交"><a href="#原子提交" class="headerlink" title="原子提交"></a>原子提交</h3><p>在分布式事务中，参与事务的所有节点必须全部执行 Commit 操作或全部执行 Abort 操作，即他们需要在”执行 Commit 还是 Abort”这一点上达成一致（其实就是共识）。理论上有许多原子提交协议：2PC 和 3PC 等等。</p><p>原子提交协议和共识协议：</p><ul><li>目的相同：<ul><li>共识问题：解决的是如何在分布式系统中的多个节点之间就某个提议达成共识。</li><li>原子提交问题：解决的是参与分布式事务的所有节点在”执行 Commit 还是 Abort”这一点上达成共识。</li></ul></li><li>范围不同：前者要求所有节点达成共识，后者要求大多数未故障的节点达成共识。</li></ul><p>有关 2PC, 3PC, TCC, SAGA 等原子提交协议的具体内容，可以参考此<a href="https://mp.weixin.qq.com/s/MbPRpBudXtdfl8o4hlqNlQ" target="_blank" rel="noopener">博客</a>。</p><p>对于业务上的分布式事务，还有 Seata 和基于 MQ 的分布式事务实现等等，花样很多，但实现各异。他们不是本文的重点，感兴趣自行谷歌即可。</p><p>对于数据库内核中的分布式事务实现，一般都是通过 2PC 的方式。为什么不用 3PC 呢？2PC 严格来说是保证了 safety 但牺牲了较多的 liveness（部分场景下资源会被永远锁定），而且正常工作时需要发两轮 RPC 来提交一个事务，其性能是被很多人诟病的一点；3PC 虽然提高了 liveness（资源锁定一定会在有限时间内被解除），但其是以牺牲 safety 为代价的，同时正常工作时需要发三轮 RPC 来提交一个事务，性能更差，因此很多人认为使用 3PC 是实现分布式事务的一条歪路，工业界也几乎很少有人使用 3PC，这一点 PingCAP 的 CTO 在分布式之美论坛上也提到过。</p><p>事实上，不论是 Spanner 还是 TiDB，他们的方式都是通过将协调者和参与者都分别组成共识组的方式来避免单点故障，从而在保证 safety 的基础上提升liveness（由于协调者节点的单点故障被共识组规避掉，参与者的资源锁定一定会在有限时间内被解除），当然，由于每一个操作和决定都需要在共识组中进行同步，不可避免的其性能也会更差，但相比 3PC，其至少没有牺牲 safety，这一点非常关键，因为只要保证了 safety，至少我们可以通过 scale out 的方式来提升性能。</p><p>对于 2PC，业界和学术界针对不同的数据模型也有一些特定的优化，比如针对 KV 模型的 Percolator 算法等，它可以将 2PC 在部分场景下优化为 1PC 并结合 Async Commit 的方式来提升性能，这里之后有时间再进行研究吧。</p><p>业界也有一些替换 2PC 的声音，可以参考此<a href="https://www.jdon.com/51588" target="_blank" rel="noopener">博客</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客简单记录了 6.824 课程中分布式事务的内容，同时进行了一点点分析和拓宽，以做记录。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-2pc.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/12.html" target="_blank" rel="noopener">6.824 视频</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Frangipani 论文阅读</title>
    <link href="/frangipani-thesis/"/>
    <url>/frangipani-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Frangipani 是一篇很古老的分布式存储论文，其设计思想在今天看来有很多已经过时了，但也有一定的参考意义。</p><p>该论文主要介绍了三个方面的工作：</p><ul><li>cache coherence</li><li>distributed transactions</li><li>distributed crash recovery</li></ul><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>具体内容可以参考此<a href="https://www.cnblogs.com/jamgun/p/14668522.html" target="_blank" rel="noopener">博客</a>和 6.824 课程的<a href="http://nil.csail.mit.edu/6.824/2020/notes/l-frangipani.txt" target="_blank" rel="noopener">讲义</a>，后者较为详细。</p><p>有关后两个工作可以直接参考以上博客的介绍，有关 cache coherence 可以进一步参考 CPU 缓存的<a href="https://mp.weixin.qq.com/s/HvgaXjHgD4_nVz81ipmbhg" target="_blank" rel="noopener">实现方式</a>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录一下 Frangipani 论文的主要思想并记录一些相关博客。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-frangipani.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/11.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/thekkath-frangipani.pdf" target="_blank" rel="noopener">论文</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chain Replication 论文阅读</title>
    <link href="/chain-replication-thesis/"/>
    <url>/chain-replication-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>对于 raft、paxos 这类共识算法，leader 节点需要将客户端的写请求编号并发送给所有 follower 以期望达成共识，这一定程度上导致写性能无法随节点个数线性增长，因为 leader 同步的数据量会随着节点数的增长而增长，从而使得主节点承载着更大的压力，往往成为了瓶颈。</p><p>2004 年，Chain Replication （之后简称 cr）方案被提出，其也能够保证多副本间的线性一致性。具体思路是每个节点只负责向后续节点进行备份，从而将压力分摊到整个链上。因为其与上述的共识协议相比，其每个节点的写入负载几乎一致，从而不存在单节点负载很高影响性能的问题。</p><p>以上都是论文中的吹的说法，我个人持怀疑态度：首先 raft 的 leader 向所有 follower 发送是并行的，而 cr 是串行的，因此就性能上，个人不觉得后者会更快；其次随着节点数增多，虽然 raft 的 leader 负载会更大可能增大延迟，但是 cr 一定会增加延迟（多一轮 RTT），因此就写扩展性上，我也没看到 cr 有什么明显的优势。至于说什么拆分读写负载，只能说 raft 早就提出 follower read 的解决方案了，cr 做的也不过是把读放到了另一个节点上，并无扩展性，craq 才相对做到了一定程度的读性能的可扩展性。 </p><p>但不论如何，很多顶级产品比如 Ceph，Parameter Server 等都用到了 cr，所以了解一下 cr 还是有必要的，其能够拓宽我们对共识协议的了解边界。毕竟其实现线性一致性的方式还是比较巧妙的，而且设计也比较简单，没 raft 那么多 corner case。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>有关 CR，可以参考此<a href="https://zhuanlan.zhihu.com/p/344522347" target="_blank" rel="noopener">博客</a>。<br>有关 CRAQ，可以参考此<a href="https://www.cnblogs.com/brianleelxt/p/13275647.html" target="_blank" rel="noopener">博客</a>和此<a href="https://zhuanlan.zhihu.com/p/344808961" target="_blank" rel="noopener">博客</a>。</p><p>CR 能够以很简单的设计实现多副本的线性一致性，不过其不能自己处理脑裂和分区的问题，因而还需要另一个高可用的配置服务器集群来协作提供高可用服务。</p><p>CRAQ 相比 CR 做了读性能优化，使得读性能可以线性扩展且保证线性一致性。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>简单记录了 cr 和 craq 的工作原理。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-craq.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/9.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="https://www.cs.cornell.edu/home/rvr/papers/OSDI04.pdf" target="_blank" rel="noopener">CR 论文</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/craq.pdf" target="_blank" rel="noopener">CRAQ 论文</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>论文阅读</tag>
      
      <tag>共识算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zookeeper 论文阅读</title>
    <link href="/zookeeper-thesis/"/>
    <url>/zookeeper-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Zookeeper 作为一个划时代的分布式协调服务，是 Hadoop 技术栈的重要组件。</p><p>本篇博客将讨论一些 zk 论文的知识点。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>最近比较忙，没多少时间总结了，论文内容概要可以参考这篇<a href="https://keys961.github.io/2019/04/01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZooKeeper/" target="_blank" rel="noopener">博客</a>，更详细的细节可以参考这篇<a href="https://mp.weixin.qq.com/s/DwyPt5YZgqE0O0HYEC1ZMQ" target="_blank" rel="noopener">博客</a>。</p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><h3 id="zk-or-etcd"><a href="#zk-or-etcd" class="headerlink" title="zk or etcd?"></a>zk or etcd?</h3><p>同为分布式协调服务和元数据存储服务的产品：老大哥 zk 伴随 NoSQL 运动崛起，坐拥 hadoop 技术栈孤独求败；后起之秀 etcd 拥抱云原生，相伴 k8s 来势汹汹。所以一个很直接的问题就出现了：zk or etcd？</p><p>网上有很多对两者优缺点的分析，例如该<a href="https://juejin.cn/post/6844904147779600391" target="_blank" rel="noopener">博客</a>。</p><p>个人对这两个系统的具体了解没有很深入，仅仅从 zab 和 raft 算法出发谈谈自己的想法。</p><p>前面的<a href="https://tanxinyu.work/consistency-and-consensus/#%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">博客</a>已经介绍过了，zab 保证的是顺序一致性语义，raft 保证的则是线性一致性语义。尽管他们都可以算强一致性，但顺序一致性并无时间维度的约束，所以可能并不满足现实世界的时序。也就是说，在现实世界中，顺序一致性是可能返回旧数据的。对于一个分布式协调服务，可能返回旧数据实际上是比较坑爹的一件事，尽管 zk 保证了单客户端 FIFO 的顺序，但有些场景还是有一些受限的。因此在这一点上，我认为 etcd 保证的线性一致性是更好的，zk 的顺序一致性有时候会有坑，这一点 PingCAP 的 CTO 也在知乎的”分布式之美”圆桌会谈上吐槽过。</p><p>当然，zk 既然有这么多的用户在用，就算有坑那一定也不是大坑，大部分情况下应该都是没有问题的。至于选谁更好，答案一定不是唯一的，根据自己的业务和理解有自己的判断就好。</p><h3 id="zk-是否能够保证线性一致性？"><a href="#zk-是否能够保证线性一致性？" class="headerlink" title="zk 是否能够保证线性一致性？"></a>zk 是否能够保证线性一致性？</h3><p>很多人可能会觉得既然 zk 支持线性一致性写，那么也可以通过 sync + read 来支持线性一致性读，理论上这样是可以支持线性一致性读的，但在 zk 真正的实现中是不能严格满足线性一致性的，具体可以参照 jepsen 中的<a href="https://github.com/jepsen-io/jepsen/issues/399" target="_blank" rel="noopener">讨论</a>。不能严格满足线性一致性的根据原因就是 zk 在实现过程中并没有将 sync 当做一个空写日志去执行，而是直接让 leader 返回一个 zxid 给 follower，然而此时的 leader 并没有像 raft 那样通过 read index 发起一轮心跳或 lease read 的方式来确保自己一定是 leader，从而可能在网络分区脑裂的 corner case 下返回旧数据，因此无法在严格意义上满足线性一致性。当然，这种 corner case 在实际中很少见，而且也应该可以修复，所以从技术上来讲，zk 应该是可以用 sync + read 来支持线性一致性读的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客对 zk 论文的内容进行了简单的记录，最后进行了一些讨论。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-zookeeper.txt" target="_blank" rel="noopener">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/8.html" target="_blank" rel="noopener">6.824 视频</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/papers/zookeeper.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="https://iswade.github.io/translate/zookeeper/" target="_blank" rel="noopener">论文翻译</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>论文阅读</tag>
      
      <tag>共识算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用 IDEA 对分布式 IoTDB 进行调试</title>
    <link href="/cluster-iotdb-idea-debugger/"/>
    <url>/cluster-iotdb-idea-debugger/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在单机数据库中，寻找 bug 相对较为简单。因为一旦可以复现 bug，那我们可以利用 IDE 在服务端打断点一步步执行并跟踪查看堆栈信息来判断代码出错的位置从而最终找到问题。</p><p>在分布式数据库中，找 bug 就变得相对困难了。一方面是因为分布式数据库较难利用 IDE 打断点，其往往通过打 log 的方式来记录错误情况，另一方面是一条客户端请求过来后往往伴随着若干并行和跨节点的 rpc，因此如果通过查 log 的方式来寻找 bug，往往很耗费时间和精力。</p><p>在生产环境中，线上 debug 显然是一个不可取（<del>被运维打死</del>）的行为，因此业务只能通过完善日志调用链路的方式来追踪一个请求在分布式系统中的行为，这种方式一方面需要各个项目能够支持对同一 requestId 请求的追踪，另一方面也需要各个应用能够打出合理数量的日志，不能太影响性能但也不能遇到问题无法定位，此外还需要支持海量日志收集（类似于 Flume + Kafka ）和全文检索（ES 或 Spark/MR）的应用，总之这是一套相对较重的框架。就分布式追踪而言，目前较火的项目有 Skywalking，Zipkin 等等，可以参考这篇<a href="https://icyfenix.cn/distribution/observability/" target="_blank" rel="noopener">博客</a>的介绍。</p><p>在测试环境中，尽管我们也可以采用生产环境的 debug 方式，但显然我们希望能够找到效率更高的方式。万幸的是，Jetbrains 全家桶为我们提供分布式系统的 debug 方式。比如对于 Java 应用而言，IDEA 就提供了远程 debug 和本地多进程 debug 的方式。对于远程 debug，可以参考这篇<a href="https://www.cnblogs.com/aligege/p/7308180.html" target="_blank" rel="noopener">博客</a>介绍的方式对远程的应用进行 debug，也可以参照这篇<a href="https://mp.weixin.qq.com/s/frwNMfr3wAkHaR9ZUXFcew" target="_blank" rel="noopener">博客</a>介绍的方式对分布式 IoTDB 远程 debug；对于本地多进程 debug，本篇博客将介绍如何利用 IDEA 对分布式 IoTDB 进行调试。</p><p>有关 Apache IoTDB 可以参考<a href="https://iotdb.apache.org/" target="_blank" rel="noopener">官方网站</a>。</p><h2 id="本地多进程-debug-调试"><a href="#本地多进程-debug-调试" class="headerlink" title="本地多进程 debug 调试"></a>本地多进程 debug 调试</h2><p>对于 IoTDB 集群的搭建示例，可以参照<a href="https://github.com/apache/iotdb/blob/master/docs/zh/UserGuide/Cluster/Cluster-Setup.md" target="_blank" rel="noopener">官方文档</a>，其分布式模块的启动主类为 <a href="https://github.com/apache/iotdb/blob/master/cluster/src/main/java/org/apache/iotdb/cluster/ClusterMain.java" target="_blank" rel="noopener">ClusterMain.java</a>，其默认的三节点分布式配置参数文件夹可以参考<a href="https://github.com/apache/iotdb/tree/master/cluster/src/test/resources" target="_blank" rel="noopener">这里</a>。</p><pre><code class="hljs JAVA"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;  <span class="hljs-keyword">if</span> (args.length &lt; <span class="hljs-number">1</span>) &#123;    logger.error(        <span class="hljs-string">"Usage: &lt;-s|-a|-r&gt; "</span>            + <span class="hljs-string">"[-D&#123;&#125; &lt;configure folder&gt;] \n"</span>            + <span class="hljs-string">"-s: start the node as a seed\n"</span>            + <span class="hljs-string">"-a: start the node as a new node\n"</span>            + <span class="hljs-string">"-r: remove the node out of the cluster\n"</span>,        IoTDBConstant.IOTDB_CONF);    <span class="hljs-keyword">return</span>;  &#125;  ...&#125;</code></pre><p>可以看到其按照 seednodes 启动时指定 -s 即可，此外其也支持通过 <code>-D{}</code> 的方式来覆盖参数，因此我们可以利用 IDEA 分别指定不同的配置文件夹并用 debug 模式来启动三个 clusterMain 进程，这样即可达到本地多进程 debug 调试的理想效果。</p><h2 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h2><ol><li>点击 <code>Edit Configurations...</code><br><img src="/cluster-iotdb-idea-debugger/step1.png" srcset="/img/loading.gif" alt></li><li>点击 <code>Add new Configuration</code><br><img src="/cluster-iotdb-idea-debugger/step2.png" srcset="/img/loading.gif" alt></li><li>点击 <code>Application</code><br><img src="/cluster-iotdb-idea-debugger/step3.png" srcset="/img/loading.gif" alt></li><li>编辑好红框的五个部分，指定配置名称，jdk 版本，启动模块，启动主类和启动参数 <code>-s</code>，然后点击绿框。<br><img src="/cluster-iotdb-idea-debugger/step4.png" srcset="/img/loading.gif" alt></li><li>点击 <code>Add VM options</code><br><img src="/cluster-iotdb-idea-debugger/step5.png" srcset="/img/loading.gif" alt></li><li>添加 <code>-D{}</code> 环境变量，例如我填的就是<code>-DIOTDB_CONF=/Users/txy/Study/incubator-iotdb/cluster/src/test/resources/node1conf</code><br><img src="/cluster-iotdb-idea-debugger/step6.png" srcset="/img/loading.gif" alt></li><li>复制两份 node1 的配置<br><img src="/cluster-iotdb-idea-debugger/step7.png" srcset="/img/loading.gif" alt></li><li>编辑好 node2 的名称和配置文件<br><img src="/cluster-iotdb-idea-debugger/step8.png" srcset="/img/loading.gif" alt></li><li>编辑好 node3 的名称和配置文件<br><img src="/cluster-iotdb-idea-debugger/step9.png" srcset="/img/loading.gif" alt></li><li>以 debug 模式分别启动 node1，node2 和 node3<br><img src="/cluster-iotdb-idea-debugger/step10.png" srcset="/img/loading.gif" alt></li><li>可以执行对应的请求来触发 bug，从而开始 debug 调试（此时相当于 idea 用 debug 模式启动了三个进程，构成了一个可以 debug 的三节点伪分布式 IoTDB）<br><img src="/cluster-iotdb-idea-debugger/step11.png" srcset="/img/loading.gif" alt></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本片博客简单介绍了如何使用 IDEA 调试分布式 IoTDB，希望能对大家 debug 分布式系统有所帮助。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://mp.weixin.qq.com/s/ra0aMHmmzst03v0rBdOVfQ" target="_blank" rel="noopener">手摸手教你阅读和调试大型开源项目 ZooKeeper</a></li><li><a href="https://mp.weixin.qq.com/s/frwNMfr3wAkHaR9ZUXFcew" target="_blank" rel="noopener">No.7 - 时序数据库随笔 - Apache IoTDB(单机&amp;集群)调试环境搭建</a></li><li><a href="https://iotdb.apache.org/" target="_blank" rel="noopener">IoTDB 官网</a></li><li><a href="https://github.com/apache/iotdb/tree/master" target="_blank" rel="noopener">IoTDB 代码库</a> </li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>开发工具配置</tag>
      
      <tag>IoTDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一致性模型与共识算法</title>
    <link href="/consistency-and-consensus/"/>
    <url>/consistency-and-consensus/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>有关一致性模型和共识算法的一致性模型这两个问题，最近阅读了一些优质博客，学到了一些新的东西，同时感觉一些定义比较混乱，特此记录一下自己的理解。</p><h2 id="一致性模型"><a href="#一致性模型" class="headerlink" title="一致性模型"></a>一致性模型</h2><p>一致性问题是分布式领域最为基础也是最重要的问题，具体来历可以参考此<a href="https://mp.weixin.qq.com/s/3odLhBtebF4cm58hl-87JA" target="_blank" rel="noopener">博客</a>。</p><p>一般来讲，分布式系统中的一致性按照对一致性要求的不同，主要分为强一致性，弱一致性这两大类，前者是基于 safety 的概念，后者是基于 liveness 的概念。</p><h3 id="强一致性"><a href="#强一致性" class="headerlink" title="强一致性"></a>强一致性</h3><p>强一致性包含线性一致性和顺序一致性，其中前者对 safety 的约束更强，也是分布式系统中能保证的最好的一致性。</p><h4 id="顺序一致性"><a href="#顺序一致性" class="headerlink" title="顺序一致性"></a>顺序一致性</h4><p>如果一个并发执行过程所包含的所有读写操作能够重排成一个全局线性有序的序列，并且这个序列满足以下两个条件，那么这个并发执行过程就是满足顺序一致性的：</p><ul><li>条件 I：重排后的序列中每一个读操作返回的值，必须等于前面对同一个数据对象的最近一次写操作所写入的值。</li><li>条件 II：原来每个进程中各个操作的执行先后顺序，在这个重排后的序列中必须保持一致。</li></ul><h4 id="线性一致性"><a href="#线性一致性" class="headerlink" title="线性一致性"></a>线性一致性</h4><p>线性一致性的定义，与顺序一致性非常相似，也是试图把所有读写操作重排成一个全局线性有序的序列，但除了满足前面的条件 I 和条件 II 之外，还要同时满足一个条件：</p><ul><li>条件 III：不同进程的操作，如果在时间上不重叠，那么它们的执行先后顺序，在这个重排后的序列中必须保持一致。</li></ul><h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><ul><li>它们都试图让系统“表现得像只有一个副本”一样。</li><li>它们都保证了程序执行顺序不会被打乱。体现在条件 II 对于进程内各个操作的排序保持上。</li><li>线性一致性考虑了时间先后顺序，而顺序一致性没有。</li><li>满足线性一致性的执行过程，肯定都满足顺序一致性；反之不一定。</li><li>线性一致性隐含了时效性保证（recency guarantee）。它保证我们总是能读到数据最新的值。</li><li>在顺序一致性中，我们有可能读到旧版本的数据。</li></ul><h3 id="弱一致性"><a href="#弱一致性" class="headerlink" title="弱一致性"></a>弱一致性</h3><p>弱一致性是指系统在数据成功写入之后，不承诺立即可以读到最新写入的值，也不会具体承诺多久读到，但是会尽可能保证在某个时间级别之后，可以让数据达到一致性状态。</p><p>可以根据能够恢复一致的时间将弱一致性进一步分类，如果是有限时间那就是最终一致性，如果是无限时间那实际上相当于没有一致性。对于前者，可以进一步分类，如下图所示：</p><p><img src="/consistency-and-consensus/consistency_level.png" srcset="/img/loading.gif" alt></p><p>其具体的定义已经在此篇<a href="https://tanxinyu.work/base-theory/#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E7%A7%8D%E7%B1%BB" target="_blank" rel="noopener">博客</a>中有过介绍，此处不再赘述。</p><h2 id="一致性与共识的区别"><a href="#一致性与共识的区别" class="headerlink" title="一致性与共识的区别"></a>一致性与共识的区别</h2><p>一致性往往指分布式系统中多个副本对外呈现的数据的状态。如前面提到的顺序一致性、线性一致性，描述了多个节点对数据状态的维护能力。</p><p>共识性则描述了分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程。</p><p>因此，一致性描述的是结果状态，共识则是一种手段。达成某种共识并不意味着就保障了一致性（这里的一致性指强一致性）。只能说共识机制，能够实现某种程度上的一致性。</p><p>实践中，要保障系统满足不同程度的一致性，核心过程往往需要通过共识算法来达成。</p><h2 id="共识算法"><a href="#共识算法" class="headerlink" title="共识算法"></a>共识算法</h2><p>常见的共识算法有 Paxos，Zab 和 Raft，此处暂只介绍后两种共识算法的一致性模型。</p><h3 id="ZooKeeper-的-Zab"><a href="#ZooKeeper-的-Zab" class="headerlink" title="ZooKeeper 的 Zab"></a>ZooKeeper 的 Zab</h3><p>一种说法是 ZooKeeper 是最终一致性，因为由于多副本、以及保证大多数成功的 Zab 协议，当一个客户端进程写入一个新值，另外一个客户端进程不能保证马上就能读到这个值，但是能保证最终能读取到这个值。</p><p>另外一种说法是 ZooKeeper 的 Zab 协议类似于 Paxos 协议，提供了线性一致性。</p><p>但这两种说法都不准确，ZooKeeper 文档中明确写明它的一致性是 Sequential consistency 即顺序一致性。</p><p>ZooKeeper 中针对同一个 follower A 提交的写请求 request1、request2，某些 follower 虽然可能不能在请求提交成功后立即看到（也就是强一致性），但经过自身与 leader 之间的同步后，这些 follower 在看到这两个请求时，一定是先看到 request1，然后再看到 request2，两个请求之间不会乱序，即顺序一致性。</p><p>其实，实现上 ZooKeeper 的一致性更复杂一些，ZooKeeper 的读操作是 sequential consistency 的，ZooKeeper 的写操作是 linearizability 的，关于这个说法，ZooKeeper 的官方文档中没有写出来，但是在社区的邮件组有详细的讨论。ZooKeeper 的论文<a href="https://www.usenix.org/system/files/conference/atc16/atc16_paper-lev-ari.pdf" target="_blank" rel="noopener">《Modular Composition of Coordination Services》</a> 中也有提到这个观点，官网上也有<a href="https://zookeeper.apache.org/doc/r3.4.9/zookeeperProgrammers.html#ch_zkGuarantees" target="_blank" rel="noopener">声明</a>。</p><p>总结一下，可以这么理解 ZooKeeper：从整体（read 操作 + write 操作）上来说是 sequential consistency，写操作实现了 Linearizability。</p><h3 id="etcd-的-Raft"><a href="#etcd-的-Raft" class="headerlink" title="etcd 的 Raft"></a>etcd 的 Raft</h3><p>对于 raft 算法，其写操作一定得从 leader 向 follower 同步，这是 raft 算法的基石，也是很难变动的。由于 leader 始终是瓶颈，所以即使我们增加节点，raft 算法的写性能也不能够线性扩展，反而会越来越差。对于读操作，raft 的默认方式是从 leader 读，这样就能够满足线性一致性，然而这样的实现方式也会导致读性能也不能随节点个数的增长而线性提升。zab 提出了一种保证顺序一致性的 follower read 优化，针对读请求能够利用所有节点的计算和 IO 资源，从而使得读性能能够线性扩展。raft 更进一步，提出了一种保证线性一致性的 follower read 优化，具体可以参考此<a href="https://zhuanlan.zhihu.com/p/78164196" target="_blank" rel="noopener">博客</a>。</p><p>对于 raft 算法，其写请求自然是满足线性一致性的，对于读请求，需要做一些额外的工作来保证线性一致性，这一点论文中也有说明实现线性一致性读的两种方式：</p><ul><li>Read Index</li><li>Lease Read</li></ul><p>对于 Read Index 方法，leader 大致的流程如下：</p><ol><li>记录当前的 commit index，称为 Read Index</li><li>向 follower 发起一次心跳，如果大多数节点回复了，那就能确定现在仍然是 leader</li><li>等待状态机至少应用到 Read Index 记录的 Log</li><li>执行读请求，将结果返回给 Client</li></ol><p>follower 大致的流程如下：</p><ol><li>向 leader 请求其 commitIndex 来作为本次查询请求的 Read Index</li><li>该 leader 向 follower 发起一次心跳，如果大多数节点回复了，那就能确定现在仍然是 leader</li><li>该 leader 返回本地的 commitIndex 给 follower</li><li>等待状态机至少应用到 Read Index 记录的 Log</li><li>执行读请求，将结果返回给 Client</li></ol><p>实际上 Read Index 流程第二步的主要作用是为了让 leader 确保自己仍是 leader。设想这样的情况：发生了网络分区，leader 被分区到了少数派中，多数派已经产生了新的 leader 并进行了新的数据写入，此时尽管老 leader 发送心跳无法得到大多数节点的回复，但其也无法主动退位。如果对于查询请求 leader 不进行第二步的检查直接读状态机的话，就可能读到旧的数据，从而使得 raft 不满足线性一致性了。 </p><p>Lease Read 与 Read Index 类似，但更进一步省去了网络交互。基本的思路是 leader 取一个比 Election Timeout 小的租期，在租期不会发生选举，确保 leader 在这个周期不会变，所以就算老 leader 被分区到了少数派，直接读状态机也一定不会读到旧数据，因为租期内新 leader 一定还没有产生，也就不会有更新的数据了。因此 Lease Read 可以跳过 Read Index 流程的第二步，从而降低读延时提升读吞吐量。不过 Lease Read 的正确性和时间挂钩，因此时间的准确性至关重要，如果时钟漂移严重，这套机制就会有问题。</p><p>以上算是 raft 实现线性一致性读的直观解释了。 </p><p>对于以上实现方式，我们还可以分析探讨一下两个问题：</p><ul><li>如果读写请求都走 leader，要想保证线性一致性还需要上述 Read Index 流程的 1，3 步骤吗？</li><li>如果开启了 follower read，要想保证线性一致性 leader 还可以 wait-free 吗？</li></ul><p>对于第一个问题，我的想法是不需要。因为这个时候读写请求都在 leader 上进行，那么整个系统表现的相当于只有一个副本。分析理论的话：对于一个写入成功的写操作，其状态一定已经被 apply 到了 leader 的状态机上，所以与其有全局偏序关系的后续读请求在执行时一定能够感知到这个写操作，这满足线性一致性；如果没有全局偏序关系，则该读请求和上一个写请求就是并发请求，那么是否感知到这个写操作都是满足线性一致性的，而且一旦该读请求感知到了这个写操作，后续与其有全局偏序的读请求就都能感知到这个写操作，这也是满足线性一致性的。这也是 <a href="https://pingcap.com/blog-cn/linearizability-and-raft/" target="_blank" rel="noopener">PingCAP 线性一致性读博客</a>中 wait-free 优化的具体含义：</p><blockquote><p>到此为止 Lease 省去了 ReadIndex 的第二步，实际能再进一步，省去第 3 步。这样的 LeaseRead 在收到请求后会立刻进行读请求，不取 commit index 也不等状态机。由于 Raft 的强 Leader 特性，在租期内的 Client 收到的 Resp 由 Leader 的状态机产生，所以只要状态机满足线性一致，那么在 Lease 内，不管何时发生读都能满足线性一致性。有一点需要注意，只有在 Leader 的状态机应用了当前 term 的第一个 Log 后才能进行 LeaseRead。因为新选举产生的 Leader，它虽然有全部 committed Log，但它的状态机可能落后于之前的 Leader，状态机应用到当前 term 的 Log 就保证了新 Leader 的状态机一定新于旧 Leader，之后肯定不会出现 stale read。</p></blockquote><p>对于第二个问题，我的想法是不可以。我们设想一个 follower apply 比 leader 快的场景（比如 leader 的磁盘是 HDD，follower 的磁盘是 SSD），比如 leader 和 follower 的日志本来均为 <code>[1，2]</code>，此时一个客户端执行了一个写请求，leader 将其进行了广播并进行了 commit，然后正在很慢的异步 apply 中，此时 leader 的日志为 <code>[1,2,3]</code>，commitIndex 为 3，applyIndex 为 2，follower 的日志为 <code>[1,2,3]</code>，commitIndex 为 3，applyIndex 为 3。此时另一个并发的客户端发起了一个查询请求，该查询请求路由到了 follower，follower 用了上述 Read Index 的步骤拿到了 leader 的 commitIndex 3 并确定自己的 applyindex &gt;= 3 后对状态机进行了查询然后返回。接着该客户端又发起了一个查询请求，该查询请求路由到了 leader，此时 leader 如果采用 wait-free 的方式，则只能对 applyindex 为 2 的状态机进行查询，那么就可能返回旧的数据。虽然此时这两个读请求和另一个写请求是并发关系，是否保证这个写操作的状态都符合线性一致性，但线性一致性还规定一旦读请求感知到了某个写操作，则与这个读请求有全局偏序关系的后续读请求都应该感知到这个写操作，那么这个例子描述的场景就不符合线性一致性了。通过这个例子我们可以看到，如果开启了 follower read，要想保证线性一致性 leader 不能再采用 wait-free 直接读的方式，必须获取 Read Index 才能保证线性一致性，这一点也可以参考 PingCAP CTO 的<a href="https://zhuanlan.zhihu.com/p/78164196" target="_blank" rel="noopener">博客</a>。</p><p>做一个总结：</p><ul><li>如果不做 follower read 的优化，读性能无法随节点个数线性提升，但 leader 可以采用 wait-free 的方式对状态机直接做读操作，这样可以保证线性一致性。（注意：仍然需要通过心跳或 lease 的方式确保自己是 leader）</li><li>如果做了 follower read 的优化，读性能可以随节点个数线性提升，但 leader 不能再采用 wait-free 的方式对状态机直接做读操作，需要严格按照 Read Index 或 Lease Read 的方式才可以保证线性一致性。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客首先简单介绍了若干一致性模型，然后介绍了一致性和共识的关系，最后对于 zab 和 raft 的一致性模型进行了较为详细的分析讨论。此外，在撰写博客的过程中也发现了一些优质博客如下：</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/35596768" target="_blank" rel="noopener">分布式系统的一致性与共识性</a></li><li><a href="https://blog.csdn.net/chao2016/article/details/81149674" target="_blank" rel="noopener">强一致性、顺序一致性、弱一致性和共识</a></li><li><a href="https://zhuanlan.zhihu.com/p/48782892" target="_blank" rel="noopener">[译] 分布式系统中的一致性模型</a></li><li><a href="https://feilengcui008.github.io/post/raft%E8%AF%BB%E8%AF%B7%E6%B1%82/" target="_blank" rel="noopener">Raft 读请求性能分析</a></li><li><a href="https://segmentfault.com/a/1190000022248118" target="_blank" rel="noopener">共识、线性一致性与顺序一致性（强推！）</a></li><li><a href="https://mp.weixin.qq.com/s/qnvl_msvw0XL7hFezo2F4w" target="_blank" rel="noopener">条分缕析分布式：到底什么是一致性？（强推！）</a></li><li><a href="https://mp.weixin.qq.com/s/3odLhBtebF4cm58hl-87JA" target="_blank" rel="noopener">条分缕析分布式：浅析强弱一致性（强推！）</a></li><li><a href="https://mp.weixin.qq.com/s/wkXsRufVsbKqTwjzTgNqYQ" target="_blank" rel="noopener">条分缕析分布式：因果一致性和相对论时空（强推！）</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>共识算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FLP 不可能定理介绍</title>
    <link href="/flp-theory/"/>
    <url>/flp-theory/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>FLP 不可能定理与 CAP，BASE 定理等一样，都是分布式系统的基本理论，因此我们有必要了解该理论。</p><p>本博客摘抄了此<a href="https://mp.weixin.qq.com/s/LOUDWn7evcePCPGWar8vEA" target="_blank" rel="noopener">博客</a>中对 FLP 不可能定理的部分描述。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>1985 年，Fisher、Lynch、Paterson 三位科学家就发表了关于分布式一致性问题的不可能定理：在完全异步的分布式网络中，故障容错问题无法被解决。（ <code>We have shown that a natural and important problem of fault-tolerant cooperative computing cannot be solved in a totally asynchronous model of computation</code>）</p><p>说得更直白点：在异步网络中，不可能存在能够容忍节点故障的一致性算法，哪怕只有一个节点故障。并且这里并没有考虑拜占庭错误，而是假设网络非常稳定、所有的消息都能被正确传递、并且仅被传递一次，即便如此都不可能找到能容忍哪怕只有一个节点失效的一致性协议，可见该结论有多强。（ <code>In this paper, we show the surprising result that no completely asynchronous consensus protocol can tolerate even a single unannounced process death. We do not consider Byzantine failures, and we assume that the message system is reliableit delivers all messages correctly and exactly once</code>）</p><p>现实生活中的系统往往都是异步系统。因为系统中各个节点之间的延时，是否宕机等等都是不确定的。那么，在最小化异步模型系统中，是否存在一个可以解决一致性问题的确定性共识算法？</p><p>FLP 不可能定理的最大适用前提是异步网络模型。何为同步、异步模型呢？</p><ul><li>所谓异步模型，是说从一个节点到另一个节点的消息延迟是有限的，但可能是无界的（finite but can be unbounded）。这就意味着如果一个节点没有收到消息，它无法判断消息到底是丢失了，还是只是延迟了。也就是说，我们无法通过超时时间来判断某个节点是否故障。</li><li>所谓同步模型，是说消息传递的延迟是有限的，且是有界的。这就意味着我们可以通过经验或采样精确估算消息的最大可能延迟，从而可以通过超时时间来确定消息是否丢失、节点是否故障。</li></ul><p>所幸的是，我们所处于的真实的网络世界更接近同步模型，在很多场景上，我们都可以通过经验或采样确定最大超时时间。举个通俗点的例子：你给朋友快递了一本书，朋友过了 3 天还没收到，此时朋友很难判断到底是快递延迟了，还是快递出问题送丢了。但是如果过了一个月，朋友仍没收到书，基本就可以断定快递送丢了。而背后的推论就是基于经验或统计：通常快递都能在 1-2 周内送达。显然，异步模型其实是反映了节点间通讯的最差情况、极端情况，异步模型包含了同步模型，即能在异步模型上有效的一致性协议，在同步模型上也同样有效。而同步模型是对异步模型做了修正和约束，从而使得更接近真实世界，也使得在实践中一致性问题有可能得到有效解。</p><p>另外，即便是在异步网络模型下，FLP 也并不意味着一致性永远无法达成，只是说无法保证在有界的时间（in bounded time）内达成。在实践上，如果放宽对 bounded time 的限制，仍然是有可能找到实践中的解法的。</p><p>根据 DLS 的<a href="http://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf" target="_blank" rel="noopener">研究</a>，一致性算法按照网络模型可以分为三大类：</p><ul><li>部分同步网络模型（partially synchronous model）中的一致性协议可以容忍最多 1/3 的任意错误。这里的部分同步模型是指网络延迟是有界的，但是我们无法提前得知。这里的容错也包含了拜占庭类错误。</li><li>异步网络模型（asynchronous model）中的确定性协议无法容忍错误。这里的异步模型即是前文所说的网络延迟是无界的。该结论其实就是 FLP 不可能定理的含义，在完全异步网络中的确定性协议不能容忍哪怕只有一个节点的错误。</li><li>同步网络模型（synchronous model）可以达到惊人的 100% 容错，虽然对错误节点超过 1/2 时的节点行为有限制。这里的同步模型是指网络延迟一定是有界的，即小于某个已知的常数。</li></ul><p>从另一个角度来理解，FLP实际上考虑了分布式系统的3个属性：安全(safety)、活性（liveness)、容错：</p><ul><li>安全是说系统内各个节点达成的值是一致的、有效的。safety 其实是保证系统一致性运行的最低要求，其核心是 cannot do something bad，即不能干坏事、不能做错事。</li><li>活性是说系统内各个节点最终（在有限时间内）必须能够达成一致，即系统必须能够向前推进，不能永远处于达不成一致的状态。liveness 其实是更高要求，意味着不能只是不干坏事，也不能一直不干事，you must do something good，即必须使得整个系统能良好运转下去。</li><li>容错是说该协议在有节点故障的情况下也必须能有效。</li></ul><p>FLP 不可能定理其实意味着在异步网络中，不可能存在同时满足这三者的分布式一致性协议。因为分布式环境中，节点故障几乎是必然的，因此容错是必须要考虑的因素，所以FLP不可能定理就意味着一致性协议在能做到容错的情况下，没办法同时做到安全性与系统活性。通常在实践中，我们可以做出部分牺牲，比如牺牲一部分安全性，意味着系统总能很快达成结论，但结论的可靠性不足；或者牺牲一部分系统活性，意味着系统达成的结论非常可靠，但可能长时间、甚至永远都在争论中，无法达成结论。所幸的是，很多时候现实世界的鲁棒性很强，使一致性协议失效的倒霉事件发生的概率也很可能极低。例如分布式共识协议 Paxos 和 Raft 都是保证了容错性和 safety，然后通过随机超时时间来规避 liveness 的问题。</p><p><img src="/flp-theory/flp.png" srcset="/img/loading.gif" alt></p><p>FLP 不可能定理的推导和应用可以参考此<a href="https://www.daimajiaoliu.com/daima/479430766100400" target="_blank" rel="noopener">博客</a>。</p><h2 id="与-CAP-定理的区别"><a href="#与-CAP-定理的区别" class="headerlink" title="与 CAP 定理的区别"></a>与 CAP 定理的区别</h2><p>CAP 与 FLP 看起来有相似之处，其实二者并不尽相同，二者是从不同的维度思考问题，另外即使是很相似的概念，内涵也并不完全一样。比如：</p><ul><li>FLP 面对的是分布式一致性问题，而 CAP 面对的是分布式网络中的数据同步与复制。</li><li>FLP 是说在异步网络模型中，三者不可能同时实现；而 CAP 是说在所有场景下，三者都不可能同时实现。</li><li>FLP 中的 liveness 强调的是一致性算法的内在属性；而 CAP 中的 availability 强调的是一致性算法对外呈现的外在属性。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客摘抄了部分优质博客对 FLP 不可能定理的介绍，以做学习记录和索引。</p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Time, Clocks, and the Ordering of Events in a Distributed System 论文阅读</title>
    <link href="/time-clock-order-in-distributed-system-thesis/"/>
    <url>/time-clock-order-in-distributed-system-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这篇文章是 Leslie Lamport 于 1978 年发表的，并在 2007 年被选入 SOSP 的名人堂，被誉为第一篇真正的”分布式系统”论文，该论文曾一度成为计算机科学史上被引用最多的文章（截止 2021 年 3 月 22 日已达到 12497 次）。文章的作者 Lamport 享有分布式计算原理之父的美誉，并且因其对分布式系统研究作出的卓越贡献，于 2013 年被授予了图灵奖。</p><p>这篇论文之所以经典，是因为它揭示了分布式系统的某些深层本质，深深地影响了人们对于分布式系统的思考方式。</p><p>当然，这篇论文除了理论意义和历史价值之外，它与业界一些重要的分布式系统实践也都有紧密的联系。比如，在大规模的分布式环境下产生单调递增的时间戳，是个很难的问题，而谷歌的全球级分布式数据库 Spanner 就解决了这个问题，甚至能够在跨越遍布全球的多个数据中心之间高效地产生单调递增的时间戳。做到这一点，靠的是一种称为 TrueTime 的机制，而这种机制的理论基础就是 Lamport 这篇论文中的物理时钟算法（两者之间有千丝万缕的联系）。再比如，这篇论文中定义的「Happened Before」关系，不仅在分布式系统设计中成为考虑不同事件之间关系的基础，而且在多线程编程模型中也是重要的概念。另外，还有让很多人忽视的一点是，利用分布式状态机来实现数据复制的通用方法（Replication State Machine，简称 RSM），其实也是这篇论文首创的。</p><p>总之，如果在整个分布式的技术领域中，你只有精力阅读一篇论文，那一定要选这一篇了。只有理解了这篇论文中揭示的这些涉及时间、时钟和排序的概念，我们才能真正在面对分布式系统的设计问题时游刃有余。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>本来想写点什么，但是又感觉写的都是垃圾。因此只能强推这篇<a href="https://mp.weixin.qq.com/s/FZnJLPeTh-bV0amLO5CnoQ" target="_blank" rel="noopener">博客</a>，将这篇论文讲的十分透彻，我跪了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇论文首先介绍了应该如何认识时间，时钟和排序，接着引入了偏序关系，全序关系，逻辑时钟，物理时钟等概念，最后给出了现实世界中一种可行的为所有事件排序的方案。</p><p>对于为分布式系统中的所有事件进行排序这个问题，Lamport 首先指出简单的物理时钟是不可行的，因为一方面物理时钟很难绝对准确，另一方面不同进程的物理时钟也一定会有误差，因此需要先从逻辑时钟入手。</p><p>为此，Lamport 利用逻辑时钟定义了偏序关系：偏序关系能够为进程内部的事件和进程之间的因果事件排序，对于进程之间的独立事件无法排序。其实从系统内部的视角来看，对进程之间的独立事件进行排序本无意义。</p><p>因此，Lamport 在偏序关系的基础上又定义了全序关系：即通过一些人为规定的方式，可以把进程之间的独立事件进行排序，这样再结合偏序关系即可达到排序所有事件的理想效果。</p><p>然而，这样人为规定的全序方案在系统外部来看可能违背因果一致性，产生这个现象的根本原因是逻辑时钟没有和真实的物理时钟绑定。即对于两个进程间的独立事件，系统无法感知到这两个事件实际上在物理时间上有先后关系，所以可能给时间上发生更晚的事件指定更小的逻辑时间。因此我们得出结论：要想为分布式系统的所有事件排序，仅仅使用逻辑时钟是不行的，还是需要使用物理时钟才行。</p><p>因此，Lamport 又结合相对论的知识介绍了我们所在的现实世界就是一种时空偏序关系，并进一步证明：如果物理时钟的误差能够限定到一个范围内，则前面提到的异常情况就不会出现，即我们便可以为分布式系统中的所有事件排序。简单来讲：根据相对论，任何信息传递的速度，最快就是光速。而一个事件要想对另一个事件产生影响，至少要在那个事件发生之前传递一定的信息到达所在的空间位置。对于两个进程间的独立事件，如果他们在物理世界中存在时空偏序关系，那信息的传递也需要一定的时间。只要系统能够保证：系统内部的时间误差不会超过两个外部事件产生因果关系的时间，则就能够正确的为所有事件排序。这也是 Google TrueTime 正确性的理论支撑。</p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>Lamport 大佬牛逼。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://mp.weixin.qq.com/s/FZnJLPeTh-bV0amLO5CnoQ" target="_blank" rel="noopener">分布式领域最重要的一篇论文，到底讲了什么？</a><br><a href="https://www.cnblogs.com/hzmark/p/Time_Clocks_Ordering.html" target="_blank" rel="noopener">论文翻译</a><br><a href="https://lamport.azurewebsites.net/pubs/time-clocks.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VM-FT 论文阅读</title>
    <link href="/vm-ft-thesis/"/>
    <url>/vm-ft-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>本论文主要介绍了一个用于提供容错虚拟机 (fault-tolerant virtual machine) 的企业级商业系统，该系统包含了两台位于不同物理机的虚拟机，其中一台为 primary，另一台为 backup，backup 备份了 primary 的所有执行。当 primary 出现故障时，backup 可以上线接管 primary 的工作，以此来提供容错。</p><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><p>论文内容可以参考此<a href="https://www.cnblogs.com/brianleelxt/p/13245754.htmls" target="_blank" rel="noopener">博客</a>和此<a href="https://developer.51cto.com/art/202103/649693.htm" target="_blank" rel="noopener">博客</a>。</p><h2 id="分布式容错方案"><a href="#分布式容错方案" class="headerlink" title="分布式容错方案"></a>分布式容错方案</h2><p>分布式系统中需要对一份数据进行冗余存储才能提供容错性，因此问题是：</p><blockquote><p>如果有一份会随时变动的数据，如何确保它正确地存储于网络中的几台不同机器之上？</p></blockquote><p>对于 primary/backup 型的同步方式，可行的解决方案有两种：状态转移（State Transfer）和操作转移（Operation Transfer）。</p><h3 id="状态转移"><a href="#状态转移" class="headerlink" title="状态转移"></a>状态转移</h3><p>对于状态转移，其方案是 primary 持续地将所有状态（包括 CPU、内存和 I/O 设备等或者整个状态机实例）变化发送给 backup，backup 不需要耗费太多 CPU 资源就可以达到跟 leader 相同的状态，这也导致传输的数据量往往会大很多。当然也可以有一些优化，比如只传送相比上次变化的数据等等，但总之这种方法所需带宽非常大，延迟较高，因此在工业界中应用不多。</p><h3 id="操作转移"><a href="#操作转移" class="headerlink" title="操作转移"></a>操作转移</h3><p>操作转移能够运行的前提是状态机，其特性是：</p><blockquote><p>任何初始状态一样的状态机，如果执行的命令序列一样，则最终达到的状态也一样。如果将此特性应用在多参与者进行协商共识上，可以理解为系统中存在多个具有完全相同的状态机（参与者），这些状态机能最终保持一致的关键就是起始状态完全一致和执行命令序列完全一致。</p></blockquote><p>操作转移一般有两种复制级别：</p><ul><li>机器级别：按序复制 CPU 指令，中断，客户端请求等等来保证不同节点间状态一致，这也就是本文的解决方案。这种方案需要解决若干问题，比如多节点间如何处理不明确性命令（获取当前时间），如果避免脑裂等等。看了论文之后，感觉很多关键点在论文中并没有纰漏实现细节，比如 disk server，test-and-set 等。</li><li>应用级别：按序复制明确性的操作来保证不同节点间的状态一致。比如 MySQL Cluster 的主从全同步复制需要保证所有的从节点接受成功才能返回成功，这一定程度上会导致扩展性受限，即每增加一个 Slave 节点，都导致造成整个系统可用性风险增加一分。因此也出现了基于 Paxos，Raft 的 quorum 同步方案，即一旦系统中过半数的节点中完成了状态的转换，就认为数据的变化已经被正确地存储在系统当中，这样就可以容忍少数（通常是不超过半数）的节点失联，使得增加机器数量对系统整体的可用性变成是有益的，这也是目前大多数应用的容错方案。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>作为一篇 10 年前的论文，在那时 Raft 还没有出现，Paxos 还没有得到广泛的承认，该论文对于分布式容错方案提出了一种基于机器级别的操作转移方案，拓展了理论的边界并证明了工业界可用，可以说是比较划时代的贡献了。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://icyfenix.cn/distribution/consensus/" target="_blank" rel="noopener">分布式共识算法讲解</a><br><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-vm-ft.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="http://nil.csail.mit.edu/6.824/2020/video/4.html" target="_blank" rel="noopener">6.824 视频</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/1899928.1899932" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GFS 论文阅读</title>
    <link href="/gfs-thesis/"/>
    <url>/gfs-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>单机文件系统我们见得已经很多，像 xfs，ext4 等等都已经是很出名的单机文件系统。然而，单机文件系统的容量始终是有限的，随着数据量的不断增大，就算对单机不断的 scale up 也会逐渐达到上限。因此，支持 scale out 的分布式文件系统是技术的必然。作为世界上数据量可能最多的公司，Google 在 21 世纪初就已经遇到了这个挑战，随之其开发了 GFS 这个创时代的分布式文件系统并将该成果发表在了 2003 年的 SOSP 会议上，之后随着 MapReduce 和 BigTable 论文的发表，Google 的三架马车整整齐齐，掀开了大数据时代的帷幕，引领工业界开始了辉煌的 NoSQL 运动。</p><p>之后，MapReduce 和 BigTable 或多或少都被新时代的大数据技术栈替代和压榨，只有 GFS 在大规模分布式文件系统领域鲜有敌手，只有某些针对不同场景的优化竞品而已，例如针对海量小文件做了优化的 haystack。总之，即使在 2021 年的今天，学习 GFS 的设计思想也是十分有意义的。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>实现一个高可用，可扩展，高性能，低成本，可容错的大规模分布式文件系统，同时为用户提供近可能简单的文件读写接口并屏蔽底层的复杂实现细节。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>本来打算好好的写一篇博客总结一下自己时隔两年后再读 GFS 的理解，但是偶然看到了一篇写的很详细且有作者想法的<a href="https://spongecaptain.cool/post/paper/googlefilesystem/" target="_blank" rel="noopener">博客</a>，因此就没必要浪费时间自己再写一遍了，感兴趣的可以去详读这篇博客。</p><p>对于博客中所述，有一处有关 chunk version 的地方我个人不太认同，原文如下：</p><blockquote><p>Master 节点在接受到修改请求时，会找此 file 文件最后一个 chunk 的 up-to-date 版本（最新版本），最新版本号应当等于 Master 节点的版本号；</p><p>什么叫最新版本。chunk 使用一个 chunk version 进行版本管理（分布式环境下普遍使用版本号进行管理，比如 Lamport 逻辑时钟）。一个修改涉及 3 个 chunk 的修改，如果某一个 chunk 因为网络原因没能够修改成功，那么其 chunk version 就会落后于其他两个 chunk，此 chunk 会被认为是过时的。</p><p>Master 在选择好 primary 节点后递增当前 chunk 的 chunk version，并通过 Master 的持久化机制持久化；</p><p>通过 Primary 与其他 chunkserver，发送修改此 chunk 的版本号的通知，而节点接收到次通知后会修改版本号，然后持久化；</p><p>Primary 然后开始选择 file 最后一个文件的 chunk 的末尾 offset 开始写入数据，写入后将此消息转发给其他 chunkserver，它们也对相同的 chunk 在 offset 处写入数据；</p></blockquote><p>博客作者认为 Master 维护 chunk version 的步骤是先递增当前 chunk 的 chunk version 再本地持久化，然后通知 Primary。个人认为这样不妥，因为一旦 Master 在持久化完 chunk version 和通知到 Primary 之间挂了，在 Master 重启之后，其会与所有 chunkserver 进行心跳来获取这些 chunkserver 拥有的 chunk 和其 chunk version，接着 Matser 会发现该 chunk 的所有 chunk version 都小于其从磁盘恢复的该 chunk 的 chunk version，从而认为真正持有最新 chunk version 的 chunkserver 还未恢复，进而不为该 chunk 提供服务。</p><p>因此我认为，Master 维护 chunk version 的步骤应该是先递增当前 chunk 的 chunk version 并通知 Primary，得到确切回复后再本地持久化，这样一旦 Master 在期间挂了，其恢复之后会发现 chunkserver 心跳上来的消息中，该 chunk 的 chunk version 大于本地磁盘恢复的 chunk version，从而认为 chunkserver 的 chunk version 最新并更新且持久化本地的 chunk version，进而能够提供服务，这样就与论文中的描述自恰了。</p><p>除以上一点外，其他的细节个人觉得博客都讲的很正确很 make sense 了~</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>优秀的设计往往很简单</li><li>很难设计出一个满足所有业务场景的系统，针对不同的场景会有不同的解决方案</li><li>设计系统时就需要考虑到系统的瓶颈在何处（磁盘，网络，CPU）</li><li>单节点的性能即使不高也没太大问题，扩展性更重要</li></ul><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-gfs.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="http://nil.csail.mit.edu/6.824/2020/video/3.html" target="_blank" rel="noopener">6.824 视频</a><br><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/gfs-sosp2003.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库系统调优时有关操作系统的知识与测试监控</title>
    <link href="/operating-system-performance-testing/"/>
    <url>/operating-system-performance-testing/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在测试数据库的性能时，往往需要通过从网卡，CPU，内存，磁盘，代码等方面出发进行性能调优，这期间可能会纵向扩展机器以获得更好的性能。然而，在不同的硬件机器上测试时，有时的结果可能与预期相差较多，单从软件方面进行猜想可能并不靠谱。因此，需要对机器硬件进行一定的了解和对应的测试，这样就能够在调优分析时利用底层的硬件信息来支撑瓶颈分析，从而更可能做出正确的判断。本篇博客将从网卡，CPU，内存，磁盘出发来介绍一些基本的知识和性能评测方式，最后也会介绍 Linux 下常用的性能测试工具 sysbench 的使用和性能监控工具 glances 的使用。</p><h2 id="网卡"><a href="#网卡" class="headerlink" title="网卡"></a>网卡</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>网卡，即网络接口卡（network interface card），也叫 NIC 卡，是一种允许网络连接的计算机硬件设备。网卡应用广泛，市场上有许多不同种类，如 PCIe网卡，服务器网卡。</p><p>基于不同的速度，网卡有 10Mbps，100Mbps， 10/100Mbps 自适应卡，1000Mbps、10Gbps、25Gbps、100Gbps 甚至更高速度的网卡。10Mbps、100Mbps 和 10/100Mbps 自适应网卡适用于小型局域网、家庭或办公室。1000Mbps 网卡可为快速以太网提供更高的带宽。10Gbps,25Gbps,100Gbps 网卡以及更高速度的网卡则受到大企业与数据中心的欢迎。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>Linux 下要想得知本机的网卡速率。可以查也可以测。<br>对于查，可以使用 <code>ifconfig</code> 找到网卡对应的设备名，然后使用 <code>ethtool &lt;device&gt;</code> 来查看对应网卡设备的额定速率。如下图所示：<br><img src="/operating-system-performance-testing/ethtool.png" srcset="/img/loading.gif" alt><br>对于测，可以使用 iperf 或者 iperf3 来测量 tcp/udp 协议的吞吐量，进而测试网卡的性能。当然，由于连接参数（比如客户端连接数，包大小）的不同，测试时可能也跑不满理论带宽，因此对实际线上系统调优操作系统网络栈就是一个可以优化性能的方向。<br>iperf 和 iperf3 的具体使用方式可参考此<a href="https://www.cnblogs.com/cloudwas/p/13084815.html" target="_blank" rel="noopener">博客</a>。</p><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><p>市面上常说的千兆网卡速率是 1000Mbps，即 125MB/s；万兆网卡速率是 10000Mbps，即 1.22GB/s；十万兆网卡速率是 100000Mbps，即 12.21 GB/s。</p><h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>中央处理器（CPU），是电子计算机的主要设备之一，电脑中的核心配件。其功能主要是解释计算机指令以及处理计算机软件中的数据。CPU 是计算机中负责读取指令，对指令译码并执行指令的核心部件。中央处理器主要包括两个部分，即控制器、运算器，其中还包括高速缓冲存储器及实现它们之间联系的数据、控制的总线。</p><p>CPU 的性能与以下衡量指标都有关系，具体可以参考此<a href="https://mp.weixin.qq.com/s/Hrn9J2l_Di3gZy0BvPcx6A" target="_blank" rel="noopener">博客</a>。</p><ul><li>主频（时钟频率）</li><li>外频（基准频率）</li><li>总线(FSB)频率</li><li>CPU 的位和字长</li><li>倍频系数</li><li>缓存</li><li>CPU 内核和 I/O 工作电压</li><li>制造工艺</li><li>指令集</li><li>CPU 扩展指令集</li><li>架构（如 UMA 或者 NUMA 架构），具体可以参考此<a href="https://www.cnblogs.com/linhaostudy/p/9980383.html" target="_blank" rel="noopener">博客</a>。</li></ul><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><p>Linux 下想要得知本机有关 CPU 的信息，可以使用 <code>lscpu</code> 指令，其会显示 CPU 的型号，架构，主频大小，缓存大小，物理和逻辑核心数等等，如下图所示。<br><img src="/operating-system-performance-testing/lscpu.png" srcset="/img/loading.gif" alt></p><p>我们在知道 CPU 型号之后也可以去相关网站寻找更多有关此 cpu 的数据信息，比如<a href="https://www.cpubenchmark.net/" target="_blank" rel="noopener">PassMark</a>。</p><p>在系统运行过程中，可以评测 CPU 使用率的一个重要参考指标就是平均负载（load average）。如下图所示<br><img src="/operating-system-performance-testing/load.png" srcset="/img/loading.gif" alt></p><p>平均负载是指单位时间内平均活跃进程数，包括可运行状态的进程数，以及不可中断状态的进程（如等待 IO,等待硬件设备响应），其可以一定程度上反映一段时间内 CPU 的繁忙程度。但是平均负载高 CPU 的使用率不一定高，其主要表现如下，具体可参考此<a href="https://www.cnblogs.com/maxwellsky/p/10629564.html" target="_blank" rel="noopener">博客</a>。</p><ul><li>CPU 密集型进程，导致平均负载和 CPU 使用率比较高</li><li>IO 密集型进程，等待 IO 会导致平均负载升高，但是 CPU 使用率不一定高</li><li>等待 CPU 的进程调度也会导致平均负载升高，此时 CPU 使用率也高</li></ul><h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h2><h3 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h3><p>内存是计算机中重要的部件之一，它是与 CPU 进行沟通的桥梁。计算机中所有程序的运行都是在内存中进行的，因此内存的性能对计算机的影响非常大。内存也被称为内存储器，其作用是用于暂时存放 CPU 中的运算数据，以及与硬盘等外部存储器交换的数据。只要计算机在运行中，CPU 就会把需要运算的数据调到内存中进行运算，当运算完成后 CPU 再将结果传送出来，内存的运行也决定了计算机的稳定运行。内存是由内存芯片、电路板、金手指等部分组成的。</p><p>有关内存的发展，分类等详细信息可以参考<a href="https://baike.baidu.com/item/%E5%86%85%E5%AD%98" target="_blank" rel="noopener">百度百科</a>。</p><h3 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h3><p>Linux 可以通过 <code>free -h</code> 命令来查看本机的内存大小等等，如下图所示<br><img src="/operating-system-performance-testing/memory.png" srcset="/img/loading.gif" alt><br>具体参数含义为：</p><ul><li>total：服务器内存总大小：31G</li><li>used：已经使用了多少内存：26G</li><li>free：未被任何应用使用的真实空闲内存</li><li>shared：被共享的物理内存</li><li>buff/cache：缓冲、缓存区内存数，缓存在应用之中</li><li>available：真正剩余的可被程序应用的内存数</li><li>Swap：swap 分区的大小</li></ul><h4 id="free-和-available-的区别"><a href="#free-和-available-的区别" class="headerlink" title="free 和 available 的区别"></a>free 和 available 的区别</h4><p>free 是真正尚未被使用的物理内存数量。<br>available 是应用程序认为可用内存数量，available = free + buffer + cache (注：只是大概的计算方法)</p><p>Linux 为了提升读写性能，会消耗一部分内存资源缓存磁盘数据，对于内核来说，buffer 和 cache 其实都属于已经被使用的内存。但当应用程序申请内存时，如果 free 内存不够，内核就会回收 buffer 和 cache 的内存来满足应用程序的请求。</p><h4 id="buff-和-cache-的区别"><a href="#buff-和-cache-的区别" class="headerlink" title="buff 和 cache 的区别"></a>buff 和 cache 的区别</h4><p>buffer 名为缓冲，cache 名为缓存。</p><p><img src="/operating-system-performance-testing/buffer_cache.png" srcset="/img/loading.gif" alt></p><ul><li><p>cache：文件系统层级的缓存，从磁盘里读取的内容是存储到这里，这样程序读取磁盘内容就会非常快，比如使用 grep 和 find 等命令查找内容和文件时，第一次会慢很多，再次执行就快好多倍，几乎是瞬间。但如上所说，如果对文件的更新不关心，就没必要清 cache，否则如果要实施同步，必须要把内存空间中的 cache 清楚下。</p></li><li><p>buffer：磁盘等块设备的缓冲，内存的这一部分是要写入到磁盘里的。这种情况需要注意，位于内存 buffer 中的数据不是即时写入磁盘，而是系统空闲或者 buffer 达到一定大小统一写到磁盘中，所以断电易失，为了防止数据丢失所以我们最好正常关机或者多执行几次 sync 命令，让位于 buffer 上的数据立刻写到磁盘里。</p></li></ul><h4 id="swap-是什么"><a href="#swap-是什么" class="headerlink" title="swap 是什么"></a>swap 是什么</h4><p>在 Linux 下，swap 的作用类似 Windows 系统下的“虚拟内存”。当物理内存不足时，拿出部分硬盘空间当 swap 分区（虚拟成内存）使用，从而解决内存容量不足的情况。</p><p>swap 意思是交换，顾名思义，当某进程向 OS 请求内存发现不足时，OS 会把内存中暂时不用的数据交换出去，放在 swap 分区中，这个过程称为 swap out。当某进程又需要这些数据且 OS 发现还有空闲物理内存时，又会把 swap 分区中的数据交换回物理内存中，这个过程称为 swap in。</p><p>当然，swap 大小是有上限的，一旦 swap 使用完，操作系统会触发 OOM-Killer 机制，把消耗内存最多的进程 kill 掉以释放内存。</p><p>有关数据库与 swap 的关系以及 swap 的细节机制可以参考此<a href="http://hbasefly.com/2017/05/24/hbase-linux/?lkfgjq=xbbdl2&amp;ekpqfu=awbp92" target="_blank" rel="noopener">博客</a>。</p><h3 id="补充-1"><a href="#补充-1" class="headerlink" title="补充"></a>补充</h3><p>CPU 的处理速度一般约为 10GB /s，常用的 DDR4 内存到 CPU 的吞吐一般约在 30GB/s ，所以内存的吞吐量往往不会成为瓶颈。当然，不同的语言对内存管理的方式不同，某些自动管理内存的语言（比如 Java）会出现 STW (Stop the world）的状况来调整内存空间，其会暂停所有用户线程，对性能影响很大。因此，基于业务负载和语言特性制定更好的 GC 策略能够更好地利用 CPU，从而进一步发掘内存的吞吐量。</p><h2 id="磁盘"><a href="#磁盘" class="headerlink" title="磁盘"></a>磁盘</h2><h3 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h3><p>磁盘是指利用磁记录技术存储数据的存储器。 磁盘是计算机主要的存储介质，可以存储大量的二进制数据，并且断电后也能保持数据不丢失。 早期计算机使用的磁盘是软磁盘（Floppy Disk，简称软盘），如今常用的磁盘是硬磁盘（Hard disk，简称硬盘）。</p><p>目前常见的硬盘大可分为三类：机械硬盘（HDD）采用磁性碟片来存储，固态硬盘（SSD）采用闪存颗粒来存储，混合硬盘（HHD）是把磁性碟片和闪存集成到一起的一种硬盘。</p><p>HDD 和 SSD 的区别主要如下：<br><img src="/operating-system-performance-testing/comparison.jpg" srcset="/img/loading.gif" alt></p><p>总体上来说，HDD 主要为 SATA 和 SAS 接口，目前家用类别的 HDD 多为 SATA 接口，SAS 接口则为企业级应用。SAS 可满足高性能、高可靠性的应用，SATA 则满足大容量、非关键业务的应用。</p><p>此外业界也常用多块 HDD 来组装磁盘阵列（比如 RAID 5 等等），从而提供更好的吞吐量和磁盘级别的容错，具体细节可参考此<a href="https://mp.weixin.qq.com/s/WRYQFRnf28RBK3CUkN7REg" target="_blank" rel="noopener">博客</a>。</p><p>至于固态硬盘，其常见接口和速率如下图所示：<br><img src="/operating-system-performance-testing/ssd.jpg" srcset="/img/loading.gif" alt></p><h3 id="测试-3"><a href="#测试-3" class="headerlink" title="测试"></a>测试</h3><p>Linux 可以通过 <code>df -hT</code> 命令来查看本机磁盘设备挂载的磁盘目录，文件系统以及容量等等，如下图所示<br><img src="/operating-system-performance-testing/df.png" srcset="/img/loading.gif" alt></p><p>此外还可以通过 <code>fdisk -l &lt;device&gt;</code> 命令来查看磁盘设备的源信息，比如可以通过此命令来查看磁盘是 SSD 还是 HDD，如果有”heads”（磁头），”track”（磁道）和”cylinders”（柱面）等等则就是 HDD，否则则是 SSD。如下图所示就是 SSD。<br><img src="/operating-system-performance-testing/fdisk.png" srcset="/img/loading.gif" alt></p><pre><code class="hljs yaml"><span class="hljs-comment"># SSD 示例</span><span class="hljs-attr">Disk /dev/nvme0n1:</span> <span class="hljs-number">238.5</span> <span class="hljs-string">GiB,</span> <span class="hljs-number">256060514304</span> <span class="hljs-string">bytes,</span> <span class="hljs-number">500118192</span> <span class="hljs-string">sectors</span><span class="hljs-attr">Units:</span> <span class="hljs-string">sectors</span> <span class="hljs-string">of</span> <span class="hljs-number">1</span> <span class="hljs-string">*</span> <span class="hljs-number">512</span> <span class="hljs-string">=</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-string">Sector</span> <span class="hljs-string">size</span> <span class="hljs-string">(logical/physical):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-string">I/O</span> <span class="hljs-string">size</span> <span class="hljs-string">(minimum/optimal):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-attr">Disklabel type:</span> <span class="hljs-string">dos</span><span class="hljs-attr">Disk identifier:</span> <span class="hljs-number">0xad91c214</span><span class="hljs-comment"># HDD 示例</span><span class="hljs-attr">Disk /dev/sda:</span> <span class="hljs-number">120.0</span> <span class="hljs-string">GB,</span> <span class="hljs-number">120034123776</span> <span class="hljs-string">bytes</span><span class="hljs-number">255</span> <span class="hljs-string">heads,</span> <span class="hljs-number">63</span> <span class="hljs-string">sectors/track,</span> <span class="hljs-number">14593</span> <span class="hljs-string">cylinders</span><span class="hljs-string">Units</span> <span class="hljs-string">=</span> <span class="hljs-string">cylinders</span> <span class="hljs-string">of</span> <span class="hljs-number">16065</span> <span class="hljs-string">*</span> <span class="hljs-number">512</span> <span class="hljs-string">=</span> <span class="hljs-number">8225280</span> <span class="hljs-string">bytes</span><span class="hljs-string">Sector</span> <span class="hljs-string">size</span> <span class="hljs-string">(logical/physical):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-string">I/O</span> <span class="hljs-string">size</span> <span class="hljs-string">(minimum/optimal):</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span> <span class="hljs-string">/</span> <span class="hljs-number">512</span> <span class="hljs-string">bytes</span><span class="hljs-attr">Disk identifier:</span> <span class="hljs-number">0x00074f7d</span></code></pre><p>如果想要测试磁盘的 IOPS 或者吞吐量，则可以使用一些磁盘测试工具进行测试，如 smartctl，sysbench 等等，后文会介绍如何使用 sysbench 测量磁盘性能。</p><p>有关文件系统的区别（xfs or ext4），可以参考此<a href="https://segmentfault.com/a/1190000008481493" target="_blank" rel="noopener">博客</a>。至于具体性能，由于上层应用不同，其在不同文件系统上的表现也不同，建议实际测试一下才能知道真实场景下哪个更优秀，一般情况下选择 xfs 就够用了。</p><p>有关磁盘 IO 的理论细节可以参考美团技术团队的<a href="https://tech.meituan.com/2017/05/19/about-desk-io.html" target="_blank" rel="noopener">博客</a>，十分详细。</p><h3 id="补充-2"><a href="#补充-2" class="headerlink" title="补充"></a>补充</h3><ul><li>吞吐量<ul><li>对于顺序读写，普通的 HDD 的吞吐量一般在 100 MB/s 左右，某些企业级 HDD 能够达到 200 MB/s 左右。对于随机读写，HDD 的性能表现很差，一般是几十兆每秒。</li><li>对于顺序读写，普通的 SSD 的吞吐量一般在 300~500MB/s 左右，某些企业级 SSD（例如适配 PCIE 3.0 接口）甚至能够达到 2GB/s。对于随机读写，SSD 的性能相比顺序读写下降较少，一般也是几百兆每秒。 </li></ul></li><li>时延<ul><li>图胜千言<br><img src="/operating-system-performance-testing/latency.jpg" srcset="/img/loading.gif" alt></li></ul></li></ul><h2 id="sysbench-测试工具"><a href="#sysbench-测试工具" class="headerlink" title="sysbench 测试工具"></a>sysbench 测试工具</h2><p>sysbench 是一款开源的多线程性能测试工具，可以执行 CPU/内存/线程/IO/数据库等方面的性能测试。<br>sysbench 支持以下几种测试模式 ：</p><ol><li>CPU 运算性能</li><li>内存分配及传输速度</li><li>磁盘 IO 性能</li><li>POSIX 线程性能</li><li>互斥锁性能测试</li><li>数据库性能(OLTP 基准测试)。目前 sysbench 主要支持 MySQL，PostgreSQL 等几种数据库。</li></ol><p>sysbench 的安装建议直接参考<a href="https://github.com/akopytov/sysbench" target="_blank" rel="noopener">官方 repo</a>，最详细最不踩坑。具体负载参数可以用 <code>sysbench --help</code> 查看也可以参考官方 repo，当然也可以查看此<a href="https://blog.51cto.com/3842834/2364563" target="_blank" rel="noopener">博客</a>和此<a href="https://blog.51cto.com/cyent/2350925" target="_blank" rel="noopener">博客</a>。</p><p>以下列出几种测试实例：</p><h3 id="CPU-运算性能"><a href="#CPU-运算性能" class="headerlink" title="CPU 运算性能"></a>CPU 运算性能</h3><h4 id="单核性能"><a href="#单核性能" class="headerlink" title="单核性能"></a>单核性能</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=cpu</span> <span class="hljs-string">--cpu-max-prime=5000</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Prime numbers limit:</span> <span class="hljs-number">5000</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">CPU speed:</span>    <span class="hljs-attr">events per second:</span>  <span class="hljs-number">3325.67</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0004s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">33274</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.30</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.30</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">1.21</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.30</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">9995.62</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">33274.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.9956</span><span class="hljs-string">/0.00</span></code></pre><h4 id="多核性能"><a href="#多核性能" class="headerlink" title="多核性能"></a>多核性能</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=cpu</span> <span class="hljs-string">--cpu-max-prime=5000</span> <span class="hljs-string">--threads=192</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">192</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Prime numbers limit:</span> <span class="hljs-number">5000</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">CPU speed:</span>    <span class="hljs-attr">events per second:</span> <span class="hljs-number">411077.30</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0017s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">4112237</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.30</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.46</span>         <span class="hljs-attr">max:</span>                                  <span class="hljs-number">147.50</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.46</span>         <span class="hljs-attr">sum:</span>                              <span class="hljs-number">1885541.21</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">21417.9010</span><span class="hljs-string">/378.98</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.8205</span><span class="hljs-string">/0.14</span></code></pre><h3 id="内存吞吐量"><a href="#内存吞吐量" class="headerlink" title="内存吞吐量"></a>内存吞吐量</h3><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=memory</span> <span class="hljs-string">--memory-block-size=4k</span> <span class="hljs-string">--memory-total-size=740G</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Running memory speed test with the following options:</span>  <span class="hljs-attr">block size:</span> <span class="hljs-string">4KiB</span>  <span class="hljs-attr">total size:</span> <span class="hljs-string">757760MiB</span>  <span class="hljs-attr">operation:</span> <span class="hljs-string">write</span>  <span class="hljs-attr">scope:</span> <span class="hljs-string">global</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">Total operations:</span> <span class="hljs-number">31972684</span> <span class="hljs-string">(3195628.16</span> <span class="hljs-string">per</span> <span class="hljs-string">second)</span><span class="hljs-number">124893.30</span> <span class="hljs-string">MiB</span> <span class="hljs-string">transferred</span> <span class="hljs-string">(12482.92</span> <span class="hljs-string">MiB/sec)</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0001s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">31972684</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">0.10</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.00</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">7085.34</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">31972684.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">7.0853</span><span class="hljs-string">/0.00</span></code></pre><h3 id="磁盘性能"><a href="#磁盘性能" class="headerlink" title="磁盘性能"></a>磁盘性能</h3><h4 id="顺序读写"><a href="#顺序读写" class="headerlink" title="顺序读写"></a>顺序读写</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=fileio</span> <span class="hljs-string">--num-threads=1</span> <span class="hljs-string">--file-num=20</span> <span class="hljs-string">--file-total-size=4G</span> <span class="hljs-string">--file-test-mode=seqrewr</span> <span class="hljs-string">prepare(run)</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Extra file open flags:</span> <span class="hljs-string">(none)</span><span class="hljs-number">20</span> <span class="hljs-string">files,</span> <span class="hljs-number">204.</span><span class="hljs-string">8MiB</span> <span class="hljs-string">each</span><span class="hljs-string">4GiB</span> <span class="hljs-string">total</span> <span class="hljs-string">file</span> <span class="hljs-string">size</span><span class="hljs-string">Block</span> <span class="hljs-string">size</span> <span class="hljs-string">16KiB</span><span class="hljs-string">Periodic</span> <span class="hljs-string">FSYNC</span> <span class="hljs-string">enabled,</span> <span class="hljs-string">calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">each</span> <span class="hljs-number">100</span> <span class="hljs-string">requests.</span><span class="hljs-string">Calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">at</span> <span class="hljs-string">the</span> <span class="hljs-string">end</span> <span class="hljs-string">of</span> <span class="hljs-string">test,</span> <span class="hljs-string">Enabled.</span><span class="hljs-string">Using</span> <span class="hljs-string">synchronous</span> <span class="hljs-string">I/O</span> <span class="hljs-string">mode</span><span class="hljs-string">Doing</span> <span class="hljs-string">sequential</span> <span class="hljs-string">rewrite</span> <span class="hljs-string">test</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">File operations:</span>    <span class="hljs-attr">reads/s:</span>                      <span class="hljs-number">0.00</span>    <span class="hljs-attr">writes/s:</span>                     <span class="hljs-number">50759.51</span>    <span class="hljs-attr">fsyncs/s:</span>                     <span class="hljs-number">10153.80</span><span class="hljs-attr">Throughput:</span>    <span class="hljs-string">read,</span> <span class="hljs-attr">MiB/s:</span>                  <span class="hljs-number">0.00</span>    <span class="hljs-string">written,</span> <span class="hljs-attr">MiB/s:</span>               <span class="hljs-number">793.07</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0011s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">609479</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.02</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">1.68</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.01</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">9834.28</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">609479.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.8343</span><span class="hljs-string">/0.00</span></code></pre><h4 id="乱序读写"><a href="#乱序读写" class="headerlink" title="乱序读写"></a>乱序读写</h4><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=fileio</span> <span class="hljs-string">--num-threads=1</span> <span class="hljs-string">--file-num=20</span> <span class="hljs-string">--file-total-size=4G</span> <span class="hljs-string">--file-test-mode=rndrw</span> <span class="hljs-string">prepare(run)</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-attr">Extra file open flags:</span> <span class="hljs-string">(none)</span><span class="hljs-number">20</span> <span class="hljs-string">files,</span> <span class="hljs-number">204.</span><span class="hljs-string">8MiB</span> <span class="hljs-string">each</span><span class="hljs-string">4GiB</span> <span class="hljs-string">total</span> <span class="hljs-string">file</span> <span class="hljs-string">size</span><span class="hljs-string">Block</span> <span class="hljs-string">size</span> <span class="hljs-string">16KiB</span><span class="hljs-attr">Number of IO requests:</span> <span class="hljs-number">0</span><span class="hljs-attr">Read/Write ratio for combined random IO test:</span> <span class="hljs-number">1.50</span><span class="hljs-string">Periodic</span> <span class="hljs-string">FSYNC</span> <span class="hljs-string">enabled,</span> <span class="hljs-string">calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">each</span> <span class="hljs-number">100</span> <span class="hljs-string">requests.</span><span class="hljs-string">Calling</span> <span class="hljs-string">fsync()</span> <span class="hljs-string">at</span> <span class="hljs-string">the</span> <span class="hljs-string">end</span> <span class="hljs-string">of</span> <span class="hljs-string">test,</span> <span class="hljs-string">Enabled.</span><span class="hljs-string">Using</span> <span class="hljs-string">synchronous</span> <span class="hljs-string">I/O</span> <span class="hljs-string">mode</span><span class="hljs-string">Doing</span> <span class="hljs-string">random</span> <span class="hljs-string">r/w</span> <span class="hljs-string">test</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">File operations:</span>    <span class="hljs-attr">reads/s:</span>                      <span class="hljs-number">43044.01</span>    <span class="hljs-attr">writes/s:</span>                     <span class="hljs-number">28696.01</span>    <span class="hljs-attr">fsyncs/s:</span>                     <span class="hljs-number">14348.50</span><span class="hljs-attr">Throughput:</span>    <span class="hljs-string">read,</span> <span class="hljs-attr">MiB/s:</span>                  <span class="hljs-number">672.54</span>    <span class="hljs-string">written,</span> <span class="hljs-attr">MiB/s:</span>               <span class="hljs-number">448.36</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0007s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">861345</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                    <span class="hljs-number">0.00</span>         <span class="hljs-attr">avg:</span>                                    <span class="hljs-number">0.01</span>         <span class="hljs-attr">max:</span>                                    <span class="hljs-number">0.41</span>         <span class="hljs-attr">95th percentile:</span>                        <span class="hljs-number">0.04</span>         <span class="hljs-attr">sum:</span>                                 <span class="hljs-number">9816.61</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">861345.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">9.8166</span><span class="hljs-string">/0.00</span></code></pre><h3 id="POSIX-线程性能"><a href="#POSIX-线程性能" class="headerlink" title="POSIX 线程性能"></a>POSIX 线程性能</h3><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span>  <span class="hljs-string">--test=threads</span> <span class="hljs-string">--num-threads=200</span> <span class="hljs-string">--thread-yields=100</span> <span class="hljs-string">--thread-locks=1</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">200</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">10.</span><span class="hljs-string">0140s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">54268</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                   <span class="hljs-number">11.03</span>         <span class="hljs-attr">avg:</span>                                   <span class="hljs-number">36.88</span>         <span class="hljs-attr">max:</span>                                   <span class="hljs-number">86.79</span>         <span class="hljs-attr">95th percentile:</span>                       <span class="hljs-number">47.47</span>         <span class="hljs-attr">sum:</span>                              <span class="hljs-number">2001322.28</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">271.3400</span><span class="hljs-string">/3.10</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">10.0066</span><span class="hljs-string">/0.00</span></code></pre><h3 id="互斥锁性能"><a href="#互斥锁性能" class="headerlink" title="互斥锁性能"></a>互斥锁性能</h3><pre><code class="hljs yaml"><span class="hljs-string">&gt;</span> <span class="hljs-string">sudo</span> <span class="hljs-string">sysbench</span> <span class="hljs-string">--test=mutex</span> <span class="hljs-string">--mutex-num=2048</span> <span class="hljs-string">--mutex-locks=20000</span> <span class="hljs-string">--mutex-loops=5000</span> <span class="hljs-string">run</span><span class="hljs-string">sysbench</span> <span class="hljs-number">1.0</span><span class="hljs-number">.20</span> <span class="hljs-string">(using</span> <span class="hljs-string">bundled</span> <span class="hljs-string">LuaJIT</span> <span class="hljs-number">2.1</span><span class="hljs-number">.0</span><span class="hljs-string">-beta2)</span><span class="hljs-attr">Running the test with following options:</span><span class="hljs-attr">Number of threads:</span> <span class="hljs-number">1</span><span class="hljs-string">Initializing</span> <span class="hljs-string">random</span> <span class="hljs-string">number</span> <span class="hljs-string">generator</span> <span class="hljs-string">from</span> <span class="hljs-string">current</span> <span class="hljs-string">time</span><span class="hljs-string">Initializing</span> <span class="hljs-string">worker</span> <span class="hljs-string">threads...</span><span class="hljs-string">Threads</span> <span class="hljs-string">started!</span><span class="hljs-attr">General statistics:</span>    <span class="hljs-attr">total time:</span>                          <span class="hljs-number">0.</span><span class="hljs-string">0387s</span>    <span class="hljs-attr">total number of events:</span>              <span class="hljs-number">1</span><span class="hljs-string">Latency</span> <span class="hljs-string">(ms):</span>         <span class="hljs-attr">min:</span>                                   <span class="hljs-number">38.49</span>         <span class="hljs-attr">avg:</span>                                   <span class="hljs-number">38.49</span>         <span class="hljs-attr">max:</span>                                   <span class="hljs-number">38.49</span>         <span class="hljs-attr">95th percentile:</span>                       <span class="hljs-number">38.25</span>         <span class="hljs-attr">sum:</span>                                   <span class="hljs-number">38.49</span><span class="hljs-attr">Threads fairness:</span>    <span class="hljs-string">events</span> <span class="hljs-string">(avg/stddev):</span>           <span class="hljs-number">1.0000</span><span class="hljs-string">/0.00</span>    <span class="hljs-string">execution</span> <span class="hljs-string">time</span> <span class="hljs-string">(avg/stddev):</span>   <span class="hljs-number">0.0385</span><span class="hljs-string">/0.00</span></code></pre><h2 id="glances-监控工具"><a href="#glances-监控工具" class="headerlink" title="glances 监控工具"></a>glances 监控工具</h2><p>glances 是一个基于 python 语言开发，可以为 Linux 或者 Unix 性能提供监视和分析性能数据的功能。glances 在用户的终端上显示重要的系统信息，并动态的进行更新，让管理员实时掌握系统资源的使用情况，而动态监控并不会消耗大量的系统资源，比如 CPU 资源，通常消耗小于 2%，glances 默认每两秒更新一次数据。同时 glances 还可以将监控数据导出到文件中，便于以后使用其他可视化工具（例如 grafana）对报告进行分析和图形绘制。</p><p>glances 可以分析系统的：</p><ul><li>CPU 使用率</li><li>内存使用率</li><li>内核统计信息和运行队列信息</li><li>磁盘 I/O 速度、传输和读/写比率</li><li>磁盘适配器</li><li>网络 I/O 速度、传输和读/写比率</li><li>页面监控</li><li>进程监控-消耗资源最多的进程</li><li>计算机信息和系统资源</li></ul><p>当然，也可以用一些专业的服务器监控平台，其包含的信息可能更详细。但是 glances 的一个重要优点是开箱即用，不用专门部署，具体安装方式和参数可参考<a href="https://github.com/nicolargo/glances" target="_blank" rel="noopener">官方 repo</a>，使用示例如下图所示：<br><img src="/operating-system-performance-testing/glances.jpeg" srcset="/img/loading.gif" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客简单介绍了网卡，CPU，内存和磁盘的一些理论知识和我个人认为写的比较好的博客，最后介绍了 Linux 下常用的性能测试工具 sysbench 的使用和性能监控工具 glances 的使用。</p><p>本篇博客涉及了较多硬件和操作系统的知识，其实这些都可以挖的更深，由于水平和时间有限，暂不继续深挖，希望看完本博客之后能对大家的系统调优有所帮助。</p>]]></content>
    
    
    
    <tags>
      
      <tag>操作系统</tag>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>开源协议解读</title>
    <link href="/open-source-license/"/>
    <url>/open-source-license/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>越来越多的开发者与设计者希望将自己的产品开源，以便其他人可以在他们的代码基础上做更多事，开源社区也因此充满生机。然而一旦开源，如何为代码选择开源许可证就是一个非常重要的问题。</p><p>相信很多人和我一样，在 Github 创建 repo 时对可以选择的多种 license 很迷糊，不知道该选哪个好，因此本篇博客就简单介绍一下常见的 license 和他们之间的区别，以做记录和学习。</p><h2 id="什么是许可协议"><a href="#什么是许可协议" class="headerlink" title="什么是许可协议"></a>什么是许可协议</h2><p>关于授权的确切含义存在很多困惑。当你授权你的作品时，你并没有放弃你的任何权利，你仍然拥有该作品的原始版权(或专利)。许可协议所做的是授予其他人使用该作品的特定权限。</p><p>不管产品是免费向公众分发还是出售，制定一份许可协议非常有用，否则，对于前者，你相当于放弃了自己所有的权利，任何人都没有义务表明你的原始作者身份，对于后者，你将不得不花费比开发更多的精力用来逐个处理用户的授权问题。</p><p>开源许可使得其他人无需寻求特殊许可就可以很容易地对项目做出贡献。它还保护了你作为原始创造者的身份，确保你至少能从自己的贡献中获得一些荣誉，这也有助于防止别人把你的工作据为己有。</p><h2 id="常见许可协议"><a href="#常见许可协议" class="headerlink" title="常见许可协议"></a>常见许可协议</h2><p>目前，国际公认的开源许可证共有 80 多种。它们的共同特征是，都允许用户免费地使用、修改、共享源码，但是都有各自的使用条件。以下介绍一些常见的许可协议</p><h3 id="GNU-GPL"><a href="#GNU-GPL" class="headerlink" title="GNU GPL"></a>GNU GPL</h3><p>GNU GPL（GNU General Public License）同其它的自由软件许可证一样，许可社会公众享有：运行、复制软件的自由，发行传播软件的自由，获得软件源码的自由，改进软件并将自己作出的改进版本向社会发行传播的自由。 </p><p>GPL 还规定：只要这种修改文本在整体上或者其某个部分来源于遵循 GPL 的程序，该修改文本的整体就必须按照 GPL 流通，不仅该修改文本的源码必须向社会公开，而且对于这种修改文本的流通不准许附加修改者自己作出的限制。因此，一项遵循 GPL 流通的程序不能同非自由的软件合并。GPL 所表达的这种流通规则称为 copyleft，表示与 copyright(版权)的概念“相左”。</p><p>GPL 协议最主要的几个原则：</p><ol><li>确保软件自始至终都以开放源代码形式发布，保护开发成果不被窃取用作商业发售。任何一套软件，只要其中使用了受 GPL 协议保护的第三方软件的源程序，并向非开发人员发布时，软件本身也就自动成为受 GPL 保护并且约束的实体。也就是说，此时它必须开放源代码。</li><li>GPL 大致就是一个左侧版权（Copyleft，或译为“反版权”、“版权属左”、“版权所无”、“版责”等）的体现。你可以去掉所有原作的版权信息，只要你保持开源，并且随源代码、二进制版附上 GPL 的许可证就行，让后人可以很明确地得知此软件的授权信息。GPL 精髓就是，只要使软件在完整开源的情况下，尽可能使使用者得到自由发挥的空间，使软件得到更快更好的发展。</li><li>无论软件以何种形式发布，都必须同时附上源代码。例如在 Web 上提供下载，就必须在二进制版本（如果有的话）下载的同一个页面，清楚地提供源代码下载的链接。如果以光盘形式发布，就必须同时附上源文件的光盘。</li><li>开发或维护遵循 GPL 协议开发的软件的公司或个人，可以对使用者收取一定的服务费用。但还是一句老话——必须无偿提供软件的完整源代码，不得将源代码与服务做捆绑或任何变相捆绑销售。</li></ol><p>GPL 协议和 BSD, Apache Licence 等鼓励代码重用的许可协议很不一样。GPL 的出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但不允许修改后和衍生的代码做为闭源的商业软件发布和销售。这也就是为什么我们能用免费的各种 linux，包括商业公司的 linux 和 linux 上各种各样的由个人，组织，以及商业软件公司开发的免费软件了。</p><h3 id="GNU-LGPL"><a href="#GNU-LGPL" class="headerlink" title="GNU LGPL"></a>GNU LGPL</h3><p>LGPL 是 GPL 的一个为主要为类库使用设计的开源协议。和 GPL 要求任何使用/修改/衍生之 GPL 类库的的软件必须采用 GPL 协议不同。LGPL 允许商业软件通过类库引用(link)方式使用 LGPL 类库而不需要开源商业软件的代码。这使得采用 LGPL 协议的开源代码可以被商业软件作为类库引用并发布和销售。</p><p>但是如果修改 LGPL 协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用 LGPL 协议。因此 LGPL 协议的开源代码很适合作为第三方类库被商业软件引用，但不适合希望以 LGPL 协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。</p><p>GPL/LGPL 都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品。</p><h3 id="GNU-AGPL"><a href="#GNU-AGPL" class="headerlink" title="GNU AGPL"></a>GNU AGPL</h3><p>原有的 GPL 协议，由于现在云服务厂商兴起（如：AWS）产生了一定的漏洞，比如使用 GPL 的自由软件，但是并不发布于网络之中，则可以自由的使用 GPL 协议却不开源自己私有的解决方案。AGPL 增加了对此做法的约束。</p><p>GPL 的约束生效的前提是“发布”软件，即使用了 GPL 成分的软件通过互联网或光盘 release 软件，就必需明示地附上源代码，并且源代码和产品也受 GPL 保护。</p><p>这样如果不“发布”就可以不受约束了。比如使用 GPL 组件编写一个 Web 系统，不发布这个系统，但是用这个系统在线提供服务，同时不开源系统代码。</p><h3 id="GNU-SSPL"><a href="#GNU-SSPL" class="headerlink" title="GNU SSPL"></a>GNU SSPL</h3><p>SSPL (Server Side Public License) 为 MongoDB 基于 GPLv3 上修改并提出的软件授权协议。</p><p>MongoDB 认为 AGPL “Remote Network Interaction” 条款叙述不够明确，容易造成混淆。加上许多云端服务商一直在挑战 AGPL 的底线，大量使用 MongoDB 来进行营利行为却不遵守 AGPL，所以才提出明确定义的 SSPL。</p><p>SSPL服务器端公共授权。许可证更改并不影响当前使用社区服务器的常规用户。根据 MongoDB 之前的 GNU AGPLv3 协议，想要将 MongoDB 作为公共服务运行的公司必须将他们的软件开源，或需要从 MongoDB 获得商业许可，该公司解释说，“然而，MongoDB 的普及使一些组织在违反 GNU AGPLv3 协议的边缘疯狂试探，甚至直接违反了协议。”</p><p>尽管 SSPL 与 GNU GPLv3 没有什么不同，但 SSPL 会明确要求托管 MongoDB 实例的云计算公司要么从 MongoDB 获取商业许可证，要么向社区开源其服务代码。最近闹得特别火的 <a href="https://mp.weixin.qq.com/s/qCJx3sw-Om3y1JwRoCnDbQ" target="_blank" rel="noopener">ElasticSearch 修改开源协议</a>就是从 Apache 2.0 修改到了 SSPL 协议以限制 AWS 厂商。</p><h3 id="BSD"><a href="#BSD" class="headerlink" title="BSD"></a>BSD</h3><p>BSD 是 “Berkeley Software Distribution” 的缩写，意思是”伯克利软件发行版”。</p><p>BSD 在软件分发方面的限制比别的开源协议（如 GNU GPL）要少。该协议有多种版本，最主要的版本有两个，新 BSD 协议与简单 BSD 协议，这两种协议经过修正，都和 GPL 兼容，并为开源组织所认可。</p><p>新 BSD 协议（3 条款协议）在软件分发方面，除需要包含一份版权提示和免责声明之外，没有任何限制。另外，该协议还禁止拿开发者的名义为衍生产品背书，但简单 BSD 协议删除了这一条款。</p><h3 id="MIT"><a href="#MIT" class="headerlink" title="MIT"></a>MIT</h3><p>MIT 是和 BSD 一样宽范的许可协议，源自麻省理工学院（Massachusetts Institute of Technology, MIT），又称 X11 协议。</p><p>MIT 协议可能是几大开源协议中最宽松的一个，核心条款是：该软件及其相关文档对所有人免费，可以任意处置，包括使用，复制，修改，合并，发表，分发，再授权，或者销售。唯一的限制是，软件中必须包含上述版权和许可提示。</p><p>这意味着：你可以自由使用，复制，修改，可以用于自己的项目。可以免费分发或用来盈利。唯一的限制是必须包含许可声明。</p><p>MIT 协议是所有开源许可中最宽松的一个，除了必须包含许可声明外，再无任何限制。</p><h3 id="Apache"><a href="#Apache" class="headerlink" title="Apache"></a>Apache</h3><p>Apache License（Apache 许可证），是 Apache 软件基金会发布的一个自由软件许可证。</p><p>Apache 协议 2.0 和别的开源协议相比，除了为用户提供版权许可之外，还有专利许可，对于那些涉及专利内容的开发者而言，该协议最适合。</p><p>Apache 协议还有以下需要说明的地方:</p><ol><li>永久权利：一旦被授权，永久拥有。</li><li>全球范围的权利：在一个国家获得授权，适用于所有国家。假如你在美国，许可是从印度授权的，也没有问题。</li><li>授权免费，且无版税：前期，后期均无任何费用。</li><li>授权无排他性：任何人都可以获得授权</li><li>授权不可撤消：一旦获得授权，没有任何人可以取消。比如，你基于该产品代码开发了衍生产品，你不用担心会在某一天被禁止使用该代码。</li></ol><p>分发代码方面包含一些要求，主要是，要在声明中对参与开发的人给予认可并包含一份许可协议原文。</p><h3 id="MPL"><a href="#MPL" class="headerlink" title="MPL"></a>MPL</h3><p>MPL（Mozilla Public License 1.1）协议允许免费重发布、免费修改，但要求修改后的代码版权归软件的发起者。这种授权维护了商业软件的利益，它要求基于这种软件的修改无偿贡献版权给该软件。这样，围绕该软件的所有代码的版权都集中在发起开发人的手中。但 MPL 是允许修改，无偿使用得。MPL 软件对链接没有要求。（要求假如你修改了一个基于MPL协议的源代码，则必须列入或公开你所做的修改，假如其他源代码不是基于MPL则不需要公开其源代码）</p><h3 id="Creative-Commons"><a href="#Creative-Commons" class="headerlink" title="Creative Commons"></a>Creative Commons</h3><p>Creative Commons (CC) 并非严格意义上的开源许可，它主要用于设计。Creative Commons 有多种协议，每种都提供了相应授权模式，CC 协议主要包含 4 种基本形式：</p><ol><li>署名权<br>必须为原始作者署名，然后才可以修改，分发，复制。</li><li>保持一致<br>作品同样可以在 CC 协议基础上修改，分发，复制。</li><li>非商业<br>作品可以被修改，分发，复制，但不能用于商业用途。但商业的定义有些模糊，比如，有的人认为非商业用途指的是不能销售，有的认为是甚至不能放在有广告的网站，也有人认为非商业的意思是非盈利。</li><li>不能衍生新作品<br>你可以复制，分发，但不能修改，也不能以此为基础创作自己的作品。</li></ol><p>CC 许可协议的这些条款可以自由组合使用。大多数的比较严格的 CC 协议会声明 “署名权，非商业用途，禁止衍生”条款，这意味着你可以自由的分享这个作品，但你不能改变它和对其收费，而且必须声明作品的归属。这个许可协议非常的有用，它可以让你的作品传播出去，但又可以对作品的使用保留部分或完全的控制。最少限制的 CC 协议类型当属 “署名”协议，这意味着只要人们能维护你的名誉，他们对你的作品怎么使用都行。</p><h2 id="许可协议区别"><a href="#许可协议区别" class="headerlink" title="许可协议区别"></a>许可协议区别</h2><p>根据使用条件的不同，开源许可证分成两大类。</p><ul><li>宽松式许可证</li><li>Copyleft 许可证</li></ul><h3 id="宽松式许可证"><a href="#宽松式许可证" class="headerlink" title="宽松式许可证"></a>宽松式许可证</h3><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><p>宽松式许可证（permissive license）是最基本的类型，对用户几乎没有限制。用户可以修改代码后闭源。</p><p>它有三个基本特点。</p><ol><li>没有使用限制：用户可以使用代码，做任何想做的事情。</li><li>没有担保：不保证代码质量，用户自担风险。</li><li>披露要求（notice requirement）：用户必须披露原始作者。</li></ol><h4 id="常见许可证"><a href="#常见许可证" class="headerlink" title="常见许可证"></a>常见许可证</h4><p>常见的宽松式许可证有四种。它们都允许用户任意使用代码，区别在于要求用户遵守的条件不同。</p><ol><li>BSD（二条款版）：分发软件时，必须保留原始的许可证声明。</li><li>BSD（三条款版）：分发软件时，必须保留原始的许可证声明。不得使用原始作者的名字为软件促销。</li><li>MIT：分发软件时，必须保留原始的许可证声明，与 BSD（二条款版）基本一致。</li><li>Apache 2：分发软件时，必须保留原始的许可证声明。凡是修改过的文件，必须向用户说明该文件修改过；没有修改过的文件，必须保持许可证不变。</li></ol><h3 id="Copyleft-许可证"><a href="#Copyleft-许可证" class="headerlink" title="Copyleft 许可证"></a>Copyleft 许可证</h3><h4 id="Copyleft-的含义"><a href="#Copyleft-的含义" class="headerlink" title="Copyleft 的含义"></a>Copyleft 的含义</h4><p>Copyleft 是理查德·斯托曼发明的一个词，作为 Copyright （版权）的反义词。</p><p>Copyright 直译是”复制权”，这是版权制度的核心，意为不经许可，用户无权复制。作为反义词，Copyleft 的含义是不经许可，用户可以随意复制。</p><p>但是，它带有前提条件，比宽松式许可证的限制要多。</p><ul><li>如果分发二进制格式，必须提供源码</li><li>修改后的源码，必须与修改前保持许可证一致</li><li>不得在原始许可证以外，附加其他限制</li></ul><p>上面三个条件的核心就是：修改后的 Copyleft 代码不得闭源。</p><h4 id="常见许可证-1"><a href="#常见许可证-1" class="headerlink" title="常见许可证"></a>常见许可证</h4><p>常见的 Copyleft 许可证也有四种（对用户的限制从最强到最弱排序）。</p><ol><li>Affero GPL (AGPL)：如果云服务（即 SAAS）用到的代码是该许可证，那么云服务的代码也必须开源。</li><li>GPL：如果项目包含了 GPL 许可证的代码，那么整个项目都必须使用 GPL 许可证。</li><li>LGPL：如果项目采用动态链接调用该许可证的库，项目可以不用开源。</li><li>Mozilla（MPL）：只要该许可证的代码在单独的文件中，新增的其他文件可以不用开源。</li></ol><h3 id="区别示意图"><a href="#区别示意图" class="headerlink" title="区别示意图"></a>区别示意图</h3><p>图胜千言<br><img src="/open-source-license/license1.png" srcset="/img/loading.gif" alt><br><img src="/open-source-license/license2.jpeg" srcset="/img/loading.gif" alt></p><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="什么叫分发（distribution）"><a href="#什么叫分发（distribution）" class="headerlink" title="什么叫分发（distribution）"></a>什么叫分发（distribution）</h3><p>除了 Affero GPL (AGPL) ，其他许可证都规定只有在”分发”时，才需要遵守许可证。换言之，如果不”分发”，就不需要遵守。</p><p>简单说，分发就是指将版权作品从一个人转移到另一个人。这意味着，如果你是自己使用，不提供给他人，就没有分发。另外，这里的”人”也指”法人”，因此如果使用方是公司，且只在公司内部使用，也不需要遵守许可证。</p><p>云服务（SaaS）是否构成”分发”呢？答案是不构成。所以你使用开源软件提供云服务，不必提供源码。但是，Affero GPL (AGPL) 许可证除外，它规定云服务也必须提供源码。</p><h3 id="开源软件的专利如何处理"><a href="#开源软件的专利如何处理" class="headerlink" title="开源软件的专利如何处理"></a>开源软件的专利如何处理</h3><p>某些许可证（Apache 2 和 GPL v3）包含明确的条款，授予用户许可，使用软件所包含的所有专利。</p><p>另一些许可证（BSD、MIT 和 GPL v2）根本没提到专利。但是一般认为，它们默认给予用户专利许可，不构成侵犯专利。</p><p>总得来说，除非有明确的”保留专利”的条款，使用开源软件都不会构成侵犯专利。</p><h3 id="什么是披露要求"><a href="#什么是披露要求" class="headerlink" title="什么是披露要求"></a>什么是披露要求</h3><p>所有的开源许可证都带有”披露要求”（notice requirement），即要求软件的分发者必须向用户披露，软件里面有开源代码。</p><p>一般来说，你只要在软件里面提供完整的原始许可证文本，并且披露原始作者，就满足了”披露要求”。</p><h3 id="GPL-病毒是真的吗"><a href="#GPL-病毒是真的吗" class="headerlink" title="GPL 病毒是真的吗"></a>GPL 病毒是真的吗</h3><p>GPL 许可证规定，只要你的项目包含了 GPL 代码，整个项目就都变成了 GPL。有人把这种传染性比喻成”GPL 病毒”。</p><p>很多公司希望避开这个条款，既使用 GPL 软件，又不把自己的专有代码开源。理论上，这是做不到的。因为 GPL 的设计目的，就是为了防止出现这种情况。</p><p>但是实际上，不遵守 GPL，最坏情况就是被起诉。如果你向法院表示无法履行 GPL 的条件，法官只会判决你停止使用 GPL 代码（法律上叫做”停止侵害”），而不会强制要求你将源码开源，因为《版权法》里面的”违约救济”没有提到违约者必须开源，只提到可以停止侵害和赔偿损失。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>随着时代的发展，开源协议也一直在迭代进化，希望我们大家都能够了解开源协议，遵守开源协议并捍卫开源协议。</p><p>这篇博客参考了许多相关博客以做一个汇总总结，如有侵权之处可联系我删除~</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html" target="_blank" rel="noopener">如何选择开源许可证？(阮一峰)</a><br><a href="https://zhuanlan.zhihu.com/p/135292511" target="_blank" rel="noopener">简介5大开源许可协议</a><br><a href="https://mp.weixin.qq.com/s/HDNYTwifzGOYTcu3x3wFjg" target="_blank" rel="noopener">拒绝云服务商白嫖，Elasticsearch 和 Kibana 变更开源许可协议</a><br><a href="https://mp.weixin.qq.com/s/wwlAH2MBAsujaPeqKqtjmw" target="_blank" rel="noopener">全球各种开源协议介绍</a><br><a href="https://mp.weixin.qq.com/s/1UXjwKjX22vIvRimr5BLNw" target="_blank" rel="noopener">若你要开源自己的代码，此文带你了解开源协议</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>开源</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Twitter 内存缓存系统分析论文阅读</title>
    <link href="/twitter-cache-analysis-thesis/"/>
    <url>/twitter-cache-analysis-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>基于一个物理现实：内存操作比磁盘操作快若干个数量级。现代网站服务广泛使用了类似 redis ，memcached 的内存缓存系统。其思想很简单：尽管海量数据最终都需要落盘持久化，但如果能够将一些常用的数据在内存中缓存以供读请求直接获取，则不仅能够降低请求时延，还能够提升系统吞吐量，而且也能够减少底层数据管理系统比如关系型数据库的负载。</p><p>内存缓存系统的大规模商用激发了业界对其的研究，现有的研究主要集中在如何提高吞吐量，如何减少缓存缺失率等等。由于用户场景与缓存系统的性能息息相关，之前也出现了生产系统的分析，但其针对的用户场景过少。这就导致了理论和生产环境之间的一个巨大 gap，因此业界需要一个能够包含大量用户场景的内存缓存系统分析；此外，业界对内存缓存系统的前提假设都是写少读多，而且许多理论比如内存管理都是基于数据大小是恒定这一假设来做的，那么实际上线的生产环境也都是理论假设的这样吗？此外，一些看似不重要的特性，比如 TTL 受到了业界极少的关注，那么它对于生产系统的性能影响真的很小吗？带着这些问题，我们开始介绍这篇论文。</p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>首先介绍数据情况，Twitter 内部的内存缓存集群都是单租户单层的容器化实例，这样的架构方便作者直接将集群的分析结果与商业逻辑对应，更能够反映现象的真实原因。可以看到，Twitter 的内存缓存集群很大。</p><p><img src="/twitter-cache-analysis-thesis/1.png" srcset="/img/loading.gif" alt></p><p>为了保证分析不受采样方法的影响，作者未取样的收集了两段区间为一周的集群请求全量元信息，在 80TB 的数据基础上做了详细的分析。而且，作者也将数据适度脱敏后进行了开源，能够让任何人去继续分析。</p><p><img src="/twitter-cache-analysis-thesis/2.png" srcset="/img/loading.gif" alt></p><h2 id="缓存场景"><a href="#缓存场景" class="headerlink" title="缓存场景"></a>缓存场景</h2><p>Twitter 内部的内存缓存系统主要有三种应用场景：</p><p><img src="/twitter-cache-analysis-thesis/3.png" srcset="/img/loading.gif" alt></p><h3 id="存储缓存"><a href="#存储缓存" class="headerlink" title="存储缓存"></a>存储缓存</h3><p>将一些在磁盘上的热点数据缓存在内存中，来减少底层涉及磁盘的数据系统比如关系型数据库的压力。</p><h3 id="计算缓存"><a href="#计算缓存" class="headerlink" title="计算缓存"></a>计算缓存</h3><p>缓存一些计算结果或预结算结果。随着 BI 时代的到来，许多公司都会利用机器学习的方法做用户画像或者是实时流推荐。这些算法的每次计算很难在秒级以下，而且用户的画像也一般不会在几小时内发生变化，因此企业一般会每计算一次用户的画像就将其设置一个几分钟或几小时的 TTL 存储到缓存中，这样既能够大幅度减少计算资源的消耗也能够保持 BI 逻辑，这种使用方式正在逐渐流行。</p><h3 id="瞬态数据缓存"><a href="#瞬态数据缓存" class="headerlink" title="瞬态数据缓存"></a>瞬态数据缓存</h3><p>有一些数据是不需要持久化的，只存在于内存缓存中。比如限速场景会把用户的请求存储到缓存中进行统计，一旦超速即开始限速，过期后隐形删除这些统计日志即可。</p><h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><h3 id="写多读少场景广泛存在"><a href="#写多读少场景广泛存在" class="headerlink" title="写多读少场景广泛存在"></a>写多读少场景广泛存在</h3><p>如下图所示，这是一个有关写请求占总请求比率的集群比例概率分布图。<br><img src="/twitter-cache-analysis-thesis/4.png" srcset="/img/loading.gif" alt><br>作者将比率大于 30% 的集群定义为写多读少负载，可以看到 twitter 内部至少 35 % 的集群都是写多读少的负载，这打破了业界对于内存缓存系统的前提假设，也展示了生产和理论之间的 gap。对于写多读少场景，长尾效应，扩展性受限都成了问题，而这一块却几乎没有学者研究过，因此作者为业界指明了一条研究方向：即写多读少场景内存缓存系统的优化。</p><h3 id="对象大小"><a href="#对象大小" class="headerlink" title="对象大小"></a>对象大小</h3><p>下图是集群中有关对象大小的若干集群比例概率分布图。<br><img src="/twitter-cache-analysis-thesis/5.png" srcset="/img/loading.gif" alt></p><h4 id="元数据大小"><a href="#元数据大小" class="headerlink" title="元数据大小"></a>元数据大小</h4><p>从前三张子图中可以看到，大部分对象数据都很小，那相比之下，原本被忽视的元数据消耗就非常大了，比如 memcached 中每个对象的元数据就有 56 字节。现有的研究大多数都是通过增加元数据来提高缓存命中率的。而实际上对于小对象场景来说，最小化元数据的大小能够增大缓存个数，也是一条提升性能的方向。</p><h4 id="key-大小"><a href="#key-大小" class="headerlink" title="key 大小"></a>key 大小</h4><p>在上图的第四张子图中可以观察到，对于 60 % 的 workload，key 和 value 的大小都在一个数量级，这很让人惊异。在观察了许多实际 key 后，作者发现很多业务使用了较为冗长的 key，比如把很长的多段命名空间都扔到了里面，这浪费了许多空间。由于直接让业务团队将 key 改小是不可行的，所以内存缓存系统也可以为 key 增加一个轻量级的压缩，这样也能够有效的增大缓存空间。这个工作实现起来很容易，但之前没有人观察到过这个现象，因此这也是作者的贡献。</p><h3 id="对象大小的动态分布"><a href="#对象大小的动态分布" class="headerlink" title="对象大小的动态分布"></a>对象大小的动态分布</h3><p>当前，内存缓存系统大多假设对象随着时间的推移大小不变，然而在生产系统中，作者观察到大部分时间中，其对象大小的分布并不是恒定的。</p><p><img src="/twitter-cache-analysis-thesis/6.png" srcset="/img/loading.gif" alt></p><p>这个不恒定主要有两种表现形式:</p><ul><li><p>第一种是周期性的变化，比如很多负载都具有白天对象更大的特征。如图所示，亮度发生变化代表分布发生了变化，很多亮度是有周期出现的。</p></li><li><p>第二种是临时的突变，过后即恢复。如图所示，存在个别无规律的亮点。</p></li></ul><p>这些突变给缓存系统的内存管理带来了很多挑战，就如同操作系统的内部碎片和外部碎片一样。作者实际测试发现，现有缓存系统的主流内存管理技术 slab-class ，在面对可变对象大小时表现十分受限，作者认为内存管理技术方面需要更大的创新，比如随着机器学习的发展，动态的预测缓存行为并做出智能的内存管理就是一个可行的方向。</p><h3 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h3><p>TTL，一个限定对象生命周期的内存缓存系统特有参数，作者进行了细致的分析。对应前面提到内存缓存的三种场景，TTL 都有作用。</p><h4 id="TTL-使用场景"><a href="#TTL-使用场景" class="headerlink" title="TTL 使用场景"></a>TTL 使用场景</h4><p><img src="/twitter-cache-analysis-thesis/7.png" srcset="/img/loading.gif" alt></p><ul><li>保证不一致上界：部分对一致性要求较低的业务可能在写缓存失败时并不重试，这可能最终导致缓存与实际数据的不一致，因此用户可以使用 TTL 这个属性给这个最终一致性的区间定一个上界，从而达到性能和不一致上限都得到保证的可控结果。</li><li>定期更新：用户画像随着时间推移可能逐渐变得不准确，不实时，因此需要一个 TTL 属性来隐性指示计算，即哪怕新一轮计算结果还没触发，每过 TTL 时间请求用户的画像就得重新计算一次，这其实是一个实时性和计算资源之间的 trade-off。</li><li>隐形删除：比如限速场景可以把用户每条请求数据的 TTL 设置为限速的时间窗口，这样既能够达到限速的目的又能够在一段时间后隐形删除掉这些数据。</li></ul><h4 id="小-TTL-能够限制工作集合的大小"><a href="#小-TTL-能够限制工作集合的大小" class="headerlink" title="小 TTL 能够限制工作集合的大小"></a>小 TTL 能够限制工作集合的大小</h4><p><img src="/twitter-cache-analysis-thesis/8.png" srcset="/img/loading.gif" alt></p><p>如图所示，作者统计了在有无 TTL 场景下的对象集合总大小和活跃对象集合大小，可以看到活跃的对象集合大小相比总大小始终在一个固定范围内。因此如果能够对过期数据清理得当，那么实际上不需要很大的缓存资源就能提供一个不错的缓存命中率。因此作者得出了结论，有效的主动过期策略比驱逐还要更重要。</p><p>现有的主动驱逐策略主要有两种：一种是 Facebook 提出的环形缓冲区策略，它在 TTL 个数较少或者多但不连续时表现都不好；另一种是当前 redis 的定期删除策略，由于其是遍历实现的，而为了保证及时删除扫表时间至少要和最小的 ttl 在一个量级 ，则对于较大的 ttl，其数据会被扫描多次。这样也一定程度上造成了 cpu 时间的浪费，而且也容易有缓存雪崩问题。因此作者认为有效的主动过期策略也需要进一步的创新。</p><h3 id="更多发现"><a href="#更多发现" class="headerlink" title="更多发现"></a>更多发现</h3><h4 id="生产数据统计"><a href="#生产数据统计" class="headerlink" title="生产数据统计"></a>生产数据统计</h4><p>请求激增不一定就是热点导致的，可能就是均匀的涨了一些。</p><h4 id="对象分布"><a href="#对象分布" class="headerlink" title="对象分布"></a>对象分布</h4><p>尽管有些许偏差，但针对对象的请求基本符合幂率分布</p><h4 id="驱逐策略"><a href="#驱逐策略" class="headerlink" title="驱逐策略"></a>驱逐策略</h4><p>尽管不同负载的表现情况不一样，但 FIFO 与 LRU 策略在大部分负载下的表现近似。这也预示业务可能实际没必要费事费力的去搞 LRU 策略，简单的 FIFO 就能达到类似的性能的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>证明了写多读少内存缓存场景的广泛存在，指明了一个可以研究的领域。</li><li>大多数场景的对象很小，要想提升吞吐量可以从减少元数据大小和适度压缩 key 入手。</li><li>在内存缓存的生产系统中，对象的大小并不是恒定不变的。基于对象大小固定不变进行的理论推理都存在问题。</li><li>有效的主动过期策略比驱逐策略更管用。</li></ul><h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><p>本文作者基于世界上最大的实时内容公司之一 Twitter 的内存缓存统计数据，使用强有力的数据分析纠正了若干业界对于内存缓存的误区，辩驳了若干业界的前提假设和主流思想，并提出了若干研究方向，开创了多个子领域。我个人认为这是一篇很有意义的文章。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://www.usenix.org/system/files/osdi20-yang.pdf" target="_blank" rel="noopener">论文</a><br><a href="https://www.usenix.org/sites/default/files/conference/protected-files/osdi20_slides_yang.pdf" target="_blank" rel="noopener">PPT</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Spark 论文阅读</title>
    <link href="/spark-thesis/"/>
    <url>/spark-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 Apache Spark 广泛使用以前，业界主要使用 MapReduce 和 Dryad 这样的集群计算框架来对大数据进行分布式处理。这类计算框架，最大的优点旨在帮助程序员专注业务编程，而非花精力分发计算任务和实现程序容错。</p><p>MapReduce 虽然对利用集群中的计算资源做了各类抽象，但还没有实现对集群内存的抽象封装。这样对那些需要重复利用中间结果集的应用就很不友好，比如机器学习和图算法，PageRank, K-means 聚类以及逻辑回归等等。此外，MapReduce 也很难支持高效的交互式数据分析，因涉及大量的即席数据查询，为确保下一次数据集可以被重用，需要借助存储物化结果集，这引发大量写入实体磁盘的操作，导致执行时间拉长。</p><p>意识到这个问题的存在，学者们做了大量尝试，比如 Pregel，它把大量中间数据缓存起来，专为图计算封装了框架；HaLoop 则提供了实现迭代算法的 MapReduce 接口。但这些仅仅对个案有帮助，回到通用的计算上来，毫无优势。比如最常见的数据分析，装载多样化多源头数据，展开即席查询等等。</p><p>综上，MapReduce 的局限可以总结为：</p><ul><li>编程模型的表达能力有限，仅靠 MapReduce 难以实现部分算法。</li><li>对分布式内存资源的使用方式有限，使得其难以满足迭代式分析场景和交互式分析场景，比如迭代式机器学习算法及图算法，交互式数据挖掘等。</li></ul><p>Spark RDD 作为一个分布式内存资源抽象便致力于解决 Hadoop MapReduce 的上述问题：</p><ul><li>通过对分布式集群的内存资源进行抽象，允许程序高效复用已有的中间结果。</li><li>提供比 MapReduce 更灵活的编程模型，兼容更多的高级算法。</li></ul><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD（Resilient Distributed Dataset，弹性分布式数据集）本质上是一种只读、分片的记录集合，只能由所支持的数据源或是由其他 RDD 经过一定的转换（Transformation）来产生。通过由用户构建 RDD 间组成的产生关系图，每个 RDD 都能记录到自己是如何由还位于持久化存储中的源数据计算得出的，即其血统（Lineage）。</p><p><img src="/spark-thesis/lineage.jpg" srcset="/img/loading.gif" alt></p><p>Spark 为 RDD 提供了 Transformation 和 Action 两种操作，前者可以从其他数据源读入数据生成 RDD 或利用已有的 RDD 生成新的只读 RDD。后者可对 RDD 进行计算操作并把一个结果值返回给客户端，或是将 RDD 里的数据写出到外部存储。</p><p><img src="/spark-thesis/Transformation_Action.jpg" srcset="/img/loading.gif" alt></p><p>Transformation 与 Action 的区别还在于，对 RDD 进行 Transformation 并不会触发计算：Transformation 方法所产生的 RDD 对象只会记录住该 RDD 所依赖的 RDD 以及计算产生该 RDD 的数据的方式；只有在用户进行 Action 操作时，Spark 才会调度 RDD 计算任务，依次为各个 RDD 计算数据，这是 RDD 典型的惰性计算。</p><h2 id="分布式共享内存模型对比"><a href="#分布式共享内存模型对比" class="headerlink" title="分布式共享内存模型对比"></a>分布式共享内存模型对比</h2><p>相比于 RDD 只能通过粗粒度的”转换”来创建（或是说写入数据），分布式共享内存（Distributed Shared Memory，DSM）是另一种分布式系统常用的分布式内存抽象模型：应用在使用分布式共享内存时可以在一个全局可见的地址空间中进行随机的读写操作。类似的系统包括了一些常见的分布式内存数据库（如 Redis、Memcached）。其最大的优点在于写一次，多机同步。集群中的所有计算机节点，在同一内存位置存储了同一份数据。弊端也很明显，一旦数据损坏，所有数据都要重新还原或重做；同步导致的延迟会很高，因为系统要保障数据的完整性，这在分布式数据库中常见。</p><p><img src="/spark-thesis/comparison.png" srcset="/img/loading.gif" alt></p><p>RDD 产生的方式限制了其只适用于那些只会进行批量数据写入的应用程序，但却使得 RDD 可以使用更为高效的高可用机制。RDD 与 DSM 的区别在于，前者是粗放式写入，通过转换函数生成，而后者在内存任意位置均可写入。RDD 不能很好地支持大批量随机写入，却可以很好的支持批量写入和分区容错。前面也说道，血统依赖是 RDD 容错的利器，丢失分区可重生。</p><p>RDD 的第二大优势在于，备份节点可以迅速的被唤起，去代替那些缓慢节点执行任务。即在缓慢节点执行任务的同时，备份节点同时也执行相同的任务，哪个节点快就用那个节点的结果。而 DSM 则会被备份节点干扰，引起大家同时缓慢，因为共享内存之间会同步状态，互相干扰。</p><p>RDD 还有两大优化点：基于数据存储分发任务和溢出缓存至硬盘。在大量写入的操作中，比如生成 RDD，会选择离数据最近的节点开始任务；而在只读操作中，大量数据没法存入内存时，会自动存到硬盘上而不是报错停止执行。</p><h2 id="计算调度"><a href="#计算调度" class="headerlink" title="计算调度"></a>计算调度</h2><p>前面我们提到，RDD 在物理形式上是分片的，其完整数据被分散在集群内若干机器的内存上。当用户通过 Transformation 创建出新的 RDD 后，新的 RDD 与原本的 RDD 便形成了依赖关系。根据用户所选 Transformation 操作的不同，RDD 间的依赖关系可以被分为两种：</p><ul><li>窄依赖（Narrow Dependency）：父 RDD 的每个分片至多被子 RDD 中的一个分片所依赖</li><li>宽依赖（Wide Dependency）：父 RDD 中的分片可能被子 RDD 中的多个分片所依赖</li></ul><p><img src="/spark-thesis/dependency.jpg" srcset="/img/loading.gif" alt></p><p>通过将窄依赖从宽依赖中区分出来，Spark 便可以针对 RDD 窄依赖进行一定的优化。首先，窄依赖使得位于该依赖链上的 RDD 计算操作可以被安排到同一个集群节点上流水线进行；其次，在节点失效需要恢复 RDD 时，Spark 只需要恢复父 RDD 中的对应分片即可，恢复父分片时还能将不同父分片的恢复任务调度到不同的节点上并发进行。</p><p>总的来说，一个 RDD 由以下几部分组成：</p><ul><li>其分片集合</li><li>其父 RDD 集合</li><li>计算产生该 RDD 的方式</li><li>描述该 RDD 所包含数据的模式、分片方式、存储位置偏好等信息的元数据</li></ul><p>在用户调用 Action 方法触发 RDD 计算时，Spark 会按照定义好的 RDD 依赖关系绘制出完整的 RDD 血统依赖，并根据图中各节点间依赖关系的不同对计算过程进行切分：</p><p><img src="/spark-thesis/stage.jpg" srcset="/img/loading.gif" alt></p><p>简单来说，Spark 会把尽可能多的可以流水线执行的窄依赖 Transformation 放到同一个 Job Stage 中，而 Job Stage 之间则要求集群对数据进行 Shuffle。Job Stage 划分完毕后，Spark 便会为每个 Partition 生成计算任务（Task）并调度到集群节点上运行。</p><p>在调度 Task 时，Spark 也会考虑计算该 Partition 所需的数据的位置：例如，如果 RDD 是从 HDFS 中读出数据，那么 Partition 的计算就会尽可能被分配到持有对应 HDFS Block 的节点上；或者，如果 Spark 已经将父 RDD 持有在内存中，子 Partition 的计算也会被尽可能分配到持有对应父 Partition 的节点上。对于不同 Job Stage 之间的 Data Shuffle，目前 Spark 采取与 MapReduce 相同的策略，会把中间结果持久化到节点的本地存储中，以简化失效恢复的过程。</p><p>当 Task 所在的节点失效时，只要该 Task 所属 Job Stage 的父 Job Stage 数据仍可用，Spark 只要将该 Task 调度到另一个节点上重新运行即可。如果父 Job Stage 的数据也已经不可用了，那么 Spark 就会重新提交一个计算父 Job Stage 数据的 Task，以完成恢复。有趣的是，从论文来看，Spark 当时还没有考虑调度模块本身的高可用，不过调度模块持有的状态只有 RDD 的血统图和 Task 分配情况，通过状态备份的方式实现高可用也是十分直观的。</p><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>Spark 为 RDD 提供了三种存储格式：</p><ul><li>内存中反序列化的 Java 对象；</li><li>内存中序列化的 Java 对象；</li><li>硬盘存储</li></ul><p>访问速度从快到慢，即第一种方式最快，无需任何转换就可以被自由访问。最后一种最慢，因每次使用，需从硬盘抽取数据，有不必要的 IO 开销。</p><p>当内存吃紧，新建的 RDD 分区没有足够内存存储时，Spark 会采用回收分区方式，以给新分区提供空间。回收机制采用的是常规 LRU（Least Recently Used）算法，即最近最少使用的算法。这套回收机制很有用，至少目前来说是。但权值机制也很有用，比如设定 RDD 的权限等级，控制 RDD 分区被回收的可能性。</p><h2 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h2><p>前面已经提到，Spark 可以利用血统依赖来恢复出现故障的 RDD，这样即可不用对中间结果做持久化。然而，在部分长链场景下，做 checkpoint 来持久化也是有必要的。这是因为如果血统依赖足够长，在故障之后，RDD 的恢复需要经历相当多的步骤，会导致时间过多的消耗，此时如果有 checkpoint 即可减少较多的时间消耗。</p><p>Spark 将 checkpoint 的决策留给了用户。实现 checkpoint 的 API 是 persist 的 replicate 开关，即:<br><pre><code class="hljs css"><span class="hljs-selector-tag">rdd</span><span class="hljs-selector-class">.persist</span>(<span class="hljs-selector-tag">REPLICATE</span>)</code></pre></p><p>通过定期将数据暂存至稳定的存储设备，可以保证在性能不大幅度下降的情况下优化 RDD 失效后的过长重算。</p><h2 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h2><p><img src="/spark-thesis/performance.jpg" srcset="/img/loading.gif" alt></p><p>Spark 在性能方面表现出众，对标物是 Hadoop，以下是基于 Amazon EC2 做出的 4 组对比数据：</p><ol><li>在图运算和迭代机器学习方面，优先 Hadoop 20 倍速度。性能的提高得益于无需硬盘 I/O，且在内存中的 Java 对象计算，没有序列化和反序列化的开销。</li><li>性能与扩展性都很好。单测一张分析报表，就比 Hadoop 提高了 40 倍性能</li><li>当有节点故障时，Spark 能自动恢复已丢失的分区</li><li>查询 1TB 的数据，延迟仅在 5-7 秒。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，Spark RDD 的亮点在于如下两点：</p><ul><li>通过对分布式集群的内存资源进行抽象，允许程序高效复用已有的中间结果且保证高可用性。</li><li>提供比 MapReduce 更灵活的编程模型，兼容更多的高级算法。</li></ul><p>比起类似于分布式内存数据库的那种分布式共享内存模型，Spark RDD 巧妙地利用了其不可变和血统依赖的特性实现了对分布式内存资源的抽象，很好地支持了批处理程序的使用场景，同时大大简化了节点失效后的数据恢复过程。</p><p>同时，我们也应该意识到，Spark 是对 MapReduce 的一种补充而不是替代：将那些能够已有的能够很好契合 MapReduce 模型的计算作业迁移到 Spark 上不会收获太多的好处（例如普通的 ETL 作业）。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf" target="_blank" rel="noopener">论文</a><br><a href="https://zhuanlan.zhihu.com/p/36288538" target="_blank" rel="noopener">Spark 博客</a><br><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-spark.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="http://nil.csail.mit.edu/6.824/2020/video/15.html" target="_blank" rel="noopener">6.824 视频</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么选择 git 来作为代码版本控制系统</title>
    <link href="/git-or-svn/"/>
    <url>/git-or-svn/</url>
    
    <content type="html"><![CDATA[<p>转载一篇介绍代码版本控制系统的<a href="https://zhuanlan.zhihu.com/p/27374348" target="_blank" rel="noopener">博客</a>，此外也可参考此<a href="https://www.zhihu.com/question/25491925" target="_blank" rel="noopener">博客</a>。</p><h2 id="版本控制系统"><a href="#版本控制系统" class="headerlink" title="版本控制系统"></a>版本控制系统</h2><p>代码作为软件研发的核心产物，在整个开发周期都在递增，不断合入新需求以及解决 bug 的新 patch，这就需要有一款系统，能够存储、追踪文件的修改历史，记录多个版本的开发和维护。于是，版本控制系统（Version Control Systems）应运而生，主要分为两类，集中式和分布式。</p><h2 id="集中式版本控制系统"><a href="#集中式版本控制系统" class="headerlink" title="集中式版本控制系统"></a>集中式版本控制系统</h2><p><img src="https://pic2.zhimg.com/80/v2-974ee843e5b45fa3c81701dcd86ce8e9_1440w.png" srcset="/img/loading.gif" alt></p><p>集中化的版本控制系统，诸如 CVS，SVN 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。</p><p>这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录。</p><p><img src="/git-or-svn/c-vcs.png" srcset="/img/loading.gif" alt></p><p>集中式版本控制系统的优点：</p><ol><li>操作简单，使用没有难度，可轻松上手。</li><li>文件夹级权限控制，权限控制粒度小。</li><li>对客户端配置要求不高，无需存储全套代码。</li></ol><p>集中式版本控制系统的缺点：</p><ol><li>网络环境要求高，相关人员必须联网才能工作。</li><li>中央服务器的单点故障影响全局，如果服务器宕机，所有人都无法工作。</li><li>中央服务器在没有备份的情况下，磁盘一旦被损坏，将丢失所有数据。</li></ol><h2 id="分布式版本控制系统"><a href="#分布式版本控制系统" class="headerlink" title="分布式版本控制系统"></a>分布式版本控制系统</h2><p><img src="https://pic2.zhimg.com/80/v2-fb54a44c9918cd2228224b89a64fa7d1_1440w.png" srcset="/img/loading.gif" alt></p><p>分布式版本控制系统的客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份。可能有人会问，我们公司使用 git 工具，也有”中央服务器”啊？其实，这个所谓的”中央服务器”仅仅是用来方便管理多人协作，任何一台客户端都可以胜任它的工作，它和所有客户端没有本质区别。</p><p><img src="/git-or-svn/d-vcs.png" srcset="/img/loading.gif" alt></p><p>分布式版本控制系统的优点：</p><ol><li>版本库本地化，版本库的完整克隆，包括标签、分支、版本记录等。</li><li>支持离线提交，适合跨地域协同开发。</li><li>分支切换快速高效，创建和销毁分支廉价。</li></ol><p>分布式版本控制系统的缺点：</p><ol><li>学习成本高，不容易上手。</li><li>只能针对整个仓库创建分支，无法根据目录建立层次性的分支。</li></ol><h2 id="svn-or-git"><a href="#svn-or-git" class="headerlink" title="svn or git?"></a>svn or git?</h2><p>svn 和 git 作为集中式和分布式版本控制系统的代表，都有广大的使用群体，两者的优缺点经常被比较。其实，工具对我们来说，就是帮助我们有效提升工作的效率与质量，最适合的就是最好的。我们引用几个开发场景来看看两个版本控制工具的适用范围。</p><h3 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h3><p>公司 A，非纯技术开发，项目包含大量媒体设计文件，相关人员只需下载自己关注的部分文件；员工 PC 电脑配置不高，没有空间拷贝整个项目资料。</p><p>适用：svn</p><p>分析：只需公司有一个足够大的服务器硬盘，员工本地只存储自己相关的文件夹，不必下载不想关的媒体文件，避免浪费文件传输时间。</p><h3 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h3><p>公司 B，嵌入式底层开发，项目人员较多并且分布在两个城市，代码庞大；用分支管理多机种并行开发，机种间经常相互合并新特性，新 patch。</p><p>适用：git</p><p>分析：</p><ol><li>git 有能力高效管理类似 Linux 内核一样的超大规模项目；</li><li>git 实现了离线开发、代码审核特性，解决了跨地域协同开发中代码质量和编码协同的问题；</li><li>分支管理功能强大，便于查询和追溯分支间的提交历史；</li><li>git 基于 DAG（有向非环图）的设计比 svn 的线性提交提供更好的合并追踪，避免不必要的冲突，提高工作效率。</li></ol><p><img src="https://pic2.zhimg.com/80/v2-6349574ea7ed79d1e9aa65fdd84bb141_1440w.png" srcset="/img/loading.gif" alt></p><h3 id="场景三"><a href="#场景三" class="headerlink" title="场景三"></a>场景三</h3><p>公司 C，某行业软件开发，包含敏感重要数据，代码仓库和版本发布权限掌握在客户手中，代码安全要求高，公司开发人员先将代码提交到本地仓库，只有在客户审核通过才能提交到发布仓库。</p><p>适用：git</p><p>分析：</p><ol><li>git 通过哈希加密保证数据的完整性，防止恶意篡改；</li><li>代码分布存储，异地容灾，保证数据安全；</li><li>git 支持团队成员自建本地版本库和分支，只有客户发出合并请求，开发人员才能提交代码，客户可以对提交说明、代码规范等方面逐一审核。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不难看出，git 凭借自身的优势，完美解决了大多数公司对版本控制工具的诉求。在当今敏捷开发成为主流，研发周期短，跨地域协同开发多的大形势下，选择 git 是大势所趋。</p>]]></content>
    
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>33 条常用 git 命令详解</title>
    <link href="/git-common-command/"/>
    <url>/git-common-command/</url>
    
    <content type="html"><![CDATA[<p>参考我的 <a href="https://github.com/LebronAl/git-tips" target="_blank" rel="noopener">Github Repo</a>。</p><p>由于作者能力有限，描述必然会有纰漏，欢迎提交 PR、创建 Issue 进一步交流。</p><p>如果看完之后有所收获，求求给个 Star 以表支持。😊</p>]]></content>
    
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MapReduce 论文阅读</title>
    <link href="/mapreduce-thesis/"/>
    <url>/mapreduce-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>在 20 世纪初，包括本文作者在内的 Google 的很多程序员，为了处理海量的原始数据，已经实现了数以百计的、专用的计算方法。这些计算方法用来处理大量的原始数据，比如，文档抓取（类似网络爬虫的程序）、Web 请求日志等等；也为了计算处理各种类型的衍生数据，比如倒排索引、Web 文档的图结构的各种表示形势、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合等等。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>大多数以上提到的数据处理运算在概念上很容易理解。然而由于输入的数据量巨大，因此要想在可接受的时间内完成运算，只有将这些计算分布在成百上千的主机上。如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>为了解决上述复杂的问题，本文设计一个新的抽象模型，使用这个抽象模型，用户只要表述想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面：利用一个输入 key/value pair 集合来产生一个输出的 key/value pair 集合。</p><p>MapReduce 库的用户可以用两个函数表达这个计算：Map 和 Reduce。</p><ul><li>用户自定义的 Map 函数接受一个输入的 key/value pair 值，然后产生一个中间 key/value pair 值的集合。MapReduce 库把所有具有相同中间 key 值 I 的中间 value 值集合在一起后传递给 reduce 函数。</li><li>用户自定义的 Reduce 函数接受一个中间 key 的值 I 和相关的一个 value 值的集合。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般的，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通常 Map 通过一个迭代器把中间 value 值提供给 Reduce 函数，这样 Reduce Worker 就可以处理无法全部放入内存中的大量的 value 值的集合。</li></ul><p>在概念上，用户定义的 Map 和 Reduce 函数都有相关联的类型：<br><pre><code class="hljs livescript"><span class="hljs-keyword">map</span><span class="hljs-function"><span class="hljs-params">(k1,v1)</span> -&gt;</span><span class="hljs-keyword">list</span>(k2,v2)reduce<span class="hljs-function"><span class="hljs-params">(k2,<span class="hljs-keyword">list</span>(v2))</span> -&gt;</span><span class="hljs-keyword">list</span>(v2)</code></pre><br>比如，输入的 key 和 value 值与输出的 key 和 value 值在类型上推导的域不同。此外，中间 key 和 value 值与输出 key 和 value 值在类型上推导的域相同。</p><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><p>通过将 Map 调用的输入数据自动分割为 M 个数据片段的集合，Map 调用被分布到多台机器上执行。输入的数据片段能够在不同的机器上并行处理。使用分区函数将 Map 调用产生的中间 key 值分成 R 个不同分区（例如，hash(key) mod R），Reduce 调用也被分布到多台机器上执行。分区数量（R）和分区函数由用户来指定。</p><p><img src="/mapreduce-thesis/mapreduce.png" srcset="/img/loading.gif" alt></p><p>上图展示了 MapReduce 实现中操作的全部流程。当用户调用 MapReduce 函数时，将发生下面的一系列动作：</p><ol><li>用户程序首先调用的 MapReduce 库将输入文件分成 M 个数据片度，每个数据片段的大小一般从 16MB 到 64MB(可以通过可选的参数来控制每个数据片段的大小)。然后用户程序在机群中创建大量的程序副本。</li><li>这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 任务或 Reduce 任务分配给一个空闲的 worker。</li><li>被分配了 map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。</li><li>缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的 key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker</li><li>当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。</li><li>Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。</li><li>当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对 MapReduce 调用才返回。</li></ol><h3 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h3><h4 id="worker-故障"><a href="#worker-故障" class="headerlink" title="worker 故障"></a>worker 故障</h4><p>master 与 worker 之间同步心跳，对于失效的 worker，根据其类型来做进一步处理：</p><ul><li>Map worker 故障：由于 Map 任务将数据临时存储在本地，所以需要重新执行。</li><li>Reduce worker 故障：由于 Reduce 任务将数据存储在全局文件系统中 ，所以不需要重新执行。</li></ul><h4 id="master-故障"><a href="#master-故障" class="headerlink" title="master 故障"></a>master 故障</h4><p>MapReduce 任务重新执行</p><h4 id="故障语义保证"><a href="#故障语义保证" class="headerlink" title="故障语义保证"></a>故障语义保证</h4><p>当用户提供的 Map 和 Reduce 操作是输入确定性函数（即相同的输入产生相同的输出）时，MapReduce 的分布式实现在任何情况下的输出都和所有程序没有出现任何错误、顺序的执行产生的输出是一样的。</p><ul><li>Map worker 任务的原子提交：每个 Map 任务生成 R 个本地临时文件，当一个 Map 任务完成时，worker 发送一个包含 R 个临时文件名的完成消息给 master。如果 master 从一个已经完成的 Map 任务再次接收到一个完成消息，master 将忽略这个消息；</li><li>Reduce worker 任务的原子提交：当 Reduce 任务完成时，Reduce worker 进程以原子的方式把临时文件重命名为最终的输出文件。如果同一个 Reduce 任务在多台机器上执行，针对同一个最终的输出文件将有多个重命名操作执行。MapReduce 依赖底层文件系统提供的重命名操作的原子性来保证最终的文件系统状态仅仅包含一个 Reduce 任务产生的数据。</li></ul><h3 id="存储位置优化"><a href="#存储位置优化" class="headerlink" title="存储位置优化"></a>存储位置优化</h3><p>核心思想：本地读文件以减少流量消耗</p><p>MapReduce 的 master 在调度 Map 任务时会考虑输入文件的位置信息，尽量将一个 Map 任务调度在包含相关输入数据拷贝的机器上执行；如果上述努力失败了，master 将尝试在保存有输入数据拷贝的机器附近的机器上执行 Map 任务(例如，分配到一个和包含输入数据的机器在一个交换机里的 worker 机器上执行)。</p><h3 id="任务粒度"><a href="#任务粒度" class="headerlink" title="任务粒度"></a>任务粒度</h3><p>理想情况下，M 和 R 应当比集群中 worker 的机器数量要多得多。在每台 worker 机器都执行大量的不同任务能够提高集群的动态的负载均衡能力，并且能够加快故障恢复的速度：失效机器上执行的大量 Map 任务都可以分布到所有其他的 worker 机器上去执行。</p><p>实际使用时建议用户选择合适的 M 值，以使得每一个独立任务都是处理大约 16M 到 64M 的输入数据（这样，上面描写的输入数据本地存储优化策略才最有效），另外，也建议把 R 值设置使用的 worker 机器数量的小倍数。比如：M=200000，R=5000，使用 2000 台 worker 机器。</p><h3 id="备用任务"><a href="#备用任务" class="headerlink" title="备用任务"></a>备用任务</h3><p>影响一个 MapReduce 的总执行时间最通常的因素是“落伍者”：在运算过程中，如果有一台机器花了很长的时间才完成最后几个 Map 或 Reduce 任务，导致 MapReduce 操作总的执行时间超过预期。</p><p>为了解决落伍者的问题，当一个 MapReduce 操作接近完成的时候，master 调度备用（backup）任务进程来执行剩下的、处于处理中状态（in-progress）的任务。无论是最初的执行进程、还是备用（backup）任务进程完成了任务，MapReduce 都把这个任务标记成为已经完成。此个机制通常只会占用比正常操作多几个百分点的计算资源。但能减少近 50% 的任务完成总时间。</p><h3 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h3><h4 id="分区函数"><a href="#分区函数" class="headerlink" title="分区函数"></a>分区函数</h4><p>MapReduce 缺省的分区函数是使用 hash 方法(比如，hash(key) mod R)进行分区。hash 方法能产生非常平衡的分区。然而，有的时候，其它的一些分区函数对 key 值进行的分区将非常有用。比如，输出的 key 值是 URLs，有的用户希望每个主机的所有条目保持在同一个输出文件中。为了支持类似的情况，MapReduce 库的用户需要提供专门的分区函数。例如，使用“hash(Hostname(urlkey))mod R”作为分区函数就可以把所有来自同一个主机的 URLs 保存在同一个输出文件中。</p><h4 id="顺序保证"><a href="#顺序保证" class="headerlink" title="顺序保证"></a>顺序保证</h4><p>MapReduce 确保在给定的分区中，中间 key/value pair 数据的处理顺序是按照 key 值增量顺序处理的。这样的顺序保证对每个分成生成一个有序的输出文件，这对于需要对输出文件按 key 值随机存取的应用非常有意义，对在排序输出的数据集也很有帮助。</p><h4 id="Combiner-函数"><a href="#Combiner-函数" class="headerlink" title="Combiner 函数"></a>Combiner 函数</h4><p>在某些情况下，Map 函数产生的中间 key 值的重复数据会占很大的比重，并且，用户自定义的 Reduce 函数满足结合律和交换律。在 2.1 节的词数统计程序是个很好的例子。由于词频率倾向于一个 zipf 分布(齐夫分布)，每个 Map 任务将产生成千上万个这样的记录<the,1>。所有的这些记录将通过网络被发送到一个单独的 Reduce 任务，然后由这个 Reduce 任务把所有这些记录累加起来产生一个数字。MapReduce 允许用户指定一个可选的 combiner 函数，combiner 函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。</the,1></p><p>Combiner 函数在每台执行 Map 任务的机器上都会被执行一次。一般情况下，Combiner 和 Reduce 函数是一样的。Combiner 函数和 Reduce 函数之间唯一的区别是 MapReduce 库怎样控制函数的输出。Reduce 函数的输出被保存在最终的输出文件里，而 Combiner 函数的输出被写到中间文件里，然后被发送给 Reduce 任务。</p><p>部分的合并中间结果可以显著的提高一些 MapReduce 操作的速度。</p><h4 id="输入和输出的类型"><a href="#输入和输出的类型" class="headerlink" title="输入和输出的类型"></a>输入和输出的类型</h4><p>支持常用的类型，可以通过提供一个简单的 Reader 接口实现来支持一个新的输入类型。Reader 并非一定要从文件中读取数据，比如可以很容易的实现一个从数据库里读记录的 Reader，或者从内存中的数据结构读取数据的 Reader。</p><h4 id="副作用"><a href="#副作用" class="headerlink" title="副作用"></a>副作用</h4><p>在某些情况下，MapReduce 的使用者发现，如果在 Map 或 Reduce 操作过程中增加辅助的输出文件会比较省事。MapReduce 依靠程序 writer 把这种“副作用”变成原子的和幂等的。通常应用程序首先把输出结果写到一个临时文件中，在输出全部数据之后，在使用系统级的原子操作 rename 重新命名这个临时文件。</p><h4 id="跳过损坏的记录"><a href="#跳过损坏的记录" class="headerlink" title="跳过损坏的记录"></a>跳过损坏的记录</h4><p>每个 worker 进程都设置了信号处理函数捕获内存段异常（segmentation violation）和总线错误（bus error）。 在执行 Map 或者 Reduce 操作之前，MapReduce 库通过全局变量保存记录序号。如果用户程序触发了一个系统信号，消息处理函数将用“最后一口气”通过 UDP 包向 master 发送处理的最后一条记录的序号。当 master 看到在处理某条特定记录不止失败一次时，master 就标志着条记录需要被跳过，并且在下次重新执行相关的 Map 或者 Reduce 任务的时候跳过这条记录。</p><h4 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h4><p>支持本地串行执行以方便调试</p><h4 id="状态信息"><a href="#状态信息" class="headerlink" title="状态信息"></a>状态信息</h4><p>master 支持嵌入 HTTP 服务器以显示一组状态信息页面，用户可以监控各种执行状态。状态信息页面显示了包括计算执行的进度，比如已经完成了多少任务、有多少任务正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理百分比等等</p><h4 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h4><p>MapReduce 库使用计数器统计不同事件发生次数。比如，用户可能想统计已经处理了多少个单词、已经索引的多少篇 German 文档等等。</p><p>这些计数器的值周期性的从各个单独的 worker 机器上传递给 master（附加在 ping 的应答包中传递）。master 把执行成功的 Map 和 Reduce 任务的计数器值进行累计，当 MapReduce 操作完成之后，返回给用户代码。 </p><p>计数器当前的值也会显示在 master 的状态页面上，这样用户就可以看到当前计算的进度。当累加计数器的值的时候，master 要检查重复运行的 Map 或者 Reduce 任务，避免重复累加（之前提到的备用任务和失效后重新执行任务这两种情况会导致相同的任务被多次执行）。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li>分布式的 Grep：Map 函数输出匹配某个模式的一行，Reduce 函数是一个恒等函数，即把中间数据复制到输出。</li><li>计算 URL 访问频率：Map 函数处理日志中 web 页面请求的记录，然后输出(URL,1)。Reduce 函数把相同 URL 的 value 值都累加起来，产生(URL,记录总数)结果。</li><li>网络链接倒排：Map 函数在源页面（source）中搜索所有的链接目标（target）并输出为(target,source)。Reduce 函数把给定链接目标（target）的链接组合成一个列表，输出(target,list(source))。</li><li>每个主机的检索词向量：检索词向量用一个(词,频率)列表来概述出现在文档或文档集中的最重要的一些词。Map 函数为每一个输入文档输出(主机名,检索词向量)，其中主机名来自文档的 URL。Reduce 函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，输出一个最终的(主机名,检索词向量)。</li><li>倒排索引：Map 函数分析每个文档输出一个(词,文档号)的列表，Reduce 函数的输入是一个给定词的所有（词，文档号），排序所有的文档号，输出(词,list（文档号）)。所有的输出集合形成一个简单的倒排索引，它以一种简单的算法跟踪词在文档中的位置。</li><li>分布式排序：Map 函数从每个记录提取 key，输出(key,record)。Reduce 函数不改变任何的值。这个运算依赖分区机制和排序属性。</li></ul><h2 id="经验分享"><a href="#经验分享" class="headerlink" title="经验分享"></a>经验分享</h2><ul><li>约束编程模式使得并行和分布式计算非常容易，也易于构造容错的计算环境；</li><li>网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽。</li><li>多次执行相同的任务可以减少硬件配置不平衡带来的负面影响，同时解决了由于机器失效导致的数据丢失问题。</li></ul><h2 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h2><ul><li>通过简单的接口实现了自动的并行化和大规模的分布式计算，通过使用 MapReduce 模型接口实现了在大量普通 PC 机上的高性能计算。</li><li>向工业界证明了 MapReduce 模型在分布式计算上的可行性，拉开了分布式计算的序幕并影响了其后所有的计算框架，包括现在流行的批处理框架 Spark 和流处理框架 Flink 都很受其影响。</li></ul><h2 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h2><ul><li>基于历史局限性和当时的成本考虑，没有利用内存去更高效的处理数据，不过也为 Spark 提供了思路。</li><li>没有将资料调度和计算调度分离，使得 MapReduce 系统看起来较为冗杂。在开源的 Hadoop 生态中，MapReduce 现只关注于计算，具体的资源调度由 Yarn 管理。 </li></ul><h2 id="相关系统"><a href="#相关系统" class="headerlink" title="相关系统"></a>相关系统</h2><ul><li>分布式存储系统：GFS/Colossus/HDFS</li><li>批处理框架：Spark</li><li>流处理框架：Flink</li><li>高可用机制：Chubby/ZooKeeper</li></ul><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="http://nil.csail.mit.edu/6.824/2020/notes/l01.txt" target="_blank" rel="noopener">6.824 讲义</a><br><a href="http://nil.csail.mit.edu/6.824/2020/video/1.html" target="_blank" rel="noopener">6.824 视频</a><br><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf" target="_blank" rel="noopener">论文</a><br><a href="https://github.com/Cxka/paper/blob/0a72fe0b354b65bac25e45163163eb2573f1faf2/map-reduce/map-reduce-cn.pdf" target="_blank" rel="noopener">中文翻译</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>Google</tag>
      
      <tag>分布式计算</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MaxCompute 跨域流量优化论文阅读</title>
    <link href="/maxcompute-yugong-thesis/"/>
    <url>/maxcompute-yugong-thesis/</url>
    
    <content type="html"><![CDATA[<h2 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h2><p>随着大数据技术的长足发展，大公司和云供应商在全球创建了数十个地理上分散的数据中心（分布式数据中心）。一个典型的数据中心可包含数万台计算机，这些数据中心为许多大规模的 IT 企业提供了计算和存储能力。在管理这种大规模、分布式数据中心的过程中，减少跨数据中心流量是提高整体性能的核心瓶颈之一。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>MaxCompute 是阿里巴巴的大型数据管理和分析平台，管理着数十个分布式数据中心。每个数据中心都包含成千上万台服务器，并且通过广域网（WAN）相互连接。这些数据中心每天新增 500 万张数据表，并为阿里巴巴的各种业务应用程序（例如淘宝，天猫等）执行多达 700 万次的分析工作。 MaxCompute 中的作业每天生产和使用大量数据，形成了复杂的数据依赖关系。尽管这些依赖关系大多都在本地数据中心内部，随着业务的增长，来自非本地数据中心的依赖关系也在迅速增加。由于跨数据中心的依赖关系，使得 MaxCompute 中大约产生了数百 PB 通过广域网传输的数据。</p><p>日益增长的跨数据中心传输需求带来了多个方面的问题。WAN 的带宽约为 Tbps，而数据中心内网络的聚合带宽则大得多；另外，WAN 延迟是数据中心内部网络延迟的 10-100 倍。除网速之外， WAN 的成本也十分昂贵。如今，跨 DC 的带宽已成为一种非常宝贵的资源，同时也是 MaxCompute 运营的性能瓶颈。在优化之前，WAN 的成本占 MaxCompute 总体运营成本的很大一部分——考虑到 MaxCompute 的日常运营规模庞大，这是巨大的财务负担。正因如此，减少跨地区带宽的使用已经逐渐成为阿里巴巴数据中心业务的一大挑战。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>MaxCompute 研发了一个跨域流量优化系统 Yugong（意译：愚公），其通过与 MaxCompute 在大范围生产环境下进行协同运算，极大地降低了由项目迁移（project migration）、副本拷贝（table replication）以及计算调度（job outsourcing）所产生的大量跨数据中心带宽流量，由此有效地降低了运营成本。</p><h3 id="业务数据分析"><a href="#业务数据分析" class="headerlink" title="业务数据分析"></a>业务数据分析</h3><p>MaxCompute 业务场景中的数据有 project，table 和 partition 三个级别。project 可以类比于关系数据库中 database 的概念，通常是业务功能上相近的表的集合。table 是用户数据的某一张关系型数据表，partition 是 table 根据时间进行分区的子表，通常每张表每天会产生一个分区。 </p><p>其特性如下：</p><ul><li>project，job 和 table 遵循幂律分布。作业和表之间的跨 DC 依赖关系呈现出长尾现象。相对较少的热表和大型作业在跨 DC 依赖关系中占很大一部分。</li><li>不论是数据大小还是分区数量，大部分作业的输入都要比输出大得多。</li><li>连续几天创建的表分区具有相似的大小。一个分区的大小比它的表的大小小数百倍。大多数作业是周期性的，连续几天的表访问模式是稳定的。</li><li>最近的表分区被更频繁地访问，其他作业对其依赖的数量随着访问偏移量的增加呈指数级下降。</li><li>一些表经常被远程 DC 中的作业读取，因此复制这些表可以节省跨 DC 的带宽。另外，调度一些作业在它们的非默认 DC 中运行可以减少跨 DC 带宽的使用。</li><li>DC 具有动态且不可预测的资源利用模式，在同一时间段内可能存在不同的资源瓶颈。</li></ul><h3 id="模型概述"><a href="#模型概述" class="headerlink" title="模型概述"></a>模型概述</h3><p>下图为下文常用到的一些符号及其解释。<br><img src="/maxcompute-yugong-thesis/notation.png" srcset="/img/loading.gif" alt></p><p>在下文中，我们将尝试最小化一天中跨集群的总带宽使用。我们假设连接每对 DC 的广域网具有相同的单位成本。我们假设来自同一个分时表的所有分区的大小是相同的，作业每天重复出现，并且它们的表访问模式连续几天都是相同的。因此，我们只需要考虑一天内运行的作业。我们使用“所有作业”来表示“在当前日期 t<sub>cur</sub> 上运行的所有作业”。表访问模式仅由在 t<sub>cur</sub> 上运行的作业生成。作业和访问模式实际上在几天内缓慢变化，我们将在 table replication 节再介绍如何应用于生产环境。</p><p><img src="/maxcompute-yugong-thesis/model_1.png" srcset="/img/loading.gif" alt><br><img src="/maxcompute-yugong-thesis/model_2.png" srcset="/img/loading.gif" alt></p><p>该分析模型在计算上是棘手的，因为它是（| T | + | P |）×| DC | 的整数规划问题，其中 | T | 是表的总数，| P | 是项目总数，并且 | DC | 是数据中心的数量。</p><p>根据前文讲述的业务数据分析，我们可以利用我们的发现来简化问题。由于作业倾向于读取最近的分区，并且它们对分区的依赖关系的大小随访问偏移的增加而呈指数下降，因此有理由首先假设我们有足够/无限的项目复制存储大小迁移，然后使用启发式方法确定固定存储预算下的寿命。这种简化极大地降低了模型的复杂性，因为我们实际上删除了存储空间的限制。</p><p>通过这种简化，我们可以将问题分解为两个问题：</p><ol><li>一个项目迁移问题，该问题首先在假定复制存储预算不受限制的情况下找到项目放置计划 P，</li><li>一个表复制问题，该问题在给定项目放置计划 P 后生成表复制计划 R，同时满足存储空间的约束。这种分离对于我们的生产环境也是很自然的，因为由于较高的迁移成本，项目放置/迁移不能频繁执行，而表复制计划可以更频繁地更新。</li></ol><h3 id="项目迁移"><a href="#项目迁移" class="headerlink" title="项目迁移"></a>项目迁移</h3><p>在简化的项目迁移模型中，我们删除了存储空间约束。根据给定的项目布置计划，可以通过以下方法获得 DC d 的最小跨 DC 带宽成本 BW<sup>opt（d）</sup>：</p><ol><li>从表 i 的 DC 中远程读取每个表 i 的所有必需数据。 如果从表 i 读取的数据量小于其分区的总大小； </li><li>否则将表 i 的所有分区存储在 DC d 中，并每天复制其最新分区。 </li></ol><p>因此，BW<sup>opt（d）</sup> 由以下公式 12 给出：</p><p><img src="/maxcompute-yugong-thesis/project_migration_1.png" srcset="/img/loading.gif" alt></p><p>上面关于 R<sub>i</sub><sup>t</sup>（d）沿时间维度 t 的总和有助于消除为每个时间分区共同考虑复制策略的复杂性。我们的目标是找到一个项目放置计划 P，以使总 BW<sup>opt（d）</sup> 最小化：</p><p><img src="/maxcompute-yugong-thesis/project_migration_2.png" srcset="/img/loading.gif" alt></p><p>即使进行了这种简化，项目和表的数量仍然很大，因为我们有成千上万个项目和数百万个表。我们利用跨 DC 依赖关系的幂律分布来进一步减小问题的大小，即少量表构成了大部分跨 DC 的依赖。因此，我们仅考虑项目迁移模型中具有最大依赖项大小的少数表。此外，在解决问题时，我们还发现，对少量项目进行迁移可以显着改善我们当前的项目布局，而随着影响力较大的项目已经放置在合适的 DC 中，随着进一步的迁移，这种改进会迅速降低。</p><p>我们的项目迁移策略还可用于解决新的项目放置问题。我们首先根据 DC的负载将新项目放置在 DC 中，然后通过将 MigCount 设置为新项目的数量来解决项目迁移问题。</p><h3 id="数据拷贝"><a href="#数据拷贝" class="headerlink" title="数据拷贝"></a>数据拷贝</h3><p>给定上一节计算出的项目放置计划 P，我们然后找到一个表复制计划（即，找到每个 DC d 中每个表 i 的寿命 L<sub>i</sub>（d））以最小化跨DC带宽的总成本， 同时满足存储空间约束。 我们首先假设连续两天的表访问模式相同，针对不同 DC 中所有表的寿命设计一种启发式方法。然后我们删除该假设，并考虑动态维护表副本的生命周期，因为表访问模式实际上随着时间逐渐变化。 请注意，我们的解决方案中的表访问矩阵 R<sub>i</sub><sup>t</sup>（d）仅包含不属于 DC d 中项目的表，即 X<sub>p（i），d</sub> = 0，因为我们仅关心远程读取。为简单起见，我们在随后的讨论中省略了 DC d。</p><h4 id="DP-解法"><a href="#DP-解法" class="headerlink" title="DP 解法"></a>DP 解法</h4><p>我们可以通过 DP 算法来获得给定复制存储大小和给定项目放置计划下的最佳表复制计划。 我们将 dp（i，s）表示为可以通过考虑复制存储大小约束 s 下的前 i 个表获得的最小跨 DC 带宽成本, 定义表 i 的寿命是 L<sub>i</sub>。 用于存储表 i 的副本的存储大小为 L<sub>i</sub>×S<sub>i</sub>。 则读取表 i 的分区所产生的跨 DC 带宽成本为：</p><ol><li>寿命未涵盖的部分的所有远程读取的总成本和。</li><li>寿命涵盖部分的复制成本。</li></ol><p><img src="/maxcompute-yugong-thesis/dp_1.png" srcset="/img/loading.gif" alt></p><p>因此 dp 算法的转移函数可以定义如下：</p><p><img src="/maxcompute-yugong-thesis/dp_2.png" srcset="/img/loading.gif" alt></p><p>存储预算为 s 的前 i 个表的最小跨 DC 带宽成本是枚举表 i 的所有可能寿命 L<sub>i</sub> 并取其中的最小值。</p><p>DP 算法的时间复杂度为 O（| T | | L | | storage |），其中 | T | 是表的数量（从几万到几百万），| L | 是每个表的可能寿命（通常为几百个），并且 | storage | 是 DP 公式中使用的复制存储单位的数量（大约十亿：大约是存储总预算（PB 级别）除以分区大小（MB 级别））。因此，DP 算法太昂贵了。</p><h4 id="贪心解法"><a href="#贪心解法" class="headerlink" title="贪心解法"></a>贪心解法</h4><p>作为 DP 算法的替代方法，我们提出了一种有效的贪心算法。 在每一步中，算法都会将表 i 的当前寿命 L<sub>i</sub> 最多提高 k 个单位，这由边际增益贪心地确定，其定义（公式 15）如下：</p><p><img src="/maxcompute-yugong-thesis/greedy.png" srcset="/img/loading.gif" alt></p><p>直观地，分子是通过将 L<sub>i</sub> 提前 k 个单位可以节省的跨 DC 带宽总成本，而分母是存储额外的 k 个分区副本所需的存储空间。 如果 Gain<sub>i</sub><sup>k</sup> &gt; 0，则意味着复制多余的 k 个分区可以进一步节省跨 DC 带宽。 请注意，当 L<sub>i</sub> = 0 时，我们需要从 Gain<sub>i</sub><sup>k</sup> 中减去 S<sub>i</sub>，因为我们需要使用跨 DC 带宽来复制分区 tp<sub>i</sub><sup>t<sub>cur</sub></sup>，而对于 L<sub>i</sub> &gt; 0，此成本 i 已经被 L<sub>i</sub>  = 0 时所覆盖。</p><p>下图就是提出的贪心算法（算法 1）。 该算法首先初始化最大优先队列（maxPQ），以在所有表 i 的 L<sub>i</sub> = 0 时，如果 Gain<sub>i</sub><sup>k</sup> &gt; 0，则保留所有可能的 Gain<sub>i</sub>。 然后，它将继续使 maxPQ 的最大增益出队，直到队列变空。 假设对于某个表 i，当前的最大增益为 Gain<sub>i</sub><sup>k’</sup> ，如果存储预算仍允许其他 k’ 个副本，则将 L<sub>i</sub> 提前 k’ 个单位。”l<sub>i</sub> = L<sub>i</sub>“ 条件是确保对于所有基于当前 L<sub>i</sub> 计算的 Gain<sub>i</sub><sup>k’</sup>，其中 0 ≤ k’&lt; k，i 只能使用一个 k’ 来推进 L<sub>i</sub>。 在L<sub>i</sub> 前进之后，将基于更新的 L<sub>i</sub> 来计算新增益 Gain<sub>i</sub><sup>k</sup> 并将其放入 maxPQ 中。</p><p><img src="/maxcompute-yugong-thesis/kprobe.png" srcset="/img/loading.gif" alt></p><p>以上算法的时间复杂度为 O（k | T || L | log（k | T || L |）），由于 k 仅为数百个数量级，因此它的耗费时间大大小于 DP 算法。 此外，我们证明了贪心算法在给定足够的存储预算的情况下可以获得最佳带宽成本，如下所示。</p><p>定理 1:</p><blockquote><p>将 STO<sub>rep</sub> 设置为达到等式 12 中的最佳带宽成本且 k 为最大寿命时使用的实际复制存储大小，算法 1 计算出的表复制计划能够给出与等式 12 相同的最佳带宽成本。</p></blockquote><p>当达到公式 12 中的最佳带宽成本时，令 L<sub>i</sub><sup>opt</sup> 为表 i 的寿命。 考虑算法 1 中表 i 的当前寿命 L<sub>i</sub>。我们有 L<sub>i</sub> &lt; L<sub>i</sub><sup>opt</sup>，并且将 L<sub>i</sub> 提升到 L<sub>i</sub><sup>opt</sup> 的收益高于任何 l &gt; L<sub>i</sub><sup>opt</sup> 的收益，因为 L<sub>i</sub><sup>opt</sup> 需要较少的存储空间，并且产生相同的读取次数（ 请注意，在给定无限复制存储预算的情况下，因此只要从该分区的远程读取大小大于该分区的大小，就可以复制任何分区，如公式 12 所示）因此，如果我们的存储预算与等式 12 中用于获得最佳带宽成本的实际复制存储大小相同，则在某一点上表 i 将从 maxPQ 出队，并得到 L<sub>i</sub> 到 L<sub>opt</sub> 的增益。 之后进一步提升 L<sub>i</sub> 的增益将变为 0，从而不会再入队。</p><h4 id="动态维护"><a href="#动态维护" class="headerlink" title="动态维护"></a>动态维护</h4><p>我们的贪心解决方案目前仅考虑固定的表访问模式。实际上，由于业务的增长和偶尔的临时工作，表访问模式实际上在随时间变化（尽管缓慢）。 因此，我们需要定期更新表复制计划。 假设计划每 δ 天更新一次。 现在的问题是，给定当前的复制计划 R，我们需要找到一个新的复制计划 R’，以便它可以用作接下来 δ 天的良好复制计划，以及最小化从 R 至 R’ 状态迁移的带宽消耗。 作为过渡成本的示例，假设表 i 的寿命是 R 中的 L<sub>i</sub> 和 R’ 中的 L’<sub>i</sub>，并且 L<sub>i</sub> &lt; L’<sub>i</sub>，这意味着 R’ 对表 i 的覆盖范围比 R 多。 为了将较旧的分区从（t<sub>cur</sub> -L’<sub>i</sub>）复制到（t<sub>cur</sub> -L<sub>i</sub>），需要额外的带宽以保证从 R 过渡到 R’。</p><p>更新复制计划的最简单方法是每 δ 天重新运行一次算法 1，但这可能会导致相当大的过渡成本。考虑到复制较旧分区所产生的成本，我们建议对增益函数进行简单的修改。 设 I<sub>i，t</sub> 为指标，如果 tp<sub>i</sub><sup>t</sup> 被 R 覆盖，则 I<sub>i，t</sub>  = 1，否则，I<sub>i，t</sub> = 0。 我们将 G<sub>i，t</sub> 定义为如果新计划涵盖 tp<sub>i</sub><sup>t</sup> 可以节省的带宽量。</p><p><img src="/maxcompute-yugong-thesis/incremental_maintenance_1.png" srcset="/img/loading.gif" alt></p><p>直觉是，如果分区 tp<sub>i</sub><sup>t</sup> 不在复制计划中，则将其包括在计划中需要付出一定的代价，该损失等于在 δ 天内复制 tp<sub>i</sub><sup>t</sup> 的摊余带宽成本。也就是说，除非收益很大，否则我们不鼓励复制较旧的分区。通过在公式 15 中用 G<sub>i</sub><sup>t</sup> 代替 R<sub>i</sub><sup>t</sup>，我们获得了一个新的增益函数：</p><p><img src="/maxcompute-yugong-thesis/incremental_maintenance_2.png" srcset="/img/loading.gif" alt></p><p>除了使用新的增益函数外，我们还取了前 δ 天的表访问矩阵 R<sub>i</sub><sup>t</sup> 的平均值以减少表访问模式中振荡的影响。</p><h3 id="计算调度"><a href="#计算调度" class="headerlink" title="计算调度"></a>计算调度</h3><p>计算调度，即将作业调度到非默认 DC 进行处理，该 DC 可能包含作业所需的全部或部分输入表。当输入数据较大时，计算调度可以减少跨 DC 的带宽使用。这也可以用于在分布式控制系统之间平衡负载和各种资源（例如，中央处理器、内存、磁盘、网络等）的利用率来提高整体资源利用率（从而节省生产成本）。</p><p>对比前两种离线方式，因为调度决策需要考虑分布式控制系统的负载和资源利用率，所以计算调度需要在线解决方案。此外我们还需要考虑远程分布式控制系统是否有空闲资源来运行该作业，以及预期的作业完成时间(包括远程分布式控制系统中的等待时间)是否短于作业的默认 DC 时间。因此，我们设计了一个简单的评分函数来决定是否将工作 j 调度给 DC d：</p><p><img src="/maxcompute-yugong-thesis/job_outsourcing.png" srcset="/img/loading.gif" alt></p><p>其中 Cost（j，d）和 WaitT（j，d）是跨 DC 带宽的总成本以及如果将工作 j 外包给 DC d 的估计等待时间，而 AvailResrc（d）是DC d 中的可用资源量。请注意，Cost（j，d）包括将所有必要的信息/数据发送到 DC d 以执行作业，并将作业输出传回默认的 DC。我们仔细调整了参数 α 和 β，以降低跨 DC 带宽的成本。</p><h3 id="效果评估"><a href="#效果评估" class="headerlink" title="效果评估"></a>效果评估</h3><p>首先报告愚公在阿里巴巴投入生产的整体表现。下图显示了在典型的一天中每个 DC 传入的跨 DC 带宽使用量的减少。 愚公将不同 DC 的跨 DC 带宽使用率从 14％ 降低到 88％ 。 DC2 具有最大的减少量，因为它具有最大的远程依赖性。当天，愚公总共减少了总带宽使用量的 76％。</p><p><img src="/maxcompute-yugong-thesis/performance.png" srcset="/img/loading.gif" alt></p><h2 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h2><ul><li>根据实际业务的工作流来将如何减少跨域流量的难题解耦为三个容易分开解决的子问题。</li><li>在数据拷贝部分进行了系统的分析与设计并就此过程中的取舍进行了讨论。</li></ul><h2 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h2><ul><li>很多简化部分很直接，这可能是容易想到的最直接最容易实现的方法，但也可能还有一定的可优化空间。</li><li>如果能够将存储空间成本和带宽成本量化似乎模型会更准确。</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="地理分布式调度"><a href="#地理分布式调度" class="headerlink" title="地理分布式调度"></a>地理分布式调度</h3><p>最近有关分析工作负载的地理分布调度工作仅考虑了少量作业，并假设数据在地理分布的 DC 之间进行了分区，并且一项任务可以在多个 DC 中运行。 Iridium 优化了任务调度和数据放置，以实现分析查询的低延迟。 Geode 和 WANalytics 使查询计划了解 WAN，并为整个 DC 的数据分析提供以网络为中心的优化。 Clarinet 通过考虑网络带宽，任务位置，网络传输调度和多个并发查询，提出了一种支持 WAN 的查询优化器。Tetrium 考虑了地理分布的 DC 中用于任务放置和作业调度的计算和网络资源。Pixida 应用图分区来最大程度地减少数据分析作业中的跨 DC 任务依赖性。Hung 提出了地理分布的作业调度算法，以最大程度地减少整体作业的运行时间，但并未考虑 DC 之间的 WAN 带宽使用情况。 Bohr 利用地理分布的 OLAP 数据立方体展示了不同 DC 中数据之间的相似性。 Lube 实时检测和缓解地理分布数据分析查询中的瓶颈。</p><p>还有其他使数据流分析，分布式机器学习和图形分析可感知 WAN 的工作。 JetStream 提出了显式的编程模型，以减少分析流数据集所需的带宽。对于机器学习工作负载，Gaia 和 GDML 开发了地理分布式解决方案，以有效利用稀缺的 WAN 带宽，同时保留 ML 算法的正确性。 Monarch 和 ASAP 提出了一种地理分布图模式挖掘的近似解决方案。</p><h3 id="云原生数据仓库"><a href="#云原生数据仓库" class="headerlink" title="云原生数据仓库"></a>云原生数据仓库</h3><p>Google BigQuery，Amazon Redshift，Microsoft Azure Cosmos DB 和 Alibaba MaxCompute 是大型数据仓库产品。 MaxCompute 中的 “project” 概念对应于 Redshift 中的 “database” 和 BigQuery 中的 “project”。 尽管愚公在此工作中主要是用作 MaxCompute 的插件构建的，但类似的想法也可以应用于其他地理分布的数据仓库平台。</p><h3 id="缓存和打包"><a href="#缓存和打包" class="headerlink" title="缓存和打包"></a>缓存和打包</h3><p>从 CPU 高速缓存，内存高速缓存到应用程序级高速缓存，高速缓存管理是在不同级别的计算机体系结构上经过充分研究的主题。 Memcached 和 Redis 是高度可用的分布式键值存储，可在磁盘上提供内存缓存。EC-Cache 和 SP-Cache 为数据密集型群集和对象存储提供了内存中缓存。Piccolo，Spark，PACMan 和 Tachyon 结合了用于集群计算框架的内存缓存。在这项工作中，我们使用磁盘存储作为远程分区的缓存，以减少跨 DC 带宽。 表复制问题是背包问题的变体，项目放置问题是装箱问题的变体。 Tetris 将多资源分配问题与多维 bin 打包问题进行了类比，以进行任务调度。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="http://www.vldb.org/pvldb/vol12/p2155-huang.pdf" target="_blank" rel="noopener">论文</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式调度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>JProfile 远程配置</title>
    <link href="/jprofile-remote/"/>
    <url>/jprofile-remote/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>JProfile 是 Java 常用的性能分析工具，常被用来查看内存占用，CPU 时间消耗等数据。本地配置 JProfile 较为简单，可以参考此<a href="https://www.javatt.com/p/48237" target="_blank" rel="noopener">博客</a>。在一些情况下，监控的 Java 服务跑在服务器上，因此不能使用本地的 JProfile 来监控。不过 JProfile 能够支持远程连接监控，只不过需要一些额外的配置。此博客的内容就是介绍如何使用 JProfile 监控远程的 Java 应用。</p><h2 id="配置方案一：ssh-免密连接"><a href="#配置方案一：ssh-免密连接" class="headerlink" title="配置方案一：ssh 免密连接"></a>配置方案一：ssh 免密连接</h2><ol><li>从<a href="https://www.ej-technologies.com/download/jprofiler/files" target="_blank" rel="noopener">官方网站</a>上根据操作系统下载对应本地和服务器的 JProfile（需要确保两者版本一致）。注意服务端选择下载 <code>Setup Executable</code> 的可执行脚本。</li><li>客户端安装配置好本地的 JProfile， 然后将服务器的 JProfile 利用 scp 等工具传到服务器上去（也可在上述网页中直接复制链接在服务器上 wget 或者 curl -O 下载）。</li><li>服务端执行 <code>sh ./jprofiler_linux_11_1_4.sh</code> 来安装 JProfile，无脑同意 OK 和 Enter 就能够让其正确安装并且后台运行起来。</li><li>服务端启动要监控的应用程序，如 IoTDB。</li><li>客户端打开 JProfile，点击 <code>Profile a demo session or a saved session</code>，选择 <code>New Session</code> 栏的 <code>New Session</code> 手动创建一个远程连接。<br><img src="/jprofile-remote/jprofile_0.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_1.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_2.png" srcset="/img/loading.gif" alt></li><li>填好 <code>Session name</code>，<code>Attach type</code> 选择 <code>Attach to remote JVM</code>，连接方式选择 <code>SSH tunnel</code>，然后点击 <code>Edit</code> 开始配置服务器 ip, host, ssh 秘钥等信息。<br><img src="/jprofile-remote/jprofile_3.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_4.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_5.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_6.png" srcset="/img/loading.gif" alt></li><li>配置完后点击 finish 再点击 ok 即可连接到远程的 JProfile，接着选择对应要监控的 Java 应用程序。<br><img src="/jprofile-remote/jprofile_7.png" srcset="/img/loading.gif" alt></li><li>配置监控模式，一般选择 <code>Async sampling</code>，该模式对性能影响最小。<br><img src="/jprofile-remote/jprofile_8.png" srcset="/img/loading.gif" alt></li><li><p>配置调用树的过滤器，该参数一般用来指示 jprofile 来记录 cpu view 时的函数耗时统计。加上要统计的包的前缀即可，比如 <code>org.apache.iotdb</code>。<br><img src="/jprofile-remote/jprofile_9.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_10.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_11.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_12.png" srcset="/img/loading.gif" alt></p></li><li><p>可以开始监控分析了！<br><img src="/jprofile-remote/jprofile_13.png" srcset="/img/loading.gif" alt></p></li><li>之后每次远程连接都可以直接到 <code>Open Session</code> 栏选择这次建好的 session 即可，不用再进行配置了。<br><img src="/jprofile-remote/jprofile_14.png" srcset="/img/loading.gif" alt></li></ol><h2 id="配置方案二：http-端口连接"><a href="#配置方案二：http-端口连接" class="headerlink" title="配置方案二：http 端口连接"></a>配置方案二：http 端口连接</h2><ol><li>从<a href="https://www.ej-technologies.com/download/jprofiler/files" target="_blank" rel="noopener">官方网站</a>上根据操作系统下载对应本地和服务器的 JProfile（需要确保两者版本一致）。注意服务端选择下载 <code>TAR.GZ Archive</code> 的压缩包。</li><li>客户端安装配置好本地的 JProfile， 然后将服务器的 JProfile 利用 scp 等工具传到服务器上去（也可在上述网页中直接复制链接在服务器上 wget 或者 curl -O 下载）。</li><li>服务端启动要监控的应用程序，如 IoTDB。</li><li>服务端执行<code>tar -zxvf jprofiler_linux_11_1_4.tar.gz</code>解压压缩包，然后执行 <code>jprofiler11.1.4/bin/jpenable -p 10000</code> 手动指定 10000 端口来启动 JProfile。（注，必须有运行的 JVM 才可以启动，并且程序一旦关闭重启服务端的 jprofile 也需要重新启动。）</li><li>客户端打开 JProfile，点击 <code>Profile a demo session or a saved session</code>，选择 <code>New Session</code> 栏的 <code>New Session</code> 手动创建一个远程连接。<br><img src="/jprofile-remote/jprofile_0.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_1.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_2.png" srcset="/img/loading.gif" alt></li><li>填好 <code>Session name</code>，<code>Attach type</code> 选择 <code>Attach to remote JVM</code>，连接方式选择 <code>Directly connection to</code>，配置好 host 并指定 <code>Profiling port</code> 为服务端手动指定的端口 10000。<br><img src="/jprofile-remote/jprofile_15.png" srcset="/img/loading.gif" alt></li><li>配置完后点击 ok 即可连接到远程的 JProfile，接着选择对应要监控的 Java 应用程序。<br><img src="/jprofile-remote/jprofile_7.png" srcset="/img/loading.gif" alt></li><li>配置监控模式，一般选择 <code>Async sampling</code>，该模式对性能影响最小。<br><img src="/jprofile-remote/jprofile_8.png" srcset="/img/loading.gif" alt></li><li>配置调用树的过滤器，该参数一般用来指示 jprofile 来记录 cpu view 时的函数耗时统计。加上要统计的包的前缀即可，比如 <code>org.apache.iotdb</code>。<br><img src="/jprofile-remote/jprofile_9.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_10.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_11.png" srcset="/img/loading.gif" alt><br><img src="/jprofile-remote/jprofile_12.png" srcset="/img/loading.gif" alt></li><li>可以开始监控分析了！<br><img src="/jprofile-remote/jprofile_13.png" srcset="/img/loading.gif" alt></li><li>之后每次远程连接都可以直接到 <code>Open Session</code> 栏选择这次建好的 session 即可，不用再进行配置了。<br><img src="/jprofile-remote/jprofile_14.png" srcset="/img/loading.gif" alt></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本博客傻瓜式介绍了使用 JProfile 来对远程服务器上运行的 Java 服务进行监控分析的两种方式。其中 ssh 免密连接方式更方便，其启动时不需要有启动的 JVM 且能够自动检测随后启动的 JVM 让客户端在连接时再挑选。 http 连接方式在监控的 JVM 关闭重启后服务端的 jprofile 也需要手动重启，较为麻烦。因此建议大家都使用第一种连接方式。</p>]]></content>
    
    
    
    <tags>
      
      <tag>开发工具配置</tag>
      
      <tag>IoTDB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BASE 定理介绍</title>
    <link href="/base-theory/"/>
    <url>/base-theory/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>CAP 理论表明，对于一个分布式系统而言，它是无法同时满足 Consistency（强一致性）、Availability（可用性）和 Partition tolerance（分区容忍性）这三个条件的，最多只能满足其中两个。</p><p>对于绝大多数互联网应用来说，由于网络环境是不可信的，所以分区容错性（P）必须满足。</p><p>如果只能在一致性和可用性之间做出选择的话，大部分情况下大家都会选择牺牲一部分一致性来保证可用性。因为如果不返回给用户数据，用户的体验会十分差，因此很多应用宁肯拒绝服务也不会能访问却没有数据。当然，在一些较为严格的场景比如支付场景下，强一致性是必须要满足的。</p><p>好了，我们只能放弃一致性，但是我们真这样做了，将一致性放弃了，现在这个系统返回的数据你敢信吗？没有一致性，系统中的数据也就从根本上变得不可信了，那这数据拿来有什么用，那这个系统也就没有任何价值，根本没用。</p><p>如上所述，由于我们三者都无法抛弃，但 CAP 定理限制了我们三者无法同时满足。在这种情况下，我们只能选择尽量靠近 CAP 定理，即尽量让 C、A、P 都满足。在此大势所趋下，eBay 的架构师 Dan Pritchett 源于对大规模分布式系统的实践总结，在 ACM 上发表文章提出 BASE 理论，其是基于 CAP 定理逐步演化而来的。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>BASE 理论是 Basically Available(基本可用)，Soft State（软状态）和 Eventually Consistent（最终一致性）三个短语的缩写。</p><p>其核心思想是：</p><blockquote><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。</p></blockquote><h3 id="Basically-Available"><a href="#Basically-Available" class="headerlink" title="Basically Available"></a>Basically Available</h3><p>基本可用是相对于正常的系统来说的，常见如下情况：</p><ul><li><p>响应时间上的损失：正常情况下的搜索引擎 0.5 秒即返回给用户结果，而基本可用的搜索引擎可以在 2 秒作用返回结果。</p></li><li><p>功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单。但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。</p></li></ul><h3 id="Soft-state"><a href="#Soft-state" class="headerlink" title="Soft state"></a>Soft state</h3><p>软状态是相对原子性来说的：</p><ul><li>原子性（硬状态）：要求多个节点的数据副本都是一致的，这是一种”硬状态”。</li><li>软状态（弱状态）：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延迟。</li></ul><h3 id="Eventually-Consistent"><a href="#Eventually-Consistent" class="headerlink" title="Eventually Consistent"></a>Eventually Consistent</h3><p>最终一致性是相对强一致性来说的：</p><ul><li>系统并不保证连续进程或者线程的访问都会返回最新的更新过的值。系统在数据写入成功之后，不承诺立即可以读到最新写入的值，也不会具体的承诺多久之后可以读到。但会尽可能保证在某个时间级别（比如秒级别）之后，可以让数据达到一致性状态。最终一致性是弱一致性的特定形式。</li><li>对于软状态，我们允许中间状态存在，但不可能一直是中间状态，必须要有个期限，系统保证在没有后续更新的前提下，在这个期限后，系统最终返回上一次更新操作的值，从而达到数据的最终一致性，这个容忍期限（不一致窗口的时间）取决于通信延迟，系统负载，数据复制方案设计，复制副本个数等。DNS 就是一个典型的最终一致性系统。</li></ul><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>讲完了定义，大家对于 BASE 到底是什么，到底有什么用可能还存在质疑，可以参照这篇<a href="https://zhuanlan.zhihu.com/p/341700125" target="_blank" rel="noopener">博客</a>。</p><h2 id="最终一致性的种类"><a href="#最终一致性的种类" class="headerlink" title="最终一致性的种类"></a>最终一致性的种类</h2><p>在实际工程实践中，最终一致性常被分为 5 种：</p><h3 id="因果一致性（Causal-consistency）"><a href="#因果一致性（Causal-consistency）" class="headerlink" title="因果一致性（Causal consistency）"></a>因果一致性（Causal consistency）</h3><p>如果节点 A 在更新完某个数据后通知了节点 B，那么节点 B 之后对该数据的访问和修改都是基于 A 更新后的值。与此同时，和节点 A 无因果关系的节点 C 的数据访问则没有这样的限制。</p><h3 id="读己之所写（Read-your-writes）"><a href="#读己之所写（Read-your-writes）" class="headerlink" title="读己之所写（Read your writes）"></a>读己之所写（Read your writes）</h3><p>节点 A 更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性。</p><h3 id="会话一致性（Session-consistency）"><a href="#会话一致性（Session-consistency）" class="headerlink" title="会话一致性（Session consistency）"></a>会话一致性（Session consistency）</h3><p>系统能保证在同一个有效的会话中实现 “读己之所写” 的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。</p><h3 id="单调读一致性（Monotonic-read-consistency）"><a href="#单调读一致性（Monotonic-read-consistency）" class="headerlink" title="单调读一致性（Monotonic read consistency）"></a>单调读一致性（Monotonic read consistency）</h3><p>如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。</p><h3 id="单调写一致性（Monotonic-write-consistency）"><a href="#单调写一致性（Monotonic-write-consistency）" class="headerlink" title="单调写一致性（Monotonic write consistency）"></a>单调写一致性（Monotonic write consistency）</h3><p>一个系统要能够保证来自同一个节点的写操作被顺序的执行。</p><p><img src="/base-theory/consistency_level.png" srcset="/img/loading.gif" alt></p><blockquote><p>在实际的实践中，这 5 种系统往往会结合使用，以构建一个具有最终一致性的分布式系统。</p></blockquote><p>事实上，最终一致性并不是只有那些大型分布式系统才设计的特性，许多现代的关系型数据库都采用了最终一致性模型。在现代关系型数据库中，大多都会采用同步和异步方式来实现主备数据复制技术。在同步方式中，数据的复制通常是更新事务的一部分，因此在事务完成后，主备数据库的数据就会达到一致。而在异步方式中，备库的更新往往存在延时，这取决于日志在主备数据库之间传输的时间长短，如果传输时间过长或者甚至在日志传输过程中出现异常导致无法及时将事务应用到备库上，那么很显然，从备库中读取的的数据将是旧的，因此就出现了不一致的情况。当然，无论是采用多次重试还是将数据修正，关系型数据库还是能够保证最终数据达到一致——这就是关系数据库提供最终一致性保证的经典案例。</p><p>生产系统的最终一致性示例可参考此<a href="https://zhuanlan.zhihu.com/p/344235098" target="_blank" rel="noopener">博客</a>。</p><h2 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>ACID，是指数据库管理系统在写入或更新数据的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：</p><ul><li>原子性（atomicity，或称不可分割性）</li><li>一致性（consistency）</li><li>隔离性（isolation，又称独立性）</li><li>持久性（durability）</li></ul><h4 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h4><p>一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。</p><h4 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h4><p>在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。</p><h4 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h4><p>数据库允许多个并发事务同时对齐数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</p><h4 id="持久性"><a href="#持久性" class="headerlink" title="持久性"></a>持久性</h4><p>事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</p><h3 id="与-BASE-的联系"><a href="#与-BASE-的联系" class="headerlink" title="与 BASE 的联系"></a>与 BASE 的联系</h3><p>可以参考 <a href="https://people.eecs.berkeley.edu/~brewer/cs262b-2004/PODC-keynote.pdf" target="_blank" rel="noopener">PODC</a>的介绍：</p><p><img src="/base-theory/podc.jpeg" srcset="/img/loading.gif" alt></p><ul><li>ACID 是传统数据库常用的设计理念, 追求强一致性模型。</li><li>BASE 支持的是大型分布式系统，提出通过牺牲强一致性获得高可用性。</li></ul><h3 id="与-CAP-的联系"><a href="#与-CAP-的联系" class="headerlink" title="与 CAP 的联系"></a>与 CAP 的联系</h3><p>　　CAP 理论的一致性是保证同样一个数据在所有不同服务器上的拷贝都是相同的，这是一种逻辑保证，而不是物理，因为光速限制，在不同服务器上这种复制是需要时间的，集群通过阻止客户端查看不同节点上还未同步的数据维持逻辑视图。</p><p>　　当跨分布式系统提供 ACID 时，这两个概念会混淆在一起，Google’s Spanner system 能够提供分布式系统的 ACID，其包含 ACID+CAP 的设计：</p><p><img src="/base-theory/spanner.png" srcset="/img/loading.gif" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体来说 BASE 理论面向的是大型高可用、可扩展的分布式系统。与传统 ACID 特性相反，不同于 ACID 的强一致性模型，BASE 理论基于 CAP 定理提出通过牺牲强一致性来获得可用性，并允许数据段时间内的不一致，但是最终达到一致状态。同时，在实际分布式场景中，不同业务对数据的一致性要求不一样。因此在设计中，ACID，BASE 和 CAP 理论往往又会结合使用。</p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Monarch 论文阅读</title>
    <link href="/monarch/"/>
    <url>/monarch/</url>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Monarch 是谷歌的一个全球分布的内存时间序列数据库系统，它被广泛用在监控谷歌上十亿用户规模的应用程序和系统的可用性、正确性、性能、负载和其他方面。</p><p>Monarch 自 2010 年开始持续运行，收集、组织、存储、查询大量全球范围内快速增长的时间序列数据。它目前在内存中存储近 PB 的压缩时间序列数据，每秒摄入 TB 的数据，每秒处理数百万次查询。</p><p>本文介绍了系统的结构，以及在区域分布式架构下实现可靠、灵活的统一系统的新机制。我们也分享了十年来在谷歌中开发和运行 Monarch 作为服务的经验教训。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>谷歌有大量的计算机系统监控需求。数以千计的团队正在运营面向全球用户的服务(如 YouTube、GMail 和 Google Maps)，或者为这些服务(如 Spanner、Borg 和 F1)提供硬件和软件基础设施。这些团队需要监视不断增长和变化的异构实体(例如设备、虚拟机和容器)集合，这些实体数量达数十亿，分布在全球各地。</p><p>谷歌必须从这些实体中收集度量，以时间序列的形式存储，并进行查询，以支持以下用例:</p><ol><li>当被监视的服务没有正确执行时检测和警报;</li><li>显示显示服务状态和运行状况的图形仪表板;</li><li>针对问题的不可知性执行特别查询，探索性能和资源使用情况。</li></ol><p>Borgmon 是谷歌内部的初代监控系统。随着大数据时代的到来，Borgmon 的部署规模逐渐扩大，暴露了一些弊端:</p><ol><li>Borgmon 常被鼓励使用成一种分散的架构，即每个团队都建立并管理自己的 Borgmon 实例，这导致一些人力资源的浪费。此外，用户经常需要跨应用程序和基础设施边界检查和关联监控数据以解决问题，而跨多个 Borgmon 实例来操作是很难或不可能实现的;</li><li>Borgmon 缺乏度量符号和度量值的图表化，导致了查询的语义歧义，限制了查询语言在数据分析过程中的表达能力;</li><li>Borgmon 不支持分布(即柱状图)值类型，这是一种强大的数据结构，能够进行复杂的统计分析(例如，计算跨多个服务器的请求延迟的99%);</li><li>Borgmon 要求用户手动跨多个实例对大量被监视的全局服务实体进行切分，并建立查询评估树。</li></ol><p>考虑到这些经验教训，Monarch 成为了谷歌的下一代大规模监控系统。它的设计是为了适应持续增长的流量，并支持不断扩展的用例集。它为所有团队提供单一的统一服务，从而最大限度地减少了操作的工作量。它有一个简化的数据模型，便于复杂的查询和分布式类型时间序列的全面支持。</p><h2 id="系统概述"><a href="#系统概述" class="headerlink" title="系统概述"></a>系统概述</h2><p>Monarch 的设计理念：</p><ul><li>Monarch 技术上被设计成了一个 AP 系统而不是 CP 系统。他们认为在监控报警的场景下可用性比一致性重要得多，因为这能够显著增加检测到异常情况和减轻异常影响的平均时间。</li><li>Monarch 的关键报警路径上不能产生循环依赖，即不能使用谷歌内部的 Bigtable, Colossus (the successor to GFS), Spanner, Blobstore, and F1 等存储系统再去监控这些存储系统的集群（非关键路径可以适当使用）。</li><li>结合全局管理和查询对区域 zone 进行本地监控。在降低延迟，减少可靠性问题的同时能够提供全球视角。</li><li>为了可靠性，Monarch 的全局组件在地理上进行复制，并使用最接近的副本与地域性组件交互以利用局部性，其区域 zone 中的组件将跨集群复制。</li></ul><p><img src="/monarch/system_overview.png" srcset="/img/loading.gif" alt></p><p>从功能上讲，为了可靠性，Monarch 组件可以分为三类：持有状态组件、数据存储组件和查询执行组件。</p><p>持有状态组件：</p><ul><li>叶子（Leaves）将监视数据存储在内存中的时间序列存储中。</li><li>恢复日志（Recover Logs）将与叶子相同的监视数据存储在磁盘上，这些数据最终会被重写到一个长期时间序列存储库中。</li><li>全局配置服务器及其分区镜将配置数据保存在 Spanner 数据库中。</li></ul><p>数据存储组件：</p><ul><li>摄取路由器（Ingestion Routers）使用时间序列键中的信息来路由数据到适当的 Monarch 区域中的叶路由器。</li><li>叶路由器（Leaf Routers）接受数据存储在一个区域 zone，并路由到叶子存储。</li><li>范围分配管理器（Range Assigner）将数据分配到叶子，以平衡在一个区域的叶子之间的负载。</li></ul><p>查询执行组件：</p><ul><li>混合器（Mixers）将查询划分为多个子查询，将其路由到叶子去执行并合并子查询结果。查询可以在根级别(由根混合器)或在区域 zone 级别(由区域混合器)发出。根级查询同时涉及根和区域 zone 混合器。</li><li>索引服务器（Index Server）为每个区域 zone 和叶节点索引数据，并指导分布式查询执行。</li><li>评估器（Evaluator）定期向混合器发出长期查询并将结果写回叶子。</li></ul><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p><img src="/monarch/data_model.png" srcset="/img/loading.gif" alt><br>从概念上讲，Monarch 将监控数据存储为结构化表中的时间序列。每个表由多个键列组成时间序列键和一个值列组成时间序列点的历史，如图 2 所示。键列也称为字段，有两个源：目标（Target）和指标（Metric）。</p><h3 id="Target"><a href="#Target" class="headerlink" title="Target"></a>Target</h3><p>Monarch 使用 Target 将每个时间序列与它的源实体(或监视实体)相关联。例如，源实体就是生成时间序列的进程或 VM。每个 Target 表示一个受监视的实体，并符合一个 Target Schema，该模式定义了一组有序的 Target 字段名和相关字段类型。上图显示了一个名为 ComputeTask 的流行目标模式：每个 ComputeTask 目标标识 Borg 集群中正在运行的任务，具有四个字段：user、job、clsuter 和 task_num。</p><p>Monarch 将数据存储在离生成数据很近的位置。每个 Target Schema 都有一个标注为 location 的字段；这个位置字段的值决定了时间序列被路由和存储到的特定 Moranch zone。</p><p>在每个 zone 内，Monarch 将同一目标的时间序列存储在同一叶子中，因为它们来自同一实体，并且更有可能在一个 join 中被一起查询。Monarch 还将目标以<code>[S_start,S_end]</code>的形式分组为不同的目标范围，其中 S_start 和 S_end 是开始和结束目标字符串。目标字符串通过有序连接 Target Schema 名称来表示。</p><p>Target 范围用于字典分片和叶节点间的负载均衡；这允许在查询中更有效地跨邻近目标聚合。</p><h3 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h3><p>Metric 测量被监视目标的一个方面，如任务所服务的 RPC 数量、VM 的内存使用情况等。与 Target 类似，Metric 也有 Metric Schema，该模式定义时间序列值类型和一组指标字段。指标的命名类似于文件。图 2 显示了一个称为 /rpc/server/latency 的示例度量，它测量 RPC 到服务器的延迟；它有两个度量字段，通过 service 和 command 来区分 RPC。</p><p><img src="/monarch/metric.png" srcset="/img/loading.gif" alt><br>值类型可以是 bool、int64、double、string、分布类型或其他类型的元组。除了分布类型之外，它们都是标准类型，分布类型是一种紧凑类型，表示大量双精度值。分布包括一个直方图，该直方图将一组 double 值划分到称为 bucket 的子集中，并使用总体统计信息(如平均值、计数和标准差)汇总每个 bucket 中的值。桶边界是可配置的，可以在数据粒度(即精度)和存储成本之间进行权衡：用户可以为更流行的值范围指定更细的桶。图 3 显示了一个测试组分布类型的 /rpc/server/latency 时间序列，它测量服务器在处理 RPC 时的延迟；它有一个固定的桶大小为 10ms。</p><p>Exemplars：分布中的每个桶可以包含该桶中的值的一个范例。这会便于用户轻易的从直方图中找到慢 RPC 的详细信息。</p><p>Metric types：可以是度量类型或累积类型。累积 Metric 对于支持由许多服务器组成的分布式系统非常重要，这些服务器可能由于作业调度而定期重新启动，在重新启动期间可能会丢失一些点。</p><h2 id="可扩展收集"><a href="#可扩展收集" class="headerlink" title="可扩展收集"></a>可扩展收集</h2><p>为了实时摄取大量的时间序列数据，Monarch 采用了两种分治策略和一种关键的优化，在收集过程中对数据进行聚合。</p><h3 id="数据收集概述"><a href="#数据收集概述" class="headerlink" title="数据收集概述"></a>数据收集概述</h3><p>摄取路由器根据位置字段将时间序列数据划分给 zone，叶路由器根据范围分配管理器将数据分布在叶子之间。</p><p>一些细节：</p><ul><li>客户端会按照特定频率将数据发送到离其最近的摄取路由器（全球分布）。</li><li>location-to-zone 的映射配置在摄取路由器中且可以动态更新。</li><li>每个叶路由器维护一个持续更新的 range map，该映射将每个目标范围映射到三个叶子副本。注意叶路由器从叶子而不是从范围分配管理器获得范围映射的更新。</li><li>最小化内存碎片和分配混乱。为了在 CPU 和内存之间实现平衡，内存存储只执行少量压缩，比如时间戳共享和增量编码。时间戳共享非常有效:一个时间戳序列平均被大约 10 个时间序列共享。</li><li>尽最大努力异步向分布式文件系统写恢复日志但不需要确认来隔离分布式文件系统的故障。 </li><li>叶子收集的数据还会触发用于约束读放大的 zone 和 root 索引服务器中的更新。</li></ul><h3 id="zone-内部的负载均衡"><a href="#zone-内部的负载均衡" class="headerlink" title="zone 内部的负载均衡"></a>zone 内部的负载均衡</h3><p>一个表模式由一个 target 模式和 schema 模式组成。target 模式的编码是 Monarch 用来分片的依据，这减少了写放大，即一次 RPC 可以携带一个 target 和几百个 metric，且这些数据最终最多被送到 3 个叶副本（不是有个 range map 吗，如果刚好在边界呢？）。这样的分配有利于查询操作的下推。</p><p>一些细节：</p><ul><li>副本数（1-3）可自由配置。</li><li>通常，Monarch zone 包含多个故障域(集群)中的叶子；分配者会将范围的副本分配到不同的故障域。</li><li>范围分配管理器会根据 CPU 负载，内存使用量等变量来对不同的 Range 进行分裂或合并。</li><li>迁移时新节点利用恢复日志来重新构建数据，此外迁移过程中是新旧双写的，这样就不影响上层应用的可用性。</li></ul><h3 id="收集聚合"><a href="#收集聚合" class="headerlink" title="收集聚合"></a>收集聚合</h3><p>在有些监控场景下，用户只想知道整个人或某个集群的某个监控信息一段时间内的统计值而不是每个值。<br><img src="/monarch/collection_aggregation.png" srcset="/img/loading.gif" alt></p><p>Monarch 利用分组连续聚合的方式收集数据并抛弃过时的数据，还用了 TrueTime 大杀器来确保时间分组的正确，实现了实时收集聚合。</p><h2 id="可扩展查询"><a href="#可扩展查询" class="headerlink" title="可扩展查询"></a>可扩展查询</h2><p>为了查询时间序列数据，Monarch 提供了一种由分布式引擎支持的表达语言，该分布式引擎使用静态不变量和新的索引来本地化查询执行。</p><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p><img src="/monarch/query_language.png" srcset="/img/loading.gif" alt><br>听他吹的很牛逼就完事了。类似于 SQL，表达能力比较强吧。</p><h3 id="读流程概述"><a href="#读流程概述" class="headerlink" title="读流程概述"></a>读流程概述</h3><p>系统中有两种查询:临时查询和长期查询。前者是系统外的用户偶尔执行的，后者是 Monarch 周期性执行的物化视图，还会被重新写入到 Monarch 中来提高查询效率和报警。</p><p>查询时也会分三层，即先从根混合器分配到 zone 混合器，然后再被分配到叶子中去。当然为了减少读放大，混合器都会向同一级别的索引服务器请教来剪枝（类似于布隆过滤器）。</p><p>Monarch 叶子的不同副本之间不是完全实时一致（最终一致），其会有一个质量参数，查询时混合器会对该时间序列不同 range 的分片挑选其质量最高的副本来读数据。</p><p>Monarch 做了租户隔离，会跟踪每个用户在集群中查询时使用的总内存，并在越界时取消查询，同时也为每个用户分配了公平的 CPU 时间。</p><h3 id="查询下推"><a href="#查询下推" class="headerlink" title="查询下推"></a>查询下推</h3><p>查询下推增加了可评估的查询规模，并减少了查询延迟，原因如下：</p><ul><li>更低级别的评估越多，意味着更稳定和均匀分布的负载。</li><li>在较低级别计算的全部或部分聚合大大减少传输到较高级别节点的数据量。</li></ul><h3 id="提示字段索引"><a href="#提示字段索引" class="headerlink" title="提示字段索引"></a>提示字段索引</h3><p>为了实现高可伸缩性，Monarch 使用存储在索引服务器中的字段提示索引(FHI)来限制从父节点向子节点发送查询时放大，方法是跳过不相关的子节点(那些没有向特定查询输入数据的子节点)。一个 FHI 是一个简明的，持续更新的索引时间序列字段值。FHIs 通过分析查询中的字段谓词跳过不相关的子字段，并有效地处理正则表达式谓词，而无需遍历精确的字段值。FHI 工作的区域有数万亿个时间序列键和超过 10000 个叶子，同时保持足够小的大小以存储在内存中。</p><h3 id="可靠查询"><a href="#可靠查询" class="headerlink" title="可靠查询"></a>可靠查询</h3><ul><li>Monarch 的查询能够在文件系统或全局组件失效时继续工作。</li><li>Monarch 时间序列的不同分片之间存在重叠。即适度的冗余存储有助于读性能的提升。</li><li>Monarch 查询时还会有一些回退叶子来防止某些慢叶子对查询的影响。即 zone 混合器会在主叶和回退叶之间并行地继续进行，并从两个中较快的叶中提取和删除响应。</li></ul><h2 id="配置管理"><a href="#配置管理" class="headerlink" title="配置管理"></a>配置管理</h2><p>由于 Monarch 作为分布式的、多租户的服务运行的特性，需要一个集中的配置管理系统来为用户提供方便的、细粒度的控制，以便在整个系统中对其监视和分布配置进行控制。用户与影响所有 Monarch zone 的单一全局配置视图交互。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p><img src="/monarch/evalution.png" srcset="/img/loading.gif" alt></p><p>数据量很大，增速很快，节点很多，我很牛逼…</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>目前有许多开源时间序列数据库：Graphite，InfluxDB， OpenTSDB， Prometheus，和 tsdb 是最受欢迎的。它们将数据存储在辅助存储上(本地或分布式存储，如HBase)；辅助存储的使用使它们在关键监视时的可使用性降低。它们通过类似于 Monarch zone 的水平扩展来支持分布式部署，但是它们缺乏 Monarch 所提供的全局配置管理和查询聚合。</p><h2 id="教训"><a href="#教训" class="headerlink" title="教训"></a>教训</h2><ul><li>时间序列键的字典序分片改进了摄取和查询的可伸缩性，使 Monarch zone 能够扩展到数以万计的叶子。</li><li>基于推的数据收集模式提高了系统的健壮性，同时简化了系统架构。</li><li>系统化的数据模型提高了健壮性和性能。</li><li>系统扩展是一个连续的过程。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Monarch 可以在很大规模下高效可靠地运行，这是由于它将自治 zone 监控子系统通过全局配置和查询平面整合成一个连贯的整体。它采用了一种新颖的、类型丰富的关系时间序列数据模型，允许高效和可伸缩的数据存储，同时为数据分析提供了一种表达性查询语言。为了适应这种大规模，Monarch 在数据收集和查询执行方面使用了各种优化技术。对于数据收集，Monarch 服务器执行区域内负载平衡和收集聚合，以提高可靠性和效率。对于查询执行，Monarch 以分布式、分层的方式执行每个查询，执行积极的过滤和聚合下推以提高性能和吞吐量，并利用紧凑而强大的分布式索引进行高效的数据修剪。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="http://www.vldb.org/pvldb/vol13/p3181-adams.pdf" target="_blank" rel="noopener">论文</a></p><p><a href="https://www.datacouncil.ai/talks/monarch-googles-planet-scale-streaming-monitoring-infrastructure" target="_blank" rel="noopener">PPT</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式存储</tag>
      
      <tag>Google</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 博士论文翻译</title>
    <link href="/raft-thesis-translate/"/>
    <url>/raft-thesis-translate/</url>
    
    <content type="html"><![CDATA[<p>参考我的 <a href="https://github.com/LebronAl/raft-thesis-zh_cn" target="_blank" rel="noopener">Github Repo</a>。</p><p>由于作者能力有限，描述必然会有纰漏，欢迎提交 PR、创建 Issue 进一步交流。</p><p>如果看完之后有所收获，求求给个 Star 以表支持。😊</p>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>共识算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Raft 算法介绍</title>
    <link href="/raft/"/>
    <url>/raft/</url>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="共识算法"><a href="#共识算法" class="headerlink" title="共识算法"></a>共识算法</h3><p>共识算法允许一组节点像一个整体一样一起工作，即使其中一些节点出现故障也能够继续工作下去，其正确性主要源于复制状态机的性质：</p><blockquote><p>任何初始状态一样的状态机，如果执行的命令序列一样，则最终达到的状态也一样。如果将此特性应用在多参与者进行协商共识上，可以理解为系统中存在多个具有完全相同的状态机（参与者），这些状态机能最终保持一致的关键就是起始状态完全一致和执行命令序列完全一致。</p></blockquote><p>共识算法常被用来确保每一个节点上的状态机一定都会按相同的顺序执行相同的命令， 并且最终会处于相同的状态。换句话说，可以理解为共识算法就是用来确保每个节点上的日志顺序都是一致的。（不过需要注意的是，只确保“提交给状态机的日志”顺序是一致的，而有些日志项可能只是暂时添加，尚未决定要提交给状态机）。正因为如此，共识算法在构建可容错的大规模分布式系统中扮演着重要的角色。</p><p><img src="/raft/rsm.png" srcset="/img/loading.gif" alt></p><p>上图就是每个节点的状态机，日志模块，共识模块与客户端交互的过程。</p><p>当然，实际使用系统中的共识算法一般满足以下特性：</p><ul><li>在非拜占庭条件下保证共识的一致性。（非拜占庭条件，指的就是每一个节点都是诚实可信的，每一次信息的传递都是真实的且符合协议要求的，当节点无法满足协议所要求的条件时，就停止服务，节点仅会因为网络延迟或崩溃出现不一致，而不会有节点传递错误的数据或故意捏造假数据。）</li><li>在多数节点存活时，保持可用性。（“多数”永远指的是配置文件中所有节点的多数，而不是存活节点的多数。）</li><li>不依赖于时间，错误的时钟和高延迟只会导致可用性问题，而不会导致一致性问题。</li><li>在多数节点一致后就返回结果，而不会受到个别慢节点的影响。</li></ul><h3 id="Raft-的由来与宗旨"><a href="#Raft-的由来与宗旨" class="headerlink" title="Raft 的由来与宗旨"></a>Raft 的由来与宗旨</h3><p>众所周知，Paxos 是一个非常划时代的共识算法。在 Raft 出现之前的 10 年里，Paxos 几乎统治着共识算法这一领域：因为绝大多数共识算法的实现都是基于 Paxos 或者受其影响，同时 Paxos 也成为了教学领域里讲解共识问题时的示例。</p><p>但是不幸的是，尽管有很多工作都在尝试降低 Paxos 的复杂性，但是它依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。比如 <code>Google Chubby</code> 的论文就提到，因为 Paxos 的描述和现实差距太大，所以最终人们总会实现一套未经证实的类 Paxos 协议。</p><p>基于以上背景，<code>Diego Ongaro</code> 在就读博士期间，深入研究 Paxos 协议后提出了 Raft 协议，旨在提供更为易于理解的共识算法。Raft 的宗旨在于可实践性和可理解性，并且相比 Paxos 几乎没有牺牲多少性能。</p><blockquote><p>趣闻：<a href="https://groups.google.com/forum/#!topic/raft-dev/95rZqptGpmU" target="_blank" rel="noopener">Raft 名字的来源</a>。简而言之，其名字即来自于 <code>R{eliable|plicated|dundant} And Fault-Tolerant</code>， 也来自于这是一艘可以帮助你逃离 Paxos 小岛的救生筏（Raft）。</p></blockquote><h3 id="工业界的实现"><a href="#工业界的实现" class="headerlink" title="工业界的实现"></a>工业界的实现</h3><ul><li><code>tikv</code></li><li><code>consul</code></li><li><code>etcd</code></li><li><code>sofajraft</code></li><li>…</li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>这一部分会简单介绍 Raft 的一些基本概念。若暂时没看懂并没有关系，后面会一一介绍清楚，带着问题耐心读完此博客即可。</p><h3 id="子问题"><a href="#子问题" class="headerlink" title="子问题"></a>子问题</h3><p>Raft 将共识算法这个难解决的问题分解成了多个易解决，相对独立的子问题，这些问题都会在接下来的章节中进行介绍。</p><ul><li><code>Leader election</code>：选出集群的 leader 来统筹全局。</li><li><code>Log replication</code>：leader 负责从客户端接收请求，并且在集群中扩散同步。</li><li><code>Safety</code>：各节点间状态机的一致性保证。</li></ul><p>在博士论文和实际生产系统中，还有更多可以探讨的模块或细节功能：</p><ul><li><code>Log compaction</code>：压缩日志以节约磁盘空间；加速重启后节点恢复速率；加速新节点 catch up 速率。 </li><li><code>Leader transfer</code>：能够将 leader 禅让另一个 follower，便于平滑的负载均衡。</li><li><code>Pre vote</code>：在竞选开始时先进行一轮预备竞选，若被允许再转变为 candidate，这样有助于防止某些异常节点扰乱整个集群的正常工作。</li><li><code>Membership change</code>：集群动态增删节点。</li><li><code>Client interaction</code>：客户端交互。</li><li><code>Linearizable read</code>：线性一致性读。</li><li><code>Optimization</code>：业界常见优化。</li><li>…</li></ul><h3 id="节点类型"><a href="#节点类型" class="headerlink" title="节点类型"></a>节点类型</h3><p>Raft 将所有节点分为三个身份：</p><ul><li><code>Leader</code>：集群内最多只会有一个 leader，负责发起心跳，响应客户端，创建日志，同步日志。</li><li><code>Candidate</code>：leader 选举过程中的临时角色，由 follower 转化而来，发起投票参与竞选。</li><li><code>Follower</code>：接受 leader 的心跳和日志同步数据，投票给 candidate。</li></ul><p><img src="/raft/state.png" srcset="/img/loading.gif" alt></p><p>上图可以看出 Raft 中节点状态之间变迁的条件。</p><p>在博士论文和实际生产系统中，其实又增加了两种身份：</p><ul><li><code>Learner</code>：不具有选举权，参与日志复制过程但不计数的节点。可以作为新节点加入集群时的过渡状态以提升可用性，也可以作为一种类似于 binlog 的对 leader 日志流进行订阅的角色，比如可以参考 PingCAP 公司 tikv 和 tiflash 的架构。</li><li><code>Pre candidate</code>：刚刚发起竞选，还在等待 <code>Pre-Vote</code> 结果的临时状态， 取决于 <code>Pre-Vote</code> 的结果，可能进化为 candidate，可能退化为 follower。</li></ul><h3 id="节点状态"><a href="#节点状态" class="headerlink" title="节点状态"></a>节点状态</h3><p>每一个节点都应该有的持久化状态：</p><ul><li><code>currentTerm</code>：当前任期，保证重启后任期不丢失。</li><li><code>votedFor</code>：在当前 term，给哪个节点投了票，值为 null 或 <code>candidate id</code>。即使节点重启，Raft 算法也能保证每个任期最多只有一个 leader。</li><li><code>log[]</code>：已经 committed 的日志，保证状态机可恢复。</li></ul><p>每一个节点都应该有的非持久化状态：</p><ul><li><code>commitindex</code>：已提交的最大 index。leader 节点重启后可以通过 appendEntries rpc 逐渐得到不同节点的 matchIndex，从而确认 commitIndex，follower 只需等待 leader 传递过来的 commitIndex 即可。</li><li><code>lastApplied</code>：已被状态机应用的最大 index。raft 算法假设了状态机本身是易失的，所以重启后状态机的状态可以通过 log[] (部分 log 可以压缩为 snapshot)来恢复。</li></ul><p>leader 的非持久化状态：</p><ul><li><code>nextindex[]</code>：为每一个 follower 保存的，应该发送的下一份 <code>entry index</code>；初始化为本地 last index + 1。</li><li><code>matchindex[]</code>：已确认的，已经同步到每一个 follower 的 <code>entry index</code>。初始化为 0，根据复制状态不断递增，<br>（注：每次选举后，leader 的此两个数组都应该立刻重新初始化并开始探测）</li></ul><h3 id="任期"><a href="#任期" class="headerlink" title="任期"></a>任期</h3><p><img src="/raft/term.png" srcset="/img/loading.gif" alt></p><p>Raft 将时间划分成为任意不同长度的 term。term 用连续的数字进行表示。每一个 term 的开始都是一次选举，一个或多个 candidate 会试图成为 leader。如果一个  candidate 赢得了选举，它就会在该 term 担任 leader。在某些情况下，选票会被均分，即 <code>split vote</code>（例如总数为偶数节点时两个 candidate 节点各获得了两票），此时无法选出该 term 的 leader，那么在该 term 的选举超时后将会开始另一个 term 的选举。</p><p>不同的服务器节点可能多次观察到 term 之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个 term 全程。term 在 Raft 算法中充当逻辑时钟（类似于 Lamport timestamp）的作用，这会允许服务器节点查明一些过期的信息比如过期的 leader。</p><p>每个节点都会存储当前 term 号，这一编号在整个时间内单调增长。当服务器之间通信的时候会交换当前 term 号；如果一个服务器的当前 term 号比其他人小，那么他会更新自己的 term 到较大的 term 值。如果一个 candidate 或者 leader 发现自己的 term 过期了，那么他会立即退回 follower。如果一个节点接收到一个包含过期 term 号的请求，那么它会拒绝或忽略这个请求。这实际上就是一个 Lamport 逻辑时钟的具体实现。</p><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><ul><li><p><code>entry</code>：Raft 中，将每一个事件都称为一个 entry，每一个 entry 都有一个表明它在 log 中位置的 index（之所以从 1 开始是为了方便 <code>prevLogIndex</code> 从 0 开始）。只有 leader 可以创建 entry。entry 的内容为 <code>&lt;term, index, cmd&gt;</code>，其中 cmd 是可以应用到状态机的操作。在 raft 组大部分节点都接收这条 entry 后，entry 可以被称为是 committed 的。</p></li><li><p><code>log</code>：由 entry 构成的数组，只有 leader 可以改变其他节点的 log。 entry 总是先被 leader 添加进本地的 log 数组中去，然后才发起共识请求，获得 quorum 同意后才会被 leader 提交给状态机。follower 只能从 leader 获取新日志和当前的 commitIndex，然后应用对应的 entry 到自己的状态机。</p></li></ul><h3 id="保证"><a href="#保证" class="headerlink" title="保证"></a>保证</h3><ul><li><code>Election Safety</code>：每个 term 最多只会有一个 leader；集群同时最多只会有一个可以读写的 leader。</li><li><code>Leader Append-Only</code>：leader 的日志是只增的。</li><li><code>Log Matching</code>：如果两个节点的日志中有两个 entry 有相同的 index 和 term，那么它们就是相同的 entry。</li><li><code>Leader Completeness</code>：一旦一个操作被提交了，那么在之后的 term 中，该操作都会存在于日志中。</li><li><code>State Machine Safety</code>：一致性，一旦一个节点应用了某个 index 的 entry 到状态机，那么其他所有节点应用的该 index 的操作都是一致的。</li></ul><h2 id="领导人选举"><a href="#领导人选举" class="headerlink" title="领导人选举"></a>领导人选举</h2><p>Raft 使用心跳来维持 leader 身份。任何节点都以 follower 的身份启动。 leader 会定期的发送心跳给所有的 follower 以确保自己的身份。 每当 follower 收到心跳后，就刷新自己的 electionElapsed，重新计时。</p><p>（后文中，会将预设的选举超时称为 electionTimeout，而将当前经过的选举耗时称为 electionElapsed）</p><p>一旦一个 follower 在指定的时间内没有收到任何 RPC（称为 electionTimeout），则会发起一次选举。 当 follower 试图发起选举后，其身份转变为 candidate，在增加自己的 term 后， 会向所有节点发起 RequestVoteRPC 请求，candidate 的状态会一直持续直到：</p><ul><li>赢得选举</li><li>其他节点赢得选举</li><li>一轮选举结束，无人胜出</li></ul><p>选举的方式非常简单，谁能获取到多数选票 <code>(N/2 + 1)</code>，谁就成为 leader。 在一个 candidate 节点等待投票响应的时候，它有可能会收到其他节点声明自己是 leader 的心跳， 此时有两种情况：</p><ul><li>该请求的 term 和自己一样或更大：说明对方已经成为 leader，自己立刻退为 follower。</li><li>该请求的 term 小于自己：拒绝请求并返回当前 term 以让请求节点更新 term。</li></ul><p>为了防止在同一时间有太多的 follower 转变为 candidate 导致无法选出绝对多数， Raft 采用了随机选举超时（<code>randomized election timeouts</code>）的机制， 每一个 candidate 在发起选举后，都会随机化一个新的选举超时时间， 一旦超时后仍然没有完成选举，则增加自己的 term，然后发起新一轮选举。 在这种情况下，应该能在较短的时间内确认出 leader。 （因为 term 较大的有更大的概率压倒其他节点）</p><p>etcd 中将随机选举超时设置为 <code>[electiontimeout, 2 * electiontimeout - 1]</code>。</p><p>通过一个节点在一个 term 只能给一个节点投票，Raft 保证了对于给定的一个 term 最多只有一个 leader，从而避免了选举导致的 <code>split brain</code> 以确保 safety；通过不同节点每次随机化选举超时时间，Raft 在实践中（注意：并没有在理论上）避免了活锁以确保 liveness。</p><p>以下是 6.824 lab2 中选举相关逻辑的具体实现，以供参考。</p><pre><code class="hljs GO"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span> <span class="hljs-title">RequestVote</span><span class="hljs-params">(request *RequestVoteRequest, response *RequestVoteResponse)</span></span> &#123;rf.mu.Lock()<span class="hljs-keyword">defer</span> rf.mu.Unlock()<span class="hljs-keyword">defer</span> rf.persist()<span class="hljs-keyword">defer</span> DPrintf(<span class="hljs-string">"&#123;Node %v&#125;'s state is &#123;state %v,term %v,commitIndex %v,lastApplied %v,firstLog %v,lastLog %v&#125; before processing requestVoteRequest %v and reply requestVoteResponse %v"</span>, rf.me, rf.state, rf.currentTerm, rf.commitIndex, rf.lastApplied, rf.getFirstLog(), rf.getLastLog(), request, response)<span class="hljs-keyword">if</span> request.Term &lt; rf.currentTerm || (request.Term == rf.currentTerm &amp;&amp; rf.votedFor != <span class="hljs-number">-1</span> &amp;&amp; rf.votedFor != request.CandidateId) &#123;response.Term = rf.currentTermresponse.VoteGranted = <span class="hljs-literal">false</span><span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> request.Term &gt; rf.currentTerm &#123;rf.ChangeState(StateFollower)rf.currentTerm = request.Termrf.votedFor = <span class="hljs-number">-1</span>&#125;<span class="hljs-keyword">if</span> !rf.isLogUpToDate(request.LastLogTerm, request.LastLogIndex) &#123;response.Term = rf.currentTermresponse.VoteGranted = <span class="hljs-literal">false</span><span class="hljs-keyword">return</span>&#125;rf.votedFor = request.CandidateIdrf.electionTimer.Reset(RandomizedElectionTimeout())response.Term = rf.currentTermresponse.VoteGranted = <span class="hljs-literal">true</span>&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span> <span class="hljs-title">StartElection</span><span class="hljs-params">()</span></span> &#123;request := rf.genRequestVoteRequest()DPrintf(<span class="hljs-string">"&#123;Node %v&#125; starts election with RequestVoteRequest %v"</span>, rf.me, request)<span class="hljs-comment">// use Closure</span>grantedVotes := <span class="hljs-number">1</span>rf.votedFor = rf.me<span class="hljs-keyword">for</span> peer := <span class="hljs-keyword">range</span> rf.peers &#123;<span class="hljs-keyword">if</span> peer == rf.me &#123;<span class="hljs-keyword">continue</span>&#125;<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(peer <span class="hljs-keyword">int</span>)</span></span> &#123;response := <span class="hljs-built_in">new</span>(RequestVoteResponse)<span class="hljs-keyword">if</span> rf.sendRequestVote(peer, request, response) &#123;rf.mu.Lock()<span class="hljs-keyword">defer</span> rf.mu.Unlock()DPrintf(<span class="hljs-string">"&#123;Node %v&#125; receives RequestVoteResponse %v from &#123;Node %v&#125; after sending RequestVoteRequest %v in term %v"</span>, rf.me, response, peer, request, rf.currentTerm)<span class="hljs-keyword">if</span> rf.currentTerm == request.Term &amp;&amp; rf.state == StateCandidate &#123;<span class="hljs-keyword">if</span> response.VoteGranted &#123;grantedVotes += <span class="hljs-number">1</span><span class="hljs-keyword">if</span> grantedVotes &gt; <span class="hljs-built_in">len</span>(rf.peers)/<span class="hljs-number">2</span> &#123;DPrintf(<span class="hljs-string">"&#123;Node %v&#125; receives majority votes in term %v"</span>, rf.me, rf.currentTerm)rf.ChangeState(StateLeader)rf.BroadcastHeartbeat(<span class="hljs-literal">true</span>)&#125;&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> response.Term &gt; rf.currentTerm &#123;DPrintf(<span class="hljs-string">"&#123;Node %v&#125; finds a new leader &#123;Node %v&#125; with term %v and steps down in term %v"</span>, rf.me, peer, response.Term, rf.currentTerm)rf.ChangeState(StateFollower)rf.currentTerm = response.Termrf.votedFor = <span class="hljs-number">-1</span>&#125;&#125;&#125;&#125;(peer)&#125;&#125;</code></pre><h2 id="日志同步"><a href="#日志同步" class="headerlink" title="日志同步"></a>日志同步</h2><p>leader 被选举后，则负责所有的客户端请求。每一个客户端请求都包含一个命令，该命令可以被作用到 RSM。</p><p>leader 收到客户端请求后，会生成一个 entry，包含 <code>&lt;index, term, cmd&gt;</code>，再将这个 entry 添加到自己的日志末尾后，向所有的节点广播该 entry。</p><p>follower 如果同意接受该 entry，则在将 entry 添加到自己的日志后，返回同意。</p><p>如果 leader 收到了多数的成功答复，则将该 entry 应用到自己的 RSM， 之后可以称该 entry 是 committed 的。该 committed 信息会随着随后的 AppendEntries 或 Heartbeat RPC 被传达到其他节点。</p><p><img src="/raft/log.png" srcset="/img/loading.gif" alt></p><p>Raft 保证下列两个性质：</p><ul><li>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd；</li><li>如果在两个日志（节点）里，有两个 entry 拥有相同的 index 和 term，那么它们前面的 entry 也一定相同。</li></ul><p>通过”仅有 leader 可以生成 entry”来确保第一个性质， 第二个性质则通过一致性检查（consistency check）来保证，该检查包含几个步骤：</p><p>leader 在通过 AppendEntriesRPC 和 follower 通讯时，会带上上一块 entry 的信息， 而 follower 在收到后会对比自己的日志，如果发现这个 entry 的信息（index、term）和自己日志内的不符合，则会拒绝该请求。一旦 leader 发现有 follower 拒绝了请求，则会与该 follower 再进行一轮一致性检查， 找到双方最大的共识点，然后用 leader 的 entries 记录覆盖 follower 所有在最大共识点之后的数据。</p><p>寻找共识点时，leader 还是通过 AppendEntriesRPC 和 follower 进行一致性检查， 方法是发送再上一块的 entry， 如果 follower 依然拒绝，则 leader 再尝试发送更前面的一块，直到找到双方的共识点。 因为分歧发生的概率较低，而且一般很快能够得到纠正，所以这里的逐块确认一般不会造成性能问题。当然，在这里进行二分查找或者某些规则的查找可能也能够在理论上得到收益。</p><p>每个 leader 都会为每一个 follower 保存一个 nextIndex 的变量， 标志了下一个需要发送给该 follower 的 entry 的 index。 在 leader 刚当选时，该值初始化为该 leader 的 log 的 index+1。 一旦 follower 拒绝了 entry，则 leader 会执行 nextIndex—，然后再次发送。直到 follower 接收后将 matchIndex 设置为此时的 nextIndex - 1，然后开始正常的复制。这里还可以做一些更细粒度的优化，比如在正常复制时可以批量复制日志以减少系统调用的开销；在寻找共识点时可以只携带一条日志以减少不必要的流量传输，具体可以参考 etcd 的<a href="https://github.com/etcd-io/etcd/blob/main/raft/tracker/state.go" target="_blank" rel="noopener">设计</a>。</p><p>以下是 6.824 lab2 中 日志同步相关逻辑的具体实现，以供参考。</p><pre><code class="hljs Go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span> <span class="hljs-title">replicateOneRound</span><span class="hljs-params">(peer <span class="hljs-keyword">int</span>)</span></span> &#123;rf.mu.Lock()<span class="hljs-keyword">if</span> rf.state != StateLeader &#123;rf.mu.Unlock()<span class="hljs-keyword">return</span>&#125;prevLogIndex := rf.nextIndex[peer] - <span class="hljs-number">1</span><span class="hljs-keyword">if</span> prevLogIndex &lt; rf.getFirstLog().Index &#123;<span class="hljs-comment">// only snapshot can catch up</span>request := rf.genInstallSnapshotRequest()rf.mu.Unlock()response := <span class="hljs-built_in">new</span>(InstallSnapshotResponse)<span class="hljs-keyword">if</span> rf.sendInstallSnapshot(peer, request, response) &#123;rf.mu.Lock()rf.handleInstallSnapshotResponse(peer, request, response)rf.mu.Unlock()&#125;&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-comment">// just entries can catch up</span>request := rf.genAppendEntriesRequest(prevLogIndex)rf.mu.Unlock()response := <span class="hljs-built_in">new</span>(AppendEntriesResponse)<span class="hljs-keyword">if</span> rf.sendAppendEntries(peer, request, response) &#123;rf.mu.Lock()rf.handleAppendEntriesResponse(peer, request, response)rf.mu.Unlock()&#125;&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span> <span class="hljs-title">AppendEntries</span><span class="hljs-params">(request *AppendEntriesRequest, response *AppendEntriesResponse)</span></span> &#123;rf.mu.Lock()<span class="hljs-keyword">defer</span> rf.mu.Unlock()<span class="hljs-keyword">defer</span> rf.persist()<span class="hljs-keyword">defer</span> DPrintf(<span class="hljs-string">"&#123;Node %v&#125;'s state is &#123;state %v,term %v,commitIndex %v,lastApplied %v,firstLog %v,lastLog %v&#125; before processing AppendEntriesRequest %v and reply AppendEntriesResponse %v"</span>, rf.me, rf.state, rf.currentTerm, rf.commitIndex, rf.lastApplied, rf.getFirstLog(), rf.getLastLog(), request, response)<span class="hljs-keyword">if</span> request.Term &lt; rf.currentTerm &#123;response.Term = rf.currentTermresponse.Success = <span class="hljs-literal">false</span><span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> request.Term &gt; rf.currentTerm &#123;rf.currentTerm = request.Termrf.votedFor = <span class="hljs-number">-1</span>&#125;rf.ChangeState(StateFollower)rf.electionTimer.Reset(RandomizedElectionTimeout())<span class="hljs-keyword">if</span> request.PrevLogIndex &lt; rf.getFirstLog().Index &#123;response.Term = <span class="hljs-number">0</span>response.Success = <span class="hljs-literal">false</span>DPrintf(<span class="hljs-string">"&#123;Node %v&#125; receives unexpected AppendEntriesRequest %v from &#123;Node %v&#125; because prevLogIndex %v &lt; firstLogIndex %v"</span>, rf.me, request, request.LeaderId, request.PrevLogIndex, rf.getFirstLog().Index)<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> !rf.matchLog(request.PrevLogTerm, request.PrevLogIndex) &#123;response.Term = rf.currentTermresponse.Success = <span class="hljs-literal">false</span>lastIndex := rf.getLastLog().Index<span class="hljs-keyword">if</span> lastIndex &lt; request.PrevLogIndex &#123;response.ConflictTerm = <span class="hljs-number">-1</span>response.ConflictIndex = lastIndex + <span class="hljs-number">1</span>&#125; <span class="hljs-keyword">else</span> &#123;firstIndex := rf.getFirstLog().Indexresponse.ConflictTerm = rf.logs[request.PrevLogIndex-firstIndex].Termindex := request.PrevLogIndex - <span class="hljs-number">1</span><span class="hljs-keyword">for</span> index &gt;= firstIndex &amp;&amp; rf.logs[index-firstIndex].Term == response.ConflictTerm &#123;index--&#125;response.ConflictIndex = index&#125;<span class="hljs-keyword">return</span>&#125;firstIndex := rf.getFirstLog().Index<span class="hljs-keyword">for</span> index, entry := <span class="hljs-keyword">range</span> request.Entries &#123;<span class="hljs-keyword">if</span> entry.Index-firstIndex &gt;= <span class="hljs-built_in">len</span>(rf.logs) || rf.logs[entry.Index-firstIndex].Term != entry.Term &#123;rf.logs = shrinkEntriesArray(<span class="hljs-built_in">append</span>(rf.logs[:entry.Index-firstIndex], request.Entries[index:]...))<span class="hljs-keyword">break</span>&#125;&#125;rf.advanceCommitIndexForFollower(request.LeaderCommit)response.Term = rf.currentTermresponse.Success = <span class="hljs-literal">true</span>&#125;</code></pre><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><h3 id="选举限制"><a href="#选举限制" class="headerlink" title="选举限制"></a>选举限制</h3><p>因为 leader 的强势地位，所以 Raft 在投票阶段就确保选举出的 leader 一定包含了整个集群中目前已 committed 的所有日志。</p><p>当 candidate 发送 RequestVoteRPC 时，会带上最后一个 entry 的信息。 所有的节点收到该请求后，都会比对自己的日志，如果发现自己的日志更新一些，则会拒绝投票给该 candidate。 （Pre-Vote 同理，如果 follower 认为 Pre-Candidate 没有资格的话，会拒绝 PreVote）</p><p>判断日志新旧的方式：获取请求的 entry 后，比对自己日志中的最后一个 entry。 首先比对 term，如果自己的 term 更大，则拒绝请求。 如果 term 一样，则比对 index，如果自己的 index 更大（说明自己的日志更长），则拒绝请求。</p><pre><code class="hljs GO"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(l *raftLog)</span> <span class="hljs-title">isUpToDate</span><span class="hljs-params">(lasti, term <span class="hljs-keyword">uint64</span>)</span> <span class="hljs-title">bool</span></span> &#123;<span class="hljs-keyword">return</span> term &gt; l.lastTerm() || (term == l.lastTerm() &amp;&amp; lasti &gt;= l.lastIndex())&#125;</code></pre><p><img src="/raft/leader_restriction.png" srcset="/img/loading.gif" alt></p><p>在上图中，raft 为了避免出现一致性问题，要求 leader 绝不会提交过去的 term 的 entry （即使该 entry 已经被复制到了多数节点上）。leader 永远只提交当前 term 的 entry， 过去的 entry 只会随着当前的 entry 被一并提交。（上图中的 c，term2 只会跟随 term4 被提交。）</p><p>如果一个 candidate 能取得多数同意，说明它的日志已经是多数节点中最完备的， 那么也就可以认为该 candidate 已经包含了整个集群的所有 committed entries。</p><p>因此 leader 当选后，应当立刻发起 AppendEntriesRPC 提交一个 no-op entry。注意，这是一个 <code>Must</code>，不是一个 <code>Should</code>，否则会有许多 corner case 存在问题。比如：</p><ul><li>读请求：leader 此时的状态机可能并不是最新的，若服务读请求可能会违反线性一致性，即出现 safety 的问题；若不服务读请求则可能会有 liveness 的问题。</li><li>配置变更：可能会导致数据丢失，具体原因和例子可以参考此<a href="https://zhuanlan.zhihu.com/p/359206808" target="_blank" rel="noopener">博客</a>。</li></ul><h3 id="节点崩溃"><a href="#节点崩溃" class="headerlink" title="节点崩溃"></a>节点崩溃</h3><p>如果 leader 崩溃，集群中的所有节点在 electionTimeout 时间内没有收到 leader的心跳信息就会触发新一轮的选主。总而言之，最终集群总会选出唯一的 leader 。按论文中的说法，计算一次 RPC 耗时高达 <code>30～40ms</code> 时，<code>99.9%</code> 的选举依然可以在 <code>3s</code> 内完成，但一般一个机房内一次 RPC 只需 1ms。当然，选主期间整个集群对外是不可用的。 </p><p>如果 follower 和 candidate 奔溃相对而言就简单很多， 因为 Raft 所有的 RPC 都是幂等的，所以 Raft 中所有的请求，只要超时，就会无限的重试。follower 和 candidate 崩溃恢复后，可以收到新的请求，然后按照上面谈论过的追加或拒绝 entry 的方式处理请求。</p><h3 id="时间与可用性"><a href="#时间与可用性" class="headerlink" title="时间与可用性"></a>时间与可用性</h3><p>Raft 原则上可以在绝大部分延迟情况下保证一致性， 不过为了保证选择和 leader 的正常工作，最好能满足下列时间条件：</p><pre><code class="hljs armasm"><span class="hljs-keyword">broadcastTime </span>&lt;&lt; electionTimeout &lt;&lt; MTBF</code></pre><ul><li><code>broadcastTime</code>：向其他节点并发发送消息的平均响应时间；</li><li><code>electionTimeout</code>：follower 判定 leader 已经故障的时间（heartbeat 的最长容忍间隔）；</li><li><code>MTBF(mean time between failures)</code>：单台机器的平均健康时间；</li></ul><p>一般来说，broadcastTime 一般为 <code>0.5～20ms</code>，electionTimeout 可以设置为 <code>10～500ms</code>，MTBF 一般为一两个月。</p><h2 id="日志压缩"><a href="#日志压缩" class="headerlink" title="日志压缩"></a>日志压缩</h2><p>Raft 的日志在正常运行期间会增长以合并更多的客户请求，但是在实际的系统中，Raft 的日志无法不受限制地增长。随着日志的增长，日志会占用更多空间，并且需要花费更多时间进行重放。如果没有某种机制可以丢弃日志中累积的过时信息，这最终将导致可用性问题。因此需要定时去做 snapshot。</p><p>snapshot 会包括：</p><ul><li>状态机当前的状态。</li><li>状态机最后一条应用的 entry 对应的 index 和 term。</li><li>集群最新配置信息。</li><li>为了保证 exactly-once 线性化语义的去重表（之后会介绍到）。</li></ul><p>各个节点自行择机完成自己的 snapshot 即可，如果 leader 发现需要发给某一个 follower 的 nextIndex 已经被做成了 snapshot，则需要将 snapshot 发送给该 follower。注意 follower 拿到非过期的 snapshot 之后直接覆盖本地所有状态即可，不需要留有部分 entry，也不会出现 snapshot 之后还存在有效的 entry。因此 follower 只需要判断 <code>InstallSnapshot RPC</code> 是否过期即可。过期则直接丢弃，否则直接替换全部状态即可。 </p><p>snapshot 可能会带来两个问题：</p><ol><li><p>做 snapshot 的策略？<br>一般为定时或者定大小，达到阈值即做 snapshot，做完后对状态机和 raft log 进行原子性替换即可。</p></li><li><p>做 snapshot 时是否还可继续提供写请求？<br>一般情况下，做 snapshot 期间需要保证状态机不发生变化，也就是需要保证 snapshot 期间状态机不处理写请求。当然 raft 层依然可以去同步，只是状态机不能变化，即不能 apply 新提交的日志到状态机中而已。要想做的更好，可以对状态机采用 <code>copy-on-write</code> 的复制来不阻塞写请求。</p></li></ol><p>以下是 6.824 lab2 中 日志压缩相关逻辑的具体实现，以供参考。</p><pre><code class="hljs Go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span> <span class="hljs-title">Snapshot</span><span class="hljs-params">(index <span class="hljs-keyword">int</span>, snapshot []<span class="hljs-keyword">byte</span>)</span></span> &#123;rf.mu.Lock()<span class="hljs-keyword">defer</span> rf.mu.Unlock()snapshotIndex := rf.getFirstLog().Index<span class="hljs-keyword">if</span> index &lt;= snapshotIndex &#123;DPrintf(<span class="hljs-string">"&#123;Node %v&#125; rejects replacing log with snapshotIndex %v as current snapshotIndex %v is larger in term %v"</span>, rf.me, index, snapshotIndex, rf.currentTerm)<span class="hljs-keyword">return</span>&#125;rf.logs = shrinkEntriesArray(rf.logs[index-snapshotIndex:])rf.logs[<span class="hljs-number">0</span>].Command = <span class="hljs-literal">nil</span>rf.persister.SaveStateAndSnapshot(rf.encodeState(), snapshot)DPrintf(<span class="hljs-string">"&#123;Node %v&#125;'s state is &#123;state %v,term %v,commitIndex %v,lastApplied %v,firstLog %v,lastLog %v&#125; after replacing log with snapshotIndex %v as old snapshotIndex %v is smaller"</span>, rf.me, rf.state, rf.currentTerm, rf.commitIndex, rf.lastApplied, rf.getFirstLog(), rf.getLastLog(), index, snapshotIndex)&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span> <span class="hljs-title">InstallSnapshot</span><span class="hljs-params">(request *InstallSnapshotRequest, response *InstallSnapshotResponse)</span></span> &#123;rf.mu.Lock()<span class="hljs-keyword">defer</span> rf.mu.Unlock()<span class="hljs-keyword">defer</span> DPrintf(<span class="hljs-string">"&#123;Node %v&#125;'s state is &#123;state %v,term %v,commitIndex %v,lastApplied %v,firstLog %v,lastLog %v&#125; before processing InstallSnapshotRequest %v and reply InstallSnapshotResponse %v"</span>, rf.me, rf.state, rf.currentTerm, rf.commitIndex, rf.lastApplied, rf.getFirstLog(), rf.getLastLog(), request, response)response.Term = rf.currentTerm<span class="hljs-keyword">if</span> request.Term &lt; rf.currentTerm &#123;<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">if</span> request.Term &gt; rf.currentTerm &#123;rf.currentTerm = request.Termrf.votedFor = <span class="hljs-number">-1</span>&#125;rf.ChangeState(StateFollower)rf.electionTimer.Reset(RandomizedElectionTimeout())<span class="hljs-comment">// outdated snapshot</span><span class="hljs-keyword">if</span> request.LastIncludedIndex &lt;= rf.commitIndex &#123;<span class="hljs-keyword">return</span>&#125;<span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;rf.applyCh &lt;- ApplyMsg&#123;SnapshotValid: <span class="hljs-literal">true</span>,Snapshot:      request.Data,SnapshotTerm:  request.LastIncludedTerm,SnapshotIndex: request.LastIncludedIndex,&#125;&#125;()&#125;<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span> <span class="hljs-title">CondInstallSnapshot</span><span class="hljs-params">(lastIncludedTerm <span class="hljs-keyword">int</span>, lastIncludedIndex <span class="hljs-keyword">int</span>, snapshot []<span class="hljs-keyword">byte</span>)</span> <span class="hljs-title">bool</span></span> &#123;rf.mu.Lock()<span class="hljs-keyword">defer</span> rf.mu.Unlock()DPrintf(<span class="hljs-string">"&#123;Node %v&#125; service calls CondInstallSnapshot with lastIncludedTerm %v and lastIncludedIndex %v to check whether snapshot is still valid in term %v"</span>, rf.me, lastIncludedTerm, lastIncludedIndex, rf.currentTerm)<span class="hljs-comment">// outdated snapshot</span><span class="hljs-keyword">if</span> lastIncludedIndex &lt;= rf.commitIndex &#123;DPrintf(<span class="hljs-string">"&#123;Node %v&#125; rejects the snapshot which lastIncludedIndex is %v because commitIndex %v is larger"</span>, rf.me, lastIncludedIndex, rf.commitIndex)<span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>&#125;<span class="hljs-keyword">if</span> lastIncludedIndex &gt; rf.getLastLog().Index &#123;rf.logs = <span class="hljs-built_in">make</span>([]Entry, <span class="hljs-number">1</span>)&#125; <span class="hljs-keyword">else</span> &#123;rf.logs = shrinkEntriesArray(rf.logs[lastIncludedIndex-rf.getFirstLog().Index:])rf.logs[<span class="hljs-number">0</span>].Command = <span class="hljs-literal">nil</span>&#125;<span class="hljs-comment">// update dummy entry with lastIncludedTerm and lastIncludedIndex</span>rf.logs[<span class="hljs-number">0</span>].Term = lastIncludedTermrf.logs[<span class="hljs-number">0</span>].Index = lastIncludedIndexrf.lastApplied = lastIncludedIndexrf.commitIndex = lastIncludedIndexrf.persister.SaveStateAndSnapshot(rf.encodeState(), snapshot)DPrintf(<span class="hljs-string">"&#123;Node %v&#125;'s state is &#123;state %v,term %v,commitIndex %v,lastApplied %v,firstLog %v,lastLog %v&#125; after accepting the snapshot which lastIncludedTerm is %v, lastIncludedIndex is %v"</span>, rf.me, rf.state, rf.currentTerm, rf.commitIndex, rf.lastApplied, rf.getFirstLog(), rf.getLastLog(), lastIncludedTerm, lastIncludedIndex)<span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>&#125;</code></pre><h2 id="禅让"><a href="#禅让" class="headerlink" title="禅让"></a>禅让</h2><p>有时候，会希望取消当前 leader 的管理权，比如：</p><ul><li>leader 节点因为运维原因需要重启；</li><li>有其他更适合当 leader 的节点；</li></ul><p>直接将 leader 节点停机的话，其他节点会等待 electionTimeout 后进入选举状态， 这期间会集群会停止响应。为了避免这一段不可用的时间，可以采用禅让机制（<code>leadership transfer</code>）。</p><p>禅让的步骤为：</p><ol><li>leader 停止响应客户端请求；</li><li>leader 向 target 节点发起一次日志同步；</li><li>leader 向 target 发起一次 TimeoutNowRPC，target 收到该请求后立刻发起一轮投票。</li></ol><p>etcd 中实现了更多的细节（也有一些改动）：</p><ol><li>leader 先检查禅让对象（leadTransferee）的身份，如果是 follower，直接忽略；</li><li>leader 检查是否有正在进行的禅让，如果有，则中止之前的禅让状态，开始处理最新的请求；</li><li>检查禅让对象是否是自己，如果是，忽略；</li><li>将禅让状态信息计入 leader 的状态，并且重置 electionElapsed（因为禅让应该在 electionTimeout 内完成）；</li><li>检查禅让对象的日志是否是最新的</li><li>如果禅让对象已经是最新，则直接发送 TimeoutNowRPC</li><li>如果不是，则发送 AppendEntriesRPC，待节点响应成功后，再发送 TimeoutNowRPC</li></ol><p>可以看出，在 etcd 中，leader 除了重置 electionElapsed 外，不会改动自己的状态。 既不会停止对客户端的响应，同时还会继续发送心跳。</p><p>因为 target 机器会更新自己的 term，而且率先发起投票，其有很大的概率赢得选举。 需要注意的是，target 发起的 RequestVoteRPC 中的 <code>isLeaderTransfer=true</code>， 以防止被其他节点忽略。</p><p>如果 target 机器没能在一次 electionTimeout 内完成选举，那么 leader 认为本次禅让失败， 立刻恢复响应客户端的请求。（这时可以再次重新发起一次禅让请求）</p><p>在 etcd/raft 中，RequestVoteRPC.context 会被设置为 campaignTransfer, 表明本次投票请求来源于 leader transfer，可以强行打断 follower 的租约发起选举。</p><h2 id="预投票"><a href="#预投票" class="headerlink" title="预投票"></a>预投票</h2><p>一个暂时脱离集群网络的节点，在重新加入集群后会干扰到集群的运行。</p><p>因为当一个节点和集群失去联系后，在等待 electionTimeout 后，它就会增加自己的 term 并发起选举， 因为联系不上其他节点，所以在 electionTimeout 后，它会继续增加自己的 term 并继续发起选举。</p><p>一段时间以后，它的 term 就会显著的高于原集群的 term。如果此后该节点重新和集群恢复了联络， 它的高 term 会导致 leader 立刻退位，并重新举行选举。</p><p>为了避免这一情形，引入了 Pre-Vote 的机制。在该机制下，一个 candidate 必须在获得了多数赞同的情形下， 才会增加自己的 term。一个节点在满足下述条件时，才会赞同一个 candidate：</p><ul><li>该 candidate 的日志足够新；</li><li>当前节点已经和 leader 失联（electionTimeout）。</li></ul><p>也就是说，candidate 会先发起一轮 Pre-Vote，获得多数同意后，更新自己的 term， 再发起一轮 RequestVoteRPC。</p><p>这种情形下，脱离集群的节点，只会不断的发起 Pre-Vote，而不会更新自己的 term。</p><p>在 etcd 的实现中，如果某个节点赞同了某个 candidate， 是不需要更新自己的状态的，它依然可以赞同其他 candidate。 而且，即使收到的 PreVote 的 term 大于自己，也不会更新自己的 term。 也就是说，PreVote 不会改变其他节点的任何状态。</p><p>etcd 中还有一个设计是，当发起 PreVote 的时候，针对的是下一轮的 term， 所以会向所有的节点发送一个 term+1 的 PreVoteReq。</p><pre><code class="hljs GO"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *raft)</span> <span class="hljs-title">campaign</span><span class="hljs-params">(t CampaignType)</span></span> &#123;<span class="hljs-keyword">var</span> term <span class="hljs-keyword">uint64</span><span class="hljs-keyword">var</span> voteMsg pb.MessageType<span class="hljs-keyword">if</span> t == campaignPreElection &#123;r.becomePreCandidate()voteMsg = pb.MsgPreVote<span class="hljs-comment">// 这里需要注意的是，PreVote 会针对“下一轮 term”发起投票，</span><span class="hljs-comment">// 而 Vote 则是针对当前 term</span><span class="hljs-comment">// PreVote RPCs are sent for the next term before we've incremented r.Term.</span>term = r.Term + <span class="hljs-number">1</span>&#125; <span class="hljs-keyword">else</span> &#123;r.becomeCandidate()voteMsg = pb.MsgVoteterm = r.Term&#125;        <span class="hljs-comment">// ...</span>        <span class="hljs-comment">// 发送投票请求</span>    r.send(pb.Message&#123;Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx&#125;)        <span class="hljs-comment">// ...</span>&#125;</code></pre><h2 id="配置变更"><a href="#配置变更" class="headerlink" title="配置变更"></a>配置变更</h2><p>raft 的配置变更一般分为两种方式：一次变更一个和一次变更多个，前者实现相对简单，改动多个节点的配置变更可以被分成多个一次变更一个来执行；后者相对复杂，并且在某些场景下可用性更强，具体可参考 TiDB 5.0 的<a href="https://mp.weixin.qq.com/s/nLWbEEBBVYuNGde0IbE3XQ" target="_blank" rel="noopener">介绍</a>。</p><p>以下仅会简单介绍两种成员变更方式，具体可以参考 raft 作者博士论文的<a href="https://github.com/LebronAl/raft-thesis-zh_cn/blob/master/raft-thesis-zh_cn.md#4-%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E6%9B%B4%E6%94%B9" target="_blank" rel="noopener">第四章</a>和<a href="https://zhuanlan.zhihu.com/p/359206808" target="_blank" rel="noopener">Raft 成员变更的工程实践</a>。</p><h3 id="一次变更一台"><a href="#一次变更一台" class="headerlink" title="一次变更一台"></a>一次变更一台</h3><p>因为在 Raft 算法中，集群中每一个节点都存有整个集群的信息，而集群的成员有可能会发生变更（节点增删、替换节点等）。 Raft 限制一次性只能增／删一个节点，在一次变更结束后，才能继续进行下一次变更。</p><p>如果一次性只变更一个节点，那么只需要简单的要求“在新／旧集群中，都必须取得多数（N/2+1）”， 那么这两个多数中必然会出现交集，这样就可以保证不会因为配置不一致而导致脑裂。</p><p><img src="/raft/singlechange.png" srcset="/img/loading.gif" alt></p><p>当 leader 收到集群变更的请求后，就会生成一个特殊的 entry 项用来保存配置， 在将配置项添加到 log 后，该配置立刻生效（也就是说任何节点在收到新配置后，就立刻启用新配置）。 然后 leader 将该 entry 扩散至多数节点，成功后则提交该 entry。 一旦一个新配置项被 committed，则视为该次变更已结束，可以继续处理下一次变更了。</p><p>为了保证可用性，需要新增一项规则，节点在响应 RPC 时，不考虑来源节点是否在自己的配置文件之中。 也就是说，即使收到了一个并不在自己配置文件之中的节点发来的 RPC， 也需要正常处理和响应，包括 AppendEntriesRPC 和 RequestVoteRPC。</p><h3 id="一次变更多台"><a href="#一次变更多台" class="headerlink" title="一次变更多台"></a>一次变更多台</h3><p>这种变更方式可以一次性变更多个节点（arbitrary configuration）。</p><p>当集群成员在变更时，为了保证服务的可用性（不发生中断），以及避免因为节点变更导致的一致性问题， Raft 提出了两阶段变更，当接收到新的配置文件后，集群会首先进入 joint consensus 状态， 待新的配置文件提交成功后，再回到普通状态。</p><p>更具体的，joint consensus 指的是包含新／旧配置文件全部节点的中间状态：</p><ul><li>entries 会被复制到新／旧配置文件中的所有节点；</li><li>新／旧配置文件中的任何一个节点都有可能被选为 leader；</li><li>共识（选举或提交）需要同时在新／旧配置文件中分别获取到多数同意（<code>separate majorities</code>）</li></ul><p>（注：<code>separate majorities</code>的意思是需要新／旧集群中的多数都同意。比如如果是从 3 节点切换为全新的 9 节点， 那么要求旧配置中的 2 个节点，和新配置中的 5 个节点都同意，才被认为达成了一次共识）</p><p>所以，在一次配置变更中，一共有三个状态：</p><ul><li><code>C_old</code>：使用旧的配置文件；</li><li><code>C_old,new</code>：同时使用新旧配置文件，也就是新／旧节点的并集；</li><li><code>C_new</code>：使用新的配置文件。</li></ul><p>配置文件使用特殊的 entries 进行存储，一个节点一旦获取到新的配置文件， 即使该配置 entry 并没有 committed，也会立刻使用该配置。 所以一次完整的配置变更可以表示为下图：</p><p><img src="/raft/jointchange.png" srcset="/img/loading.gif" alt></p><ol><li>C_old,new 被创建，集群进入 joint consensus，leader 开始传播该 entry；</li><li>C_old,new 被 committed，也就是说此时多数节点都拥有了 C_old,new，此后 C_old 已经不再可能被选为 leader；</li><li>leader 创建并传播 C_new；</li><li>C_new 被提交，此后不在 C_new 内的节点不允许被选为 leader，如有 leader 不在 C_new 则自行退位。</li></ol><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>不在 C_new 的节点可能会干扰集群。</p><p>当 C_new 开始生效后，被移除的节点就会收不到 heartbeat，所以在 electionTimeout 后， 这些节点会更新自己的 term 然后开始尝试竞选，这会导致目前的 leader 被迫退回 follower 并启动一轮投票。 这会反复发生，严重影响效率。</p><p>解决办法是，每一个 follower 增加一个机制，当节点处于 minimum election timeout 之内时， 也就是当一个节点认为自己的 leader 依然存活时，将会拒绝 RequestVoteRPC，既不会同意投票， 也不会根据 RequestVoteRPC 更新自己的 term。</p><p>但如果此时集群正好处于选举之中，那么 C_old 集群的节点可能还是会造成干扰， 所以结合 PreVote 更为可靠。</p><p>这一机制会干扰到 leader transfer 机制，因为在 leader transfer 中，即使 electionTimeout 未到， RequestVoteRPC 也应该打断所有的节点，要求立刻开始进行选举。 所以需要给 RequestVoteRPC 增加一个 flag 来表明是否来自于 leader transfer。</p><p>etcd 中也是增加了一个 flag campaignTransfer 来做特殊标记，见上面的「禅让」一节。</p><h2 id="客户端交互"><a href="#客户端交互" class="headerlink" title="客户端交互"></a>客户端交互</h2><p>虽然说 raft 算法只是一个 RSM，其只需要保证不同节点上的日志相同即可，其他的事情它都不需要关心。但是要想保证线性一致性语义，对于基于 raft 的 KV 往往还需要额外做一些事情，比如即使客户端会超时重试，也要保证日志的 exactly-once 执行。</p><p><img src="/raft/client.png" srcset="/img/loading.gif" alt></p><p>考虑这样一个场景，客户端向服务端提交了一条日志，服务端将其在 raft 组中进行了同步并成功 commit，接着在 apply 后返回给客户端执行结果。然而不幸的是，该 rpc 在传输中发生了丢失，客户端并没有收到写入成功的回复。因此，客户端只能进行重试直到明确地写入成功或失败为止，这就可能会导致相同地命令被执行多次，从而违背线性一致性。</p><p>有人可能认为，只要写请求是幂等的，那重复执行多次也是可以满足线性一致性的，实际上则不然。考虑这样一个例子：对于一个仅支持 put 和 get 接口的 raftKV 系统，其每个请求都具有幂等性。设 x 的初始值为 0，此时有两个并发客户端，客户端 1 执行 put(x,1)，客户端 2 执行 get(x) 再执行 put(x,2)，问（客户端 2 读到的值，x 的最终值）是多少。对于线性一致的系统，答案可以是 (0,1)，(0,2) 或 (1,2)。然而，如果客户端 1 执行 put 请求时发生了上段描述的情况，然后客户端 2 读到 x 的值为 1 并将 x 置为了 2，最后客户端 1 超时重试且再次将 x 置为 1。对于这种场景，答案是 (1,1)，这就违背了线性一致性。归根究底还是由于幂等的 put(x,1) 请求在状态机上执行了两次，有两个 LZ 点。因此，即使写请求的业务语义能够保证幂等，不进行额外的处理让其重复执行多次也会破坏线性一致性。当然，读请求由于不改变系统的状态，重复执行多次是没问题的。</p><p>对于这个问题，raft 作者介绍了想要实现线性化语义，就需要保证日志仅被执行一次，即它可以被 commit 多次，但一定只能 apply 一次。其解决方案原文如下：</p><blockquote><p>The solution is for clients to assign unique serial numbers to every command. Then, the state machine tracks the latest serial number processed for each client, along with the associated response. If it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.</p></blockquote><p>基本思路便是：</p><ul><li>每个 client 都需要一个唯一的标识符，它的每个不同命令需要有一个顺序递增的 commandId，clientId 和这个 commandId，clientId 可以唯一确定一个不同的命令，从而使得各个 raft 节点可以记录保存各命令是否已应用以及应用以后的结果。</li></ul><p>为什么要记录应用的结果？因为通过这种方式同一个命令的多次 apply 最终只会实际应用到状态机上一次，之后相同命令 apply 的时候实际上是不应用到状态机上的而是直接返回的，那么这时候应该返回什么呢？直接返回成功吗？不行，如果第一次应用时状态机报了什么例如 key not exist 等业务上的错而没有被记录，之后就很难捕捉到这个执行结果了，所以也需要将应用结果保存下来。</p><p>如果默认一个客户端只能串行执行请求的话，服务端这边只需要记录一个 map，其 key 是 clientId，其 value 是该 clientId 执行的最后一条日志的 commandId 和状态机的输出即可。</p><p>raft 论文中还考虑了对这个 map 进行一定大小的限制，防止其无线增长。这就带来了两个问题：</p><ul><li>集群间的不同节点如何就某个 clientId 过期达成共识。</li><li>不小心驱逐了活跃的 clientId 怎么办，其之后不论是新建一个 clientId 还是复用之前的 clientId 都可能导致命令的重执行。</li></ul><p>这些问题在工程实现上都较为麻烦。比如后者如果业务上是事务那直接 abort 就行，但如果不是事务就很难办了。</p><p>实际上，个人感觉 clientId 是与 session 绑定的，其生命周期应该与 session 一致，开启 session 时从 map 中保存该 clientId，关闭 session 时从 map 中删除该 clientId 及其对应的 value 即可。map 中一个 clientId 对应的内存占用可能都不足 30 字节，相比携带更多业务语义的 session 其实很小，所以感觉没太大必要去严格控制该 map 的内存占用，还不如考虑下怎么控制 session 更大地内存占用呢。这样就不用去纠结前面提到的两个问题了。</p><p>感兴趣的可以参照 raft 博士论文第六章-<a href="https://github.com/LebronAl/raft-thesis-zh_cn/blob/master/raft-thesis-zh_cn.md#6-%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%BA%A4%E4%BA%92" target="_blank" rel="noopener">客户端交互</a> 和 dragonboat 作者在知乎上对其的<a href="https://www.zhihu.com/question/278551592" target="_blank" rel="noopener">讨论</a>。</p><h2 id="线性一致性读"><a href="#线性一致性读" class="headerlink" title="线性一致性读"></a>线性一致性读</h2><p>有关 raft 一致性读的实现，可以参考本人写的另一篇<a href="https://tanxinyu.work/consistency-and-consensus/#etcd-%E7%9A%84-Raft" target="_blank" rel="noopener">博客</a>。</p><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><p>业界的实践很多，可以参考 tikv 的<a href="https://zhuanlan.zhihu.com/p/25735592" target="_blank" rel="noopener">优化</a> 和 dragonboat 的<a href="https://zhuanlan.zhihu.com/p/52620657" target="_blank" rel="noopener">优化</a>。</p><p>以下简单列举几种：</p><ul><li>batching：降低 system call 的开销。</li><li>pipeline：提升 leader 向 follower 同步的吞吐量。</li><li>异步 apply：提升 raft 算法吞吐量。无法降低延迟，但能增加吞吐量。</li><li>并行同步日志与刷盘：并行 IO，提升 raft 算法吞吐量。</li><li>…</li></ul><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>所有节点间仅通过三种类型的 RPC 进行通信：</p><ul><li><code>AppendEntriesRPC</code>：最常用的，leader 向 follower 发送心跳或同步日志。</li><li><code>RequestVoteRPC</code>：选举时，candidate 发起的竞选请求。</li><li><code>InstallsnapshotRPC</code>：用于 leader 下发 snapshot。</li></ul><p>在 Diego 后续的博士论文中，又增加了一些 RPCs：</p><ul><li><code>AddServerRPC</code>：添加单台节点。</li><li><code>RemoveServerRPC</code>：移除一个节点。</li><li><code>TimeoutNowRPC</code>：立刻发起竞选。<br>（实际上 etcd 的实现中定义了几十种消息类型，甚至把内部事件也封装为消息一并处理。）</li></ul><h3 id="AppendEntriesRPC"><a href="#AppendEntriesRPC" class="headerlink" title="AppendEntriesRPC"></a>AppendEntriesRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 当前的 term；</li><li><code>leaderId</code>：leader 的 节点id，让 follower 可以重定向客户端的连接；</li><li><code>prevLogIndex</code>：前一块 entry 的 index；</li><li><code>prevlogterm</code>：前一块 entry 的 term；</li><li><code>entries[]</code>：给 follower 发送的 entry，可以一次发送多个，heartbeat 时该项可缺省；</li><li><code>leaderCommit</code>：leader 当前的 <code>committed index</code>，follower 收到后可用于自己的状态机。</li></ul><p>返回：</p><ul><li><code>term</code>：响应者自己的 term；</li><li><code>success</code>：bool，是否接受请求。<br>该请求通过 leaderCommit 通知 follower 提交相应的 entries 到。通过 entries[] 复制 leader 的日志到所有的 follower。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，立刻返回 false</li><li>如果 prevLogIndex 不匹配，返回 false</li><li>如果自己有块 entry 和新的 entry 不匹配（在相同的 index 上有不同的 term）， 删除自己的那一块以及之后的所有 entry；</li><li>把新的 entries 添加到自己的 log；<br>5 。如果 <code>leaderCommit &gt; commitindex</code>，将 commitIndex 设置为 <code>min(leaderCommit, last index)</code>， 并且提交相应的 entries。</li></ol><h3 id="RequestVoteRPC"><a href="#RequestVoteRPC" class="headerlink" title="RequestVoteRPC"></a>RequestVoteRPC</h3><p>参数：</p><ul><li><code>term</code>：candidate 当前的 term；</li><li><code>candidateId</code>：candidate 的节点 id</li><li><code>lastlogindex</code>：candidate 最后一个 entry 的 index；</li><li><code>lastlogterm</code>：candidate 最后一个 entry 的 term。</li><li><code>isleaderTransfer</code>：用于表明该请求来自于禅让，无需等待 electionTimeout，必须立刻响应。</li><li><code>isPreVote</code>：用来表明当前是 PreVote 还是真实投票</li></ul><p>返回：</p><ul><li><code>term</code>：响应者当前的 term；</li><li><code>voteGranted</code>：bool，是否同意投票。</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code>，返回 false；</li><li>如果 votedFor 为空或者为该 <code>candidated id</code>，且日志项不落后于自己，则同意投票。</li></ol><h3 id="InstallsnapshotRPC"><a href="#InstallsnapshotRPC" class="headerlink" title="InstallsnapshotRPC"></a>InstallsnapshotRPC</h3><p>参数：</p><ul><li><code>term</code>：leader 的 term</li><li><code>leaderId</code>：leader 的 节点 id</li><li><code>lastIncludedindex</code>：snapshot 中最后一块 entry 的 index；</li><li><code>lastIncludedterm</code>：snapshot 中最后一块 entry 的 term；</li><li><code>offset</code>：该份 chunk 的 offset；</li><li><code>data[]</code>：二进制数据；</li><li><code>done</code>：是否是最后一块 chunk</li></ul><p>返回：</p><ul><li><code>term</code>：follower 当前的 term</li></ul><p>实现细节：</p><ol><li>如果 <code>term &lt; currentTerm</code> 就立即回复</li><li>如果是第一个分块（offset 为 0）就创建一个新的快照</li><li>在指定偏移量写入数据</li><li>如果 done 是 false，则继续等待更多的数据</li><li>保存快照文件，丢弃索引值小于快照的日志</li><li>如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保持</li><li>丢弃整个日志</li><li>使用快照重置状态机</li></ol><h3 id="AddServerRPC"><a href="#AddServerRPC" class="headerlink" title="AddServerRPC"></a>AddServerRPC</h3><p>参数：</p><ul><li><code>newServer</code>：新节点地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否添加成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>如果没有在 electionTimeout 内处理，则返回 TIMEOUT；</li><li>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="RemoveServerRPC"><a href="#RemoveServerRPC" class="headerlink" title="RemoveServerRPC"></a>RemoveServerRPC</h3><p>参数：</p><ul><li><code>oldServer</code>：要删除的节点的地址</li></ul><p>返回：</p><ul><li><code>status</code>：bool，是否删除成功；</li><li><code>leaderHint</code>：当前 leader 的信息。</li></ul><p>实现细节：</p><ol><li>如果节点不是 leader，返回 NOT_LEADER；</li><li>等待上一次配置变更完成后，再处理当前变更；</li><li>将新的配置项加入 log，然后发起多数共识，通过后再提交；</li><li>返回 OK。</li></ol><h3 id="TimeoutNowRPC"><a href="#TimeoutNowRPC" class="headerlink" title="TimeoutNowRPC"></a>TimeoutNowRPC</h3><p>由 leader 发起，告知 target 节点立刻发起竞选，无视 electionTimeout。主要用于禅让。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇博客较为详细的介绍了 raft 算法的实现，希望能对读者理解 raft 算法有所帮助。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ul><li><a href="https://raft.github.io/" target="_blank" rel="noopener">raft 官网</a></li><li><a href="http://thesecretlivesofdata.com/raft/" target="_blank" rel="noopener">raft 动画教程</a></li><li><a href="https://raft.github.io/raft.pdf" target="_blank" rel="noopener">raft 会议论文</a></li><li><a href="https://web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf" target="_blank" rel="noopener">raft 博士论文</a></li><li><a href="https://blog.laisky.com/p/raft/#%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E5%8F%98%E6%9B%B4-ohtdR" target="_blank" rel="noopener">raft 论文笔记</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-raft.txt" target="_blank" rel="noopener">6.824 Raft 讲义 1</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l-raft2.txt" target="_blank" rel="noopener">6.824 Raft 讲义 2</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
      <tag>共识算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CAP 定理介绍</title>
    <link href="/cap-theory/"/>
    <url>/cap-theory/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在互联网行业飞速发展的 21 世纪，分布式系统正变得越来越重要，大型互联网公司如 Google，Amazon，MicroSoft，Alibaba，Tencent 等之所以被认为技术很厉害，很大程度上是因为其后台十分强悍，而这些后台一定是由若干个大的分布式系统组成的，因此理解分布式系统的运行原理对于程序员有非常重要的意义。</p><p>CAP 定理是分布式系统方向一个比较宽泛但很重要的基本定理，也可以作为理解分布式系统的起点。</p><p>这篇博客将简单介绍一下 CAP 定理。</p><h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>2000 年，柏克莱加州大学（University of California, Berkeley）的计算机科学家 Eric Brewer 在分布式计算原则研讨会（Symposium on Principles of Distributed Computing）提出，分布式系统有三个指标。</p><ul><li>Consistency（可用性）</li><li>Availability（一致性）</li><li>Partition tolerance（网络分区容错性）</li></ul><p>它们的第一个字母分别是 C、A、P。</p><p>理论上，只能从 CAP 三者中选择两者，然而，这种选择的边界并非是非此即彼的（not binary），很多时候混合考虑不同程度的各个因素，结果可能是更好的。（ <code>The whole spectrum in between is useful; mixing different levels of Availability and Consistency usually yields a better result</code>）</p><p>需要注意的是，尽管我们常说某个系统能够满足 CAP 属性中的 2 个，但并不是必须满足 2 个，许多系统只具有 0 或 1 个 CAP 属性。</p><p>此外，很多同学会纠结单机系统到底是 CA 系统还是 CP 系统，个人觉得纠结这个问题没有任何意义，注意 CAP 定律的完整表述：<code>Any networked shared-data system can have at most two of the three desired properties</code>，也就是说 CAP 定理并不针对单机系统做限定，因此将这个不属于单机的概念强加在单机系统上并无意义。</p><h2 id="定义-amp-证明"><a href="#定义-amp-证明" class="headerlink" title="定义 &amp; 证明"></a>定义 &amp; 证明</h2><p>其实 CAP 定理已经被大家讲烂了，网上一搜都会出现很多附带个人解读且不一定正确的博客，我自己之前也写过一版，后面也觉得太烂了。</p><p>幸运的是我找到了一篇<a href="https://zhuanlan.zhihu.com/p/338835258" target="_blank" rel="noopener">博客</a>，这篇博客的后半部分（ <code>对 CAP 的常见误解</code> 章节， <code>CAP 理论的一些疑问</code> 章节和 <code>CAP 的不足</code> 章节）总结的很清晰中肯，在这里分享给大家，我之前写的垃圾就删了吧。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>个人认为，CAP 定理的核心在于，在网络分区的情况下，我们需要对 C 和 A 做出相应的妥协，我们不可能完全满足 CA，但是我们可以合理控制 C 和 A 之间的比例让我们的应用/中间件正常提供服务，同时也尽量提升基础设施的稳定性来保障 P。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>《Spanner，真时，CAP 理论》是 Google VP，CAP 理论之父在情人节当天撰写的，主要介绍了 Google 的 Spanner 数据库的真时（TrueTime）服务和 CA 特性，以及结合 CAP 理论的一些思考，建议阅读，阅读 Spanner 论文后阅读更佳。</p><ul><li><p><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45855.pdf" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》</a></p></li><li><p><a href="https://toutiao.io/posts/zdqrx0/preview" target="_blank" rel="noopener">《Spanner，真时，CAP 理论》中文</a></p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>分布式系统理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>iTerm2 快捷键介绍</title>
    <link href="/iTerm2-hotkeys/"/>
    <url>/iTerm2-hotkeys/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>iTerm2 是 MacOS 独有的终端工具，其有许多快捷键可以使用。为了便于开发并节约之后再次在搜索引擎上查询的时间成本，特写此博客以供自己日后查看。</p><h2 id="快捷键介绍"><a href="#快捷键介绍" class="headerlink" title="快捷键介绍"></a>快捷键介绍</h2><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><ul><li>新建标签：Command + T</li><li>关闭标签：Command + W</li><li>切换标签：Command + 数字 或 Command + 左右方向键</li></ul><h3 id="分屏"><a href="#分屏" class="headerlink" title="分屏"></a>分屏</h3><ul><li>垂直分屏：Command + D</li><li>水平分屏：Command + Shift + D</li><li>切换屏幕：Command + Option + 方向键 或 Command + [ / ]</li></ul><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><ul><li>局部搜索(包含单个终端)：Command + F</li><li>全局搜索(包含所有Tab)：Command + Option + E</li><li>搜索历史指令：Ctrl + R</li></ul><h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><ul><li>查看历史命令：Command + ;</li><li>查看剪贴板历史：Command + Shift + H</li><li>上一条命令：Ctrl + P 或 上方向键</li></ul><h3 id="单行"><a href="#单行" class="headerlink" title="单行"></a>单行</h3><ul><li>光标到行首：Ctrl + A</li><li>光标到行尾：Ctrl + E</li><li>删除当前行：Ctrl + U</li><li>删除当前光标的字符：Ctrl + D</li><li>删除光标之前的字符：Ctrl + H</li><li>删除光标之前的单词：Ctrl + W</li><li>删除到文本末尾：Ctrl + K</li></ul><h3 id="内容大小"><a href="#内容大小" class="headerlink" title="内容大小"></a>内容大小</h3><ul><li>放大终端：Command + +</li><li>缩小终端：Command + - </li></ul><h3 id="常用快捷功能"><a href="#常用快捷功能" class="headerlink" title="常用快捷功能"></a>常用快捷功能</h3><ul><li>清屏：Command + R 或 Crtl + L</li><li>切换全屏：Command + Enter</li><li>选中即复制：在 iTerm2 界面，选择了一行就已经复制了</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>开发工具配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的高效 Macbook 工作环境配置</title>
    <link href="/mac-configuration/"/>
    <url>/mac-configuration/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>工欲善其事，必先利其器，工具永远都是用来解决问题的，没必要为了工具而工具，一切工具都是为了能快速准确的完成工作和学习任务而服务。</p><p>我呢，在使用了 Windows，Ubuntu 和 MacOS 三种操作系统之后。结合种种体验和踩坑，最终还是觉得 MacOS 更舒适一点。每个人都有每个人的看法，每个人都有每个人的舒适点，MacOS 恰好捏住了我的舒适点。因此，我之后都将从 MacOS 上工作学习。</p><p>前一段时间我从公司实习离职，上交了公司发给我的 MacBook（<del>停止薅羊毛</del>），然而我又不想回到 Windows，于是打算自己买一台 MacBook。但是 MacBook 从 2016 年开始更换的蝶式键盘很让我恶心，姑且不说故障率高，触感实在太差劲了。尽管 2020 年新出的 16 寸 Pro 已经重回剪刀脚键盘了，但是我的需求是轻薄的 13 寸而不是 16 寸（<del>只是没钱而已</del>）。尽管听到业界的呼声说 2020 年的 MacBook 应该都会回到剪刀脚键盘，但由于 2020 年会换新模具，我也不想踩第一代模具的坑，因而暂且将目标定为 2021 年的 MacBook，目前一年多买个二手过渡下就可以了。</p><p>1 月份我在某宝平台上买了一台 2014 款 8+256 的二手 MacBook Pro，即使前期做了许多选店和辨伪的功课，拿到手之后却依然中招，总是无理由黑屏然后再无法开机一天，十分坑爹。所幸可以十五天无理由退换货，就赶快退了。之前早就听说二手 Mac 的水很深，被坑一次之后更加确信。接着我做了更多的功课，学到了许多辨伪技巧，浏览了许多店铺，也算有点心得，之后要是有时间可以写出来分享给大家。</p><p>前几天经过慎重选择我又在某东平台上入手了一台 2015 款 8+128 的二手 MacBook Pro。这次总算没什么问题，但比较有趣的一点是我买的 8+128 的，老板发给我的是 8+256 的，平白无故赚了 128G 的固态，只能说真的舒服了。</p><p><img src="/mac-configuration/mac-configuration.jpeg" srcset="/img/loading.gif" alt></p><p>这是一个新的 MacBook 刚打开后的主页，接下来我要通过一系列的配置使其成为一个十分符合我开发习惯的机器，可供大家参考。</p><h2 id="系统篇"><a href="#系统篇" class="headerlink" title="系统篇"></a>系统篇</h2><h3 id="触屏板"><a href="#触屏板" class="headerlink" title="触屏板"></a>触屏板</h3><ul><li>2016 年及之后的 MacBook 触屏板都有 Force touch 的功能，即可以按压两次来实现更多的功能，但是我一直用不来这个功能，因此我的第一件事就是调整触摸屏板，首先先关掉 Force touch 的功能，然后开启轻点来点按的点击方式，个人觉得这样才符合 MacBook 轻巧的特性嘛，每次都按下去多麻烦啊，现在手指轻轻一碰触摸板，就达到鼠标单击的顺滑效果。</li><li>除此以外，可以根据自己的习惯开启或关闭一些手势。</li></ul><p><img src="/mac-configuration/touch_1.png" srcset="/img/loading.gif" alt><br><img src="/mac-configuration/touch_2.png" srcset="/img/loading.gif" alt><br><img src="/mac-configuration/touch_3.png" srcset="/img/loading.gif" alt></p><h3 id="键盘"><a href="#键盘" class="headerlink" title="键盘"></a>键盘</h3><ul><li>由于 MacBook 默认的重复前延迟和按键重复配置太慢，限制了程序员们优秀的打字速度，所以建议都调整到最快的速度。</li><li>可以在闲置 5 分钟后关闭键盘背光灯来省点电。</li></ul><p><img src="/mac-configuration/keyboard.png" srcset="/img/loading.gif" alt></p><h3 id="输入法"><a href="#输入法" class="headerlink" title="输入法"></a>输入法</h3><ul><li>由于 MacBook 默认的切换大小写的方式是长按 Caps 键，时间较慢需要等待，较为影响开发效率，建议关闭长按改为短按，配合极低的按键延迟会十分舒爽。</li></ul><p><img src="/mac-configuration/input.png" srcset="/img/loading.gif" alt></p><ul><li>建议安装搜狗输入法 Mac 版替代系统自带输入法。</li></ul><h3 id="快速锁定屏幕"><a href="#快速锁定屏幕" class="headerlink" title="快速锁定屏幕"></a>快速锁定屏幕</h3><ul><li><p>如果你长时间离开电脑，最好锁定你的屏幕，以防止数据泄露。 那如何快速的锁定你的 MacBook 呢？ 答案是只需要一摸触摸板就可以了。</p><ul><li><p>打开系统偏好设置，点击桌面与屏幕保护程序图标，选择屏幕保护程序这个 Tab，再点击触发角，在弹出的如下界面里面，右下角选择将显示器置入睡眠状态，再确定即可。</p><p><img src="/mac-configuration/screen_saver.png" srcset="/img/loading.gif" alt></p></li><li><p>再打开系统偏好设置，点击安全性与隐私图标，在通用 Tab 内，勾选为进入睡眠或开始屏幕保护程序<strong>立即</strong>要求输入密码。</p><p><img src="/mac-configuration/screen_security.png" srcset="/img/loading.gif" alt></p></li></ul></li></ul><h2 id="开发环境篇"><a href="#开发环境篇" class="headerlink" title="开发环境篇"></a>开发环境篇</h2><h3 id="Xcode"><a href="#Xcode" class="headerlink" title="Xcode"></a>Xcode</h3><ul><li><p>首先安装 Xcode，然后使用下面的命令安装 Xcode command line tools，这将为我们安装很多终端下面常用的命令，将来很可能会使用到。</p>  <pre><code class="hljs Shell">xcode-select --install</code></pre></li></ul><h3 id="Homebrew"><a href="#Homebrew" class="headerlink" title="Homebrew"></a>Homebrew</h3><ul><li><p>Homebrew 是一款终端下的命令程序包管理器，安装非常简单，复制如下命令在终端下运行，按回车并输入密码后等待安装成功：</p>  <pre><code class="hljs Shell">/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"</code></pre></li></ul><h3 id="iTerm2-Zsh-Z"><a href="#iTerm2-Zsh-Z" class="headerlink" title="iTerm2 + Zsh + Z"></a>iTerm2 + Zsh + Z</h3><ul><li>常用终端 iTerm2 + 优秀 Shell Zsh + 扁平目录跳转命令 Z，安装好之后开发十分舒适。具体安装可参考这个<a href="https://www.zcfy.cc/article/become-a-command-line-power-user-with-oh-my-zsh-and-z" target="_blank" rel="noopener">博客</a>。</li></ul><h3 id="快捷键迅速打开-iTerm2"><a href="#快捷键迅速打开-iTerm2" class="headerlink" title="快捷键迅速打开 iTerm2"></a>快捷键迅速打开 iTerm2</h3><ul><li>可以设置快捷键再 Home 页面输入 Command + , 直接打开 iTerm2，这样就不用再去点击 iTerm2 了。</li></ul><p><img src="/mac-configuration/iTerm2_hotkey.png" srcset="/img/loading.gif" alt></p><ul><li>可以设置 iTerm2 默认占满全屏，这样子快捷键打开之后就直接是一个全屏的 iTerm2 可以使用了</li></ul><p><img src="/mac-configuration/iTerm2_screen.png" srcset="/img/loading.gif" alt> </p><h3 id="VScode命令行迅速打开"><a href="#VScode命令行迅速打开" class="headerlink" title="VScode命令行迅速打开"></a>VScode命令行迅速打开</h3><ul><li>打开VScode后输入 Command + Shift + P 打开命令面板，再输入 code，再确定</li></ul><p><img src="/mac-configuration/vscode_code.png" srcset="/img/loading.gif" alt></p><h3 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h3><ul><li>创建新的公钥私钥并与自己的 Github 账户连起来，这样就可以开始在 Github 遨游啦。</li></ul><h3 id="Alfred"><a href="#Alfred" class="headerlink" title="Alfred"></a>Alfred</h3><ul><li>安装 macOS 提升效率神器 Alfred，具体用法可以参考<a href="https://juejin.cn/post/6844904062484217863" target="_blank" rel="noopener">博客</a>。</li></ul><h2 id="常用软件"><a href="#常用软件" class="headerlink" title="常用软件"></a>常用软件</h2><ul><li>网易云音乐</li><li>微信</li><li>QQ</li><li>腾讯会议</li><li>V2Ray</li><li>Chrome</li><li>VScode</li><li>GoLand</li><li>IDEA</li><li>JProfile</li><li>Docker</li><li>…</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>开发工具配置</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
